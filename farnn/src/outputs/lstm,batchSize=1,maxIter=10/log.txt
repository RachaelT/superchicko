LSTM training error vs. epoch	
 2.6798e-02	
 1.3091e-02	
 3.2052e-02	
 1.4953e-01	
 1.1236e-02	
 2.2263e-02	
 1.5908e-01	
 1.3148e-02	
 1.5500e-01	
 1.5998e-01	
 2.6388e-02	
 1.3241e-02	
 2.1114e-02	
 1.5523e-01	
 2.6395e-02	
 1.4903e-01	
 2.5982e-02	
 1.1548e-02	
 1.4427e-01	
 2.6362e-02	
 1.4433e-01	
 2.5370e-02	
 2.3454e-02	
 1.4533e-02	
 1.3024e-02	
 1.1994e-02	
 1.2703e-02	
 2.6703e-02	
 1.5532e-01	
 1.6918e-01	
 1.4203e-01	
 2.6607e-02	
 1.5030e-01	
 1.5188e-01	
 1.5827e-01	
 1.2154e-02	
 2.5790e-02	
 1.3463e-02	
 1.3638e-02	
 1.2965e-02	
 1.5514e-01	
 1.4315e-01	
 2.5071e-02	
 9.5382e-03	
 2.4534e-02	
 1.3400e-02	
 1.4681e-01	
 1.0325e-02	
 2.3500e-02	
 3.1502e-02	
 2.4420e-02	
 1.5172e-01	
 7.4311e-03	
 1.4972e-01	
 1.4205e-01	
 1.4501e-02	
 1.1373e-02	
 2.7104e-02	
 2.5932e-02	
 1.2714e-02	
 2.8072e-02	
 2.5309e-02	
 1.6886e-01	
 1.4509e-01	
 2.5987e-02	
 1.3781e-01	
 1.4581e-01	
 2.7108e-02	
 1.5394e-01	
 1.5472e-01	
 2.5394e-02	
 2.9884e-02	
 1.5198e-01	
 2.8627e-02	
 1.0325e-02	
 1.4446e-02	
 2.6384e-02	
 2.4825e-02	
 2.2885e-02	
 1.5397e-01	
 1.2235e-02	
 1.5454e-01	
 1.3494e-02	
 1.5236e-01	
 2.6676e-02	
 1.5480e-01	
 2.7624e-02	
 1.1287e-02	
 3.0200e-02	
 1.0029e-02	
 1.3604e-02	
 1.4902e-01	
 1.4411e-01	
 1.5623e-01	
 1.3627e-02	
 1.4619e-01	
 9.6149e-03	
 2.6160e-02	
 1.1360e-02	
 1.2143e-02	
 2.4956e-02	
 1.0600e-02	
 1.0025e-02	
 1.5224e-01	
 2.0546e-02	
 1.1822e-02	
 1.5005e-01	
 1.1900e-02	
 2.2679e-02	
 2.9617e-02	
 2.5492e-02	
 2.3534e-02	
 1.5559e-01	
 1.4487e-01	
 2.8341e-02	
 1.1254e-02	
 1.4845e-01	
 1.5431e-01	
 1.1682e-02	
 2.7585e-02	
 2.5135e-02	
 9.9351e-03	
 1.1070e-02	
 1.2451e-02	
 1.5906e-01	
 2.3604e-02	
 1.5174e-01	
 2.5319e-02	
 1.4849e-01	
 2.4188e-02	
 1.5252e-01	
 2.8741e-02	
 1.5948e-02	
 1.5536e-01	
 9.8256e-03	
 1.4212e-01	
 2.5126e-02	
 2.7347e-02	
 1.0731e-02	
 2.8528e-02	
 1.0564e-02	
 1.5107e-01	
 1.2793e-02	
 9.0656e-03	
 2.3398e-02	
 1.4427e-01	
 2.5497e-02	
 1.2963e-02	
 1.3153e-02	
 1.5323e-01	
 1.6171e-01	
 1.1148e-02	
 1.4852e-01	
 1.6128e-01	
 1.1916e-02	
 1.3423e-01	
 2.6407e-02	
 9.3096e-03	
 1.0514e-02	
 8.3076e-03	
 9.2613e-03	
 1.1302e-02	
 2.5823e-02	
 1.5262e-01	
 2.4035e-02	
 2.6850e-02	
 1.4081e-01	
 1.2813e-02	
 1.5726e-01	
 1.1903e-02	
 1.4566e-01	
 1.4247e-01	
 1.4686e-01	
 1.0805e-02	
 2.7165e-02	
 2.3976e-02	
 1.5143e-01	
 2.8260e-02	
 1.1910e-02	
 1.0775e-02	
 1.5686e-01	
 2.4193e-02	
 1.5446e-01	
 1.4861e-01	
 8.2836e-03	
 1.5105e-01	
 1.3868e-01	
 1.4292e-01	
 1.3814e-01	
 1.2095e-02	
 1.4456e-01	
 8.5155e-03	
 9.2819e-03	
 2.3774e-02	
 1.4353e-01	
 2.5972e-02	
 1.4822e-01	
 2.8363e-02	
 1.4447e-02	
 3.1156e-02	
 1.0547e-02	
 1.0807e-02	
 9.2532e-03	
 1.3394e-02	
 2.6056e-02	
 2.2331e-02	
 1.3852e-01	
 2.5825e-02	
 1.3725e-01	
 1.1927e-02	
 2.9806e-02	
 1.0713e-02	
 9.9972e-03	
 1.4879e-01	
 9.4976e-03	
 9.2063e-03	
 2.3068e-02	
 1.4592e-01	
 1.2119e-02	
 1.4531e-01	
 1.3857e-01	
 1.4277e-01	
 9.2089e-03	
 8.8754e-03	
 2.7799e-02	
 1.5165e-01	
 1.5119e-01	
 2.5024e-02	
 2.5901e-02	
 1.4929e-01	
 1.0900e-02	
 2.2801e-02	
 1.1456e-02	
 1.0694e-02	
 2.7585e-02	
 3.0062e-02	
 1.3569e-01	
 2.1367e-02	
 2.5926e-02	
 1.3812e-01	
 1.2850e-01	
 1.4666e-01	
 9.7763e-03	
 9.2929e-03	
 2.6896e-02	
 2.1403e-02	
 2.6374e-02	
 1.4944e-01	
 2.4489e-02	
 9.1132e-03	
 7.6504e-03	
 2.7661e-02	
 1.4675e-01	
 1.4385e-01	
 1.0033e-02	
 9.7478e-03	
 2.3405e-02	
 8.3874e-03	
 1.4842e-01	
 2.3806e-02	
 1.4286e-01	
 1.3423e-01	
 1.0467e-02	
 1.4076e-01	
 5.9285e-03	
 1.5483e-01	
 2.7454e-02	
 2.5923e-02	
 1.1499e-02	
 1.3310e-01	
 1.4525e-01	
 2.5086e-02	
 1.2407e-02	
 1.4259e-01	
 1.4477e-01	
 1.2026e-02	
 2.5108e-02	
 1.4413e-01	
 1.3972e-01	
 1.0232e-02	
 9.5749e-03	
 1.4831e-01	
 1.0428e-02	
 6.5475e-03	
 1.2156e-02	
 2.3994e-02	
 1.0417e-02	
 1.5064e-01	
 8.5809e-03	
 2.3750e-02	
 1.4606e-01	
 1.5066e-01	
 1.4841e-01	
 1.3258e-02	
 2.7069e-02	
 1.0177e-02	
 1.4160e-01	
 1.3427e-01	
 1.2992e-01	
 1.4025e-01	
 2.8413e-02	
 8.6619e-03	
 3.0747e-02	
 3.2243e-02	
 1.3414e-01	
 1.1355e-02	
 2.9481e-02	
 2.4165e-02	
 9.5454e-03	
 2.6286e-02	
 1.2967e-01	
 8.5367e-03	
 1.4082e-01	
 1.3983e-01	
 1.0327e-02	
 1.2050e-02	
 1.3979e-01	
 1.3380e-01	
 8.4333e-03	
 1.3226e-01	
 2.4893e-02	
 2.6385e-02	
 2.4330e-02	
 1.3842e-01	
 2.5553e-02	
 2.4543e-02	
 1.2062e-02	
 1.3145e-01	
 2.0120e-02	
 9.7852e-03	
 1.4207e-02	
 2.4454e-02	
 1.3170e-01	
 1.3789e-01	
 1.1280e-02	
 1.3653e-01	
 2.7144e-02	
 2.8640e-02	
 7.3363e-03	
 1.2871e-02	
 1.4372e-01	
 9.5034e-03	
 3.0288e-02	
 2.7040e-02	
 1.3234e-01	
 1.3717e-02	
 1.3717e-01	
 1.0781e-02	
 1.4452e-01	
 2.3180e-02	
 1.4470e-01	
 1.3024e-01	
 8.8367e-03	
 9.7590e-03	
 2.9216e-02	
 3.0335e-02	
 2.7084e-02	
 1.3512e-01	
 1.2706e-01	
 1.0132e-02	
 2.6014e-02	
 2.4531e-02	
 1.3720e-01	
 1.4414e-01	
 2.5691e-02	
 1.3481e-01	
 1.3979e-01	
 1.1707e-02	
 2.5738e-02	
 3.1323e-02	
 2.8014e-02	
 1.2443e-01	
 9.2026e-03	
 2.3084e-02	
 8.1793e-03	
 8.4624e-03	
 2.6013e-02	
 1.2842e-01	
 1.4005e-01	
 1.4845e-01	
 1.3848e-01	
 3.0245e-02	
 1.3758e-01	
 1.3470e-01	
 1.3496e-01	
 9.2332e-03	
 1.1087e-02	
 2.2485e-02	
 1.4158e-01	
 2.2663e-02	
 1.3781e-01	
 7.4977e-03	
 2.4697e-02	
 1.3732e-01	
 1.4387e-01	
 1.3319e-02	
 2.4230e-02	
 2.5992e-02	
 8.3679e-03	
 1.3683e-01	
 1.3475e-01	
 3.0816e-02	
 9.1247e-03	
 1.3190e-01	
 2.8082e-02	
 7.6267e-03	
 2.9634e-02	
 1.3768e-01	
 2.3291e-02	
 1.2606e-01	
 9.0389e-03	
 2.3273e-02	
 1.3873e-01	
 1.4087e-01	
 8.1020e-03	
 2.5624e-02	
 2.7609e-02	
 2.9558e-02	
 2.0547e-02	
 1.3503e-01	
 9.1821e-03	
 5.9158e-03	
 1.2746e-02	
 1.1200e-02	
 1.3940e-01	
 2.7999e-02	
 1.0026e-02	
 1.1012e-02	
 1.3346e-01	
 7.2500e-03	
 2.8890e-02	
 7.6838e-03	
 1.4601e-01	
 1.4653e-01	
 1.4229e-01	
 1.3118e-01	
 9.8813e-03	
 2.3701e-02	
 1.3361e-01	
 9.9997e-03	
 7.4955e-03	
 4.8159e-03	
 1.4466e-01	
 1.0140e-02	
 2.7422e-02	
 2.5012e-02	
 2.7081e-02	
 7.0904e-03	
 3.1103e-02	
 2.7986e-02	
 1.3765e-01	
 1.2715e-01	
 1.3844e-01	
 1.3238e-01	
 2.4221e-02	
 7.3746e-03	
 3.3231e-02	
 2.5859e-02	
 7.2787e-03	
 6.4726e-03	
 9.3249e-03	
 1.3165e-01	
 6.7670e-03	
 5.1758e-03	
 1.5172e-01	
 7.5707e-03	
 1.1340e-02	
 1.0204e-02	
 7.0135e-03	
 1.4029e-01	
 5.5364e-03	
 1.4317e-01	
 1.2790e-01	
 7.0855e-03	
 2.7932e-02	
 8.8841e-03	
 1.3360e-01	
 1.3446e-01	
 1.3705e-01	
 3.1875e-02	
 3.0102e-02	
 1.0331e-02	
 1.3904e-01	
 2.8213e-02	
 2.7548e-02	
 8.0111e-03	
 6.0064e-03	
 1.3571e-01	
 2.5272e-02	
 2.5026e-02	
 3.1213e-02	
 2.7517e-02	
 6.4207e-03	
 2.7432e-02	
 1.3274e-01	
 2.4976e-02	
 2.9725e-02	
 8.4418e-03	
 1.4179e-01	
 2.9994e-02	
0744  0.0384  0.0169 -0.0354 -0.0670 -0.0889 -0.0799
-0.0734 -0.0800  0.0696 -0.0026  0.0693 -0.0936  0.0313 -0.0539  0.0510  0.0374
-0.0272 -0.0212 -0.0259  0.0107  0.0790  0.0122 -0.0111  0.0559  0.0868  0.0199
 0.0627 -0.0567  0.0452  0.0230 -0.0065 -0.0614 -0.0472  0.0800  0.0428  0.0264
 0.0049 -0.0158 -0.0635 -0.0545  0.0820  0.1000 -0.0767  0.0334  0.0963 -0.0703

Columns 11 to 20
-0.0916 -0.0505 -0.0697  0.0428 -0.0893 -0.0581  0.0121 -0.0368  0.0583  0.0196
-0.0776  0.0515  0.0160  0.0913  0.0391 -0.0277  0.0335 -0.0857 -0.0022  0.0829
 0.0879 -0.0119  0.0324 -0.0711  0.0883  0.0577  0.0259 -0.0833  0.0989 -0.0609
-0.0956 -0.0692  0.0491  0.0016  0.0731  0.0046 -0.0761  0.0368  0.0098  0.0973
-0.0564  0.0322  0.0925 -0.0446 -0.0826 -0.0178 -0.0108  0.0579  0.0858 -0.0215
-0.0060  0.0961  0.0999 -0.0695 -0.0225  0.0223 -0.0245 -0.0129  0.0423  0.0121

Columns 21 to 30
-0.0487  0.0221  0.0447  0.0610 -0.0560  0.0508  0.0911 -0.0818  0.0618  0.0629
-0.0536 -0.0810 -0.0818  0.0052  0.0035 -0.0009  0.0058 -0.0149 -0.0923  0.0794
 0.0028 -0.0305 -0.0959 -0.0502 -0.0058  0.0958 -0.0580 -0.0967  0.0660 -0.0746
 0.0892  0.0325  0.0209 -0.0292 -0.0821  0.0329 -0.0652  0.0563  0.0151 -0.0378
 0.0585  0.0045 -0.0108  0.0341  0.0797 -0.0090 -0.0843 -0.0462 -0.0376  0.0261
 0.0668  0.0224 -0.0961 -0.0753 -0.0164 -0.0187 -0.0384 -0.0707  0.0459 -0.0659

Columns 31 to 40
 0.0124 -0.0051 -0.0674 -0.0167  0.0992 -0.0985  0.0837 -0.0374  0.0180  0.0241
 0.0247 -0.0635  0.0167 -0.0868  0.0634  0.0056  0.0205  0.0045  0.0490  0.0137
 0.0114 -0.0588  0.0214  0.0887 -0.0272 -0.0743 -0.0210  0.0344  0.0540  0.0567
-0.0996 -0.0229  0.0903 -0.0015 -0.0217  0.0631  0.0869 -0.0657  0.0319 -0.0447
-0.0362  0.0569  0.0256 -0.0932  0.0312  0.0987 -0.0149  0.0163  0.0568  0.0093
-0.0028  0.0020 -0.0096 -0.0860  0.0739 -0.0700  0.0378  0.0011  0.0238  0.0415

Columns 41 to 50
-0.0530  0.0049 -0.0878 -0.0505  0.0065 -0.0948  0.0614 -0.0599  0.0641  0.0962
 0.0839 -0.0004  0.0240  0.0208  0.0137  0.0353 -0.0675  0.0656 -0.0927 -0.0380
 0.0735  0.0568  0.0902  0.0897  0.0536  0.0814 -0.0639  0.0884 -0.0704 -0.0044
-0.0914  0.0464  0.0158  0.0011 -0.0229  0.0632  0.0330 -0.0363 -0.0348 -0.0143
-0.0407 -0.0359  0.0235  0.0551 -0.0047  0.0579 -0.0819  0.0214 -0.0052 -0.0995
-0.0642  0.0761 -0.0606  0.0101 -0.0064 -0.0561 -0.0979  0.0506  0.0798  0.0092

Columns 51 to 60
-0.0975  0.0970  0.0790  0.0145  0.0660 -0.0871 -0.0049  0.0538 -0.0261  0.0728
 0.0306  0.0440 -0.0124  0.0750 -0.0007 -0.0128 -0.0182  0.0963  0.0645 -0.0073
-0.0734 -0.0691  0.0684  0.0049  0.0465  0.0325 -0.0729  0.0778 -0.0218  0.0554
-0.0076 -0.0346  0.0305 -0.0675 -0.0975 -0.0915 -0.0342  0.0585  0.0421  0.0476
 0.0192 -0.0174  0.0307  0.0794 -0.0116  0.0172  0.0532  0.0579  0.0400  0.0628
 0.0233 -0.0736 -0.0473 -0.0169 -0.0551  0.0941 -0.0273  0.0629 -0.0420  0.0314

Columns 61 to 70
-0.0421  0.0492 -0.0379 -0.0929 -0.0603  0.0735 -0.0125  0.0551  0.0809 -0.0666
-0.0762  0.0059  0.0735  0.0552  0.0324 -0.0965 -0.0927  0.0365  0.0077  0.0428
-0.0974  0.0843 -0.0455  0.0914 -0.0376 -0.0221 -0.0959 -0.0513  0.0545 -0.0526
 0.0505 -0.0552  0.0011  0.0991 -0.0561 -0.0373 -0.0802  0.0623 -0.0958 -0.0310
 0.0852 -0.0150 -0.0673 -0.0428  0.0871  0.0716 -0.0273 -0.0440 -0.0334 -0.0951
-0.0128  0.0171 -0.0821  0.0015  0.0485  0.0864  0.0940  0.0145 -0.0189 -0.0254

Columns 71 to 80
-0.0356 -0.0329 -0.0728 -0.0355  0.0993 -0.0213  0.0481 -0.0495 -0.0207  0.0795
-0.0159 -0.0365 -0.0122 -0.0954 -0.0702  0.0978 -0.0537  0.0476  0.0470  0.0427
-0.0251  0.0281  0.0636  0.0065  0.0255 -0.0706 -0.0133  0.0207 -0.0723 -0.0928
 0.0532 -0.0182 -0.0402 -0.0743 -0.0041  0.0929 -0.0282 -0.0952  0.0975 -0.0048
-0.0250  0.0364 -0.0723 -0.0315  0.0493  0.0086  0.0514  0.0405  0.0777  0.0299
-0.0814 -0.0322  0.0180  0.0919  0.0209 -0.0872 -0.0739 -0.0645 -0.0789  0.0719

Columns 81 to 90
-0.0195 -0.0989  0.0805  0.0111  0.0579 -0.0945  0.0355 -0.0128 -0.0783  0.0082
-0.0900  0.0799 -0.0986  0.0630  0.0966 -0.0437 -0.0061  0.0509 -0.0437 -0.0147
-0.0097 -0.0816 -0.0278  0.0611  0.0210  0.0706  0.0727  0.0491  0.0628  0.0222
-0.0511  0.0734 -0.0573  0.0522 -0.0214 -0.0179  0.0088 -0.0557 -0.0880  0.0311
-0.0146  0.0870 -0.0567  0.0871  0.0357  0.0958  0.0542  0.0167 -0.0873 -0.0697
 0.0840 -0.0936  0.0840  0.0133  0.0284  0.0583 -0.0464  0.0280 -0.0627 -0.0269

Columns 91 to 100
 0.0130  0.0989  0.0661  0.0257 -0.0272 -0.0215  0.0552 -0.0030  0.0154  0.0472
-0.0755  0.0573 -0.0247 -0.0955 -0.0506  0.0422 -0.0706  0.0826  0.0305 -0.0692
-0.0487 -0.0516  0.0345 -0.0600 -0.0544 -0.0473 -0.0140 -0.0719  0.0207 -0.0428
 0.0893  0.0529 -0.0613  0.0664 -0.0300 -0.0800 -0.0522  0.0578  0.0879  0.0295
-0.0225  0.0265 -0.0940  0.0367  0.0827 -0.0546  0.0222 -0.0116  0.0027 -0.0736
-0.0392  0.0333 -0.0034  0.0863  0.0883 -0.0642  0.0665  0.0401  0.0182  0.0205
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:01[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:01[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:01[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 2 [batchSize = 1]
 
2016-10-13 16:47:01[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.41463915507ms 
2016-10-13 16:47:01[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:01[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0551  0.0300  0.0813 -0.0746  0.0171  0.0436 -0.0806  0.0762 -0.0743  0.0713
 0.0742  0.0335 -0.0912 -0.0744  0.0384  0.0170 -0.0354 -0.0670 -0.0889 -0.0799
-0.0734 -0.0800  0.0696 -0.0025  0.0693 -0.0935  0.0313 -0.0539  0.0510  0.0374
-0.0272 -0.0213 -0.0259  0.0107  0.0790  0.0123 -0.0111  0.0560  0.0867  0.0199
 0.0627 -0.0567  0.0452  0.0230 -0.0065 -0.0613 -0.0472  0.0800  0.0428  0.0264
 0.0049 -0.0158 -0.0635 -0.0545  0.0820  0.1000 -0.0767  0.0334  0.0963 -0.0703

Columns 11 to 20
-0.0916 -0.0505 -0.0697  0.0428 -0.0893 -0.0581  0.0121 -0.0368  0.0583  0.0196
-0.0776  0.0516  0.0160  0.0913  0.0390 -0.0277  0.0335 -0.0858 -0.0022  0.0829
 0.0879 -0.0119  0.0324 -0.0712  0.0883  0.0577  0.0259 -0.0833  0.0989 -0.0609
-0.0956 -0.0692  0.0491  0.0015  0.0731  0.0046 -0.0762  0.0368  0.0097  0.0972
-0.0564  0.0322  0.0925 -0.0446 -0.0826 -0.0178 -0.0108  0.0579  0.0858 -0.0215
-0.0060  0.0961  0.0999 -0.0695 -0.0225  0.0223 -0.0245 -0.0130  0.0423  0.0120

Columns 21 to 30
-0.0487  0.0221  0.0447  0.0610 -0.0560  0.0508  0.0911 -0.0818  0.0618  0.0629
-0.0537 -0.0810 -0.0818  0.0053  0.0035 -0.0009  0.0057 -0.0149 -0.0923  0.0794
 0.0027 -0.0305 -0.0959 -0.0502 -0.0058  0.0958 -0.0581 -0.0966  0.0660 -0.0746
 0.0892  0.0325  0.0209 -0.0291 -0.0821  0.0329 -0.0652  0.0563  0.0151 -0.0379
 0.0585  0.0046 -0.0108  0.0342  0.0797 -0.0091 -0.0844 -0.0462 -0.0376  0.0261
 0.0668  0.0224 -0.0961 -0.0753 -0.0164 -0.0187 -0.0385 -0.0707  0.0458 -0.0659

Columns 31 to 40
 0.0124 -0.0051 -0.0674 -0.0168  0.0992 -0.0985  0.0837 -0.0374  0.0180  0.0241
 0.0247 -0.0635  0.0167 -0.0868  0.0634  0.0056  0.0205  0.0046  0.0490  0.0137
 0.0114 -0.0588  0.0214  0.0887 -0.0272 -0.0743 -0.0210  0.0345  0.0540  0.0567
-0.0996 -0.0229  0.0903 -0.0016 -0.0217  0.0632  0.0869 -0.0657  0.0319 -0.0447
-0.0363  0.0569  0.0256 -0.0932  0.0312  0.0987 -0.0149  0.0163  0.0568  0.0093
-0.0028  0.0020 -0.0096 -0.0861  0.0739 -0.0700  0.0378  0.0011  0.0238  0.0415

Columns 41 to 50
-0.0530  0.0049 -0.0878 -0.0505  0.0065 -0.0948  0.0614 -0.0599  0.0641  0.0962
 0.0839 -0.0004  0.0240  0.0208  0.0137  0.0353 -0.0676  0.0656 -0.0927 -0.0380
 0.0735  0.0568  0.0902  0.0897  0.0536  0.0814 -0.0639  0.0884 -0.0704 -0.0044
-0.0913  0.0464  0.0158  0.0010 -0.0228  0.0632  0.0330 -0.0363 -0.0349 -0.0143
-0.0407 -0.0359  0.0236  0.0550 -0.0047  0.0579 -0.0819  0.0214 -0.0052 -0.0995
-0.0642  0.0761 -0.0606  0.0101 -0.0064 -0.0561 -0.0979  0.0506  0.0798  0.0092

Columns 51 to 60
-0.0975  0.0970  0.0790  0.0145  0.0660 -0.0871 -0.0049  0.0538 -0.0261  0.0728
 0.0306  0.0441 -0.0123  0.0750 -0.0007 -0.0128 -0.0182  0.0963  0.0645 -0.0073
-0.0734 -0.0691  0.0684  0.0049  0.0465  0.0325 -0.0729  0.0779 -0.0218  0.0554
-0.0076 -0.0346  0.0307 -0.0674 -0.0975 -0.0915 -0.0342  0.0585  0.0421  0.0476
 0.0192 -0.0174  0.0307  0.0794 -0.0116  0.0172  0.0532  0.0579  0.0400  0.0628
 0.0233 -0.0736 -0.0473 -0.0168 -0.0551  0.0941 -0.0273  0.0629 -0.0420  0.0314

Columns 61 to 70
-0.0421  0.0492 -0.0379 -0.0929 -0.0603  0.0735 -0.0125  0.0551  0.0809 -0.0666
-0.0762  0.0060  0.0735  0.0552  0.0324 -0.0965 -0.0927  0.0365  0.0077  0.0427
-0.0973  0.0844 -0.0455  0.0914 -0.0376 -0.0220 -0.0959 -0.0513  0.0546 -0.0526
 0.0506 -0.0551  0.0011  0.0991 -0.0561 -0.0373 -0.0802  0.0623 -0.0958 -0.0311
 0.0852 -0.0149 -0.0673 -0.0428  0.0871  0.0716 -0.0273 -0.0439 -0.0334 -0.0951
-0.0127  0.0171 -0.0821  0.0015  0.0485  0.0864  0.0940  0.0145 -0.0189 -0.0254

Columns 71 to 80
-0.0356 -0.0329 -0.0728 -0.0355  0.0993 -0.0213  0.0481 -0.0495 -0.0207  0.0795
-0.0159 -0.0365 -0.0122 -0.0953 -0.0702  0.0978 -0.0537  0.0476  0.0470  0.0426
-0.0251  0.0282  0.0636  0.0065  0.0255 -0.0706 -0.0133  0.0207 -0.0722 -0.0928
 0.0532 -0.0182 -0.0402 -0.0743 -0.0042  0.0930 -0.0282 -0.0952  0.0975 -0.0048
-0.0250  0.0364 -0.0723 -0.0315  0.0493  0.0086  0.0513  0.0405  0.0777  0.0298
-0.0814 -0.0322  0.0180  0.0919  0.0209 -0.0872 -0.0739 -0.0645 -0.0789  0.0719

Columns 81 to 90
-0.0195 -0.0989  0.0805  0.0112  0.0579 -0.0945  0.0355 -0.0128 -0.0783  0.0082
-0.0900  0.0799 -0.0986  0.0630  0.0966 -0.0437 -0.0061  0.0509 -0.0437 -0.0147
-0.0097 -0.0816 -0.0278  0.0612  0.0210  0.0707  0.0727  0.0491  0.0628  0.0222
-0.0511  0.0734 -0.0573  0.0523 -0.0215 -0.0179  0.0088 -0.0557 -0.0881  0.0311
-0.0146  0.0869 -0.0567  0.0871  0.0357  0.0958  0.0542  0.0167 -0.0873 -0.0697
 0.0840 -0.0936  0.0840  0.0134  0.0284  0.0583 -0.0464  0.0280 -0.0627 -0.0269

Columns 91 to 100
 0.0130  0.0989  0.0661  0.0257 -0.0272 -0.0215  0.0552 -0.0030  0.0154  0.0472
-0.0755  0.0573 -0.0248 -0.0956 -0.0506  0.0422 -0.0706  0.0825  0.0305 -0.0692
-0.0488 -0.0516  0.0345 -0.0600 -0.0543 -0.0473 -0.0140 -0.0719  0.0207 -0.0427
 0.0892  0.0529 -0.0614  0.0664 -0.0299 -0.0800 -0.0522  0.0578  0.0878  0.0296
-0.0225  0.0265 -0.0940  0.0367  0.0828 -0.0546  0.0223 -0.0116  0.0027 -0.0735
-0.0392  0.0333 -0.0034  0.0862  0.0883 -0.0642  0.0665  0.0401  0.0181  0.0205
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:01[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:01[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:01[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 3 [batchSize = 1]
 
2016-10-13 16:47:02[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.202685038249ms 
2016-10-13 16:47:02[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:02[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0551  0.0300  0.0813 -0.0745  0.0171  0.0436 -0.0806  0.0763 -0.0743  0.0713
 0.0743  0.0334 -0.0912 -0.0743  0.0384  0.0170 -0.0354 -0.0669 -0.0890 -0.0799
-0.0733 -0.0800  0.0696 -0.0025  0.0693 -0.0935  0.0313 -0.0538  0.0509  0.0374
-0.0272 -0.0213 -0.0259  0.0107  0.0790  0.0124 -0.0111  0.0560  0.0867  0.0199
 0.0627 -0.0567  0.0452  0.0230 -0.0065 -0.0613 -0.0472  0.0800  0.0427  0.0264
 0.0049 -0.0158 -0.0635 -0.0545  0.0820  0.1001 -0.0767  0.0334  0.0963 -0.0703

Columns 11 to 20
-0.0916 -0.0505 -0.0697  0.0428 -0.0893 -0.0581  0.0121 -0.0368  0.0583  0.0196
-0.0775  0.0516  0.0161  0.0913  0.0390 -0.0277  0.0335 -0.0858 -0.0022  0.0829
 0.0879 -0.0119  0.0324 -0.0712  0.0883  0.0577  0.0259 -0.0834  0.0989 -0.0609
-0.0956 -0.0691  0.0492  0.0015  0.0730  0.0047 -0.0762  0.0367  0.0097  0.0972
-0.0564  0.0322  0.0925 -0.0447 -0.0826 -0.0178 -0.0108  0.0579  0.0858 -0.0215
-0.0060  0.0961  0.1000 -0.0695 -0.0225  0.0223 -0.0245 -0.0130  0.0423  0.0120

Columns 21 to 30
-0.0487  0.0221  0.0447  0.0610 -0.0560  0.0508  0.0911 -0.0818  0.0618  0.0629
-0.0537 -0.0809 -0.0818  0.0053  0.0035 -0.0010  0.0057 -0.0149 -0.0923  0.0794
 0.0027 -0.0305 -0.0958 -0.0502 -0.0058  0.0958 -0.0581 -0.0966  0.0660 -0.0746
 0.0891  0.0326  0.0210 -0.0291 -0.0821  0.0329 -0.0653  0.0563  0.0151 -0.0379
 0.0585  0.0046 -0.0108  0.0342  0.0797 -0.0091 -0.0844 -0.0462 -0.0376  0.0261
 0.0668  0.0224 -0.0961 -0.0753 -0.0164 -0.0187 -0.0385 -0.0707  0.0458 -0.0659

Columns 31 to 40
 0.0124 -0.0051 -0.0674 -0.0168  0.0992 -0.0985  0.0837 -0.0374  0.0180  0.0241
 0.0247 -0.0635  0.0167 -0.0868  0.0634  0.0056  0.0205  0.0046  0.0490  0.0137
 0.0113 -0.0587  0.0213  0.0887 -0.0272 -0.0743 -0.0211  0.0345  0.0539  0.0567
-0.0996 -0.0229  0.0903 -0.0016 -0.0217  0.0632  0.0868 -0.0656  0.0318 -0.0447
-0.0363  0.0569  0.0256 -0.0933  0.0312  0.0988 -0.0149  0.0163  0.0568  0.0093
-0.0028  0.0020 -0.0096 -0.0861  0.0739 -0.0700  0.0377  0.0012  0.0238  0.0416

Columns 41 to 50
-0.0529  0.0049 -0.0878 -0.0505  0.0065 -0.0948  0.0614 -0.0599  0.0641  0.0961
 0.0840 -0.0004  0.0241  0.0207  0.0137  0.0353 -0.0676  0.0656 -0.0928 -0.0381
 0.0735  0.0568  0.0902  0.0897  0.0536  0.0814 -0.0639  0.0884 -0.0705 -0.0045
-0.0913  0.0464  0.0158  0.0010 -0.0228  0.0632  0.0330 -0.0363 -0.0349 -0.0144
-0.0406 -0.0359  0.0236  0.0550 -0.0047  0.0579 -0.0819  0.0214 -0.0052 -0.0995
-0.0642  0.0761 -0.0606  0.0101 -0.0064 -0.0561 -0.0979  0.0506  0.0798  0.0092

Columns 51 to 60
-0.0975  0.0970  0.0790  0.0145  0.0660 -0.0871 -0.0049  0.0538 -0.0261  0.0728
 0.0306  0.0441 -0.0123  0.0750 -0.0007 -0.0128 -0.0182  0.0963  0.0646 -0.0073
-0.0734 -0.0691  0.0684  0.0049  0.0465  0.0325 -0.0729  0.0779 -0.0217  0.0554
-0.0076 -0.0345  0.0307 -0.0674 -0.0975 -0.0916 -0.0343  0.0585  0.0421  0.0476
 0.0192 -0.0174  0.0308  0.0794 -0.0116  0.0172  0.0532  0.0580  0.0400  0.0628
 0.0233 -0.0736 -0.0473 -0.0168 -0.0551  0.0941 -0.0273  0.0629 -0.0420  0.0314

Columns 61 to 70
-0.0421  0.0492 -0.0379 -0.0929 -0.0603  0.0735 -0.0125  0.0551  0.0809 -0.0666
-0.0762  0.0060  0.0735  0.0552  0.0324 -0.0965 -0.0927  0.0366  0.0078  0.0427
-0.0973  0.0844 -0.0455  0.0914 -0.0376 -0.0220 -0.0959 -0.0513  0.0546 -0.0526
 0.0506 -0.0551  0.0011  0.0991 -0.0561 -0.0372 -0.0802  0.0624 -0.0957 -0.0311
 0.0852 -0.0149 -0.0673 -0.0428  0.0871  0.0716 -0.0273 -0.0439 -0.0333 -0.0951
-0.0127  0.0171 -0.0821  0.0015  0.0485  0.0864  0.0940  0.0145 -0.0188 -0.0254

Columns 71 to 80
-0.0356 -0.0329 -0.0728 -0.0355  0.0993 -0.0213  0.0481 -0.0495 -0.0207  0.0795
-0.0159 -0.0364 -0.0122 -0.0953 -0.0702  0.0978 -0.0538  0.0476  0.0470  0.0426
-0.0251  0.0282  0.0636  0.0065  0.0255 -0.0706 -0.0134  0.0207 -0.0722 -0.0928
 0.0532 -0.0182 -0.0401 -0.0743 -0.0042  0.0930 -0.0283 -0.0952  0.0975 -0.0048
-0.0250  0.0365 -0.0723 -0.0315  0.0493  0.0086  0.0513  0.0405  0.0777  0.0298
-0.0814 -0.0322  0.0180  0.0919  0.0209 -0.0872 -0.0739 -0.0645 -0.0789  0.0719

Columns 81 to 90
-0.0195 -0.0989  0.0805  0.0112  0.0579 -0.0945  0.0354 -0.0128 -0.0783  0.0082
-0.0900  0.0799 -0.0985  0.0630  0.0966 -0.0437 -0.0061  0.0509 -0.0437 -0.0147
-0.0097 -0.0816 -0.0278  0.0612  0.0210  0.0707  0.0727  0.0491  0.0628  0.0222
-0.0511  0.0734 -0.0572  0.0523 -0.0215 -0.0179  0.0087 -0.0557 -0.0881  0.0311
-0.0146  0.0869 -0.0567  0.0871  0.0357  0.0958  0.0542  0.0167 -0.0873 -0.0697
 0.0840 -0.0936  0.0840  0.0134  0.0284  0.0583 -0.0464  0.0280 -0.0627 -0.0269

Columns 91 to 100
 0.0130  0.0989  0.0661  0.0257 -0.0272 -0.0215  0.0552 -0.0030  0.0154  0.0472
-0.0755  0.0573 -0.0248 -0.0956 -0.0506  0.0422 -0.0706  0.0825  0.0304 -0.0692
-0.0488 -0.0516  0.0344 -0.0600 -0.0543 -0.0473 -0.0140 -0.0719  0.0206 -0.0427
 0.0892  0.0529 -0.0614  0.0663 -0.0299 -0.0800 -0.0522  0.0578  0.0878  0.0297
-0.0225  0.0265 -0.0940  0.0366  0.0828 -0.0546  0.0223 -0.0116  0.0027 -0.0735
-0.0392  0.0334 -0.0034  0.0862  0.0883 -0.0642  0.0665  0.0401  0.0181  0.0205
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:02[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:02[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:02[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 4 [batchSize = 1]
 
2016-10-13 16:47:02[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.292012532552ms 
2016-10-13 16:47:02[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:02[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0550  0.0300  0.0813 -0.0745  0.0171  0.0436 -0.0806  0.0763 -0.0743  0.0713
 0.0743  0.0334 -0.0913 -0.0743  0.0384  0.0170 -0.0354 -0.0669 -0.0890 -0.0798
-0.0733 -0.0801  0.0696 -0.0025  0.0693 -0.0934  0.0313 -0.0538  0.0509  0.0374
-0.0271 -0.0215 -0.0259  0.0108  0.0791  0.0125 -0.0112  0.0561  0.0866  0.0200
 0.0627 -0.0568  0.0452  0.0231 -0.0065 -0.0612 -0.0473  0.0801  0.0427  0.0264
 0.0049 -0.0159 -0.0635 -0.0544  0.0820  0.1001 -0.0767  0.0334  0.0963 -0.0703

Columns 11 to 20
-0.0915 -0.0505 -0.0697  0.0428 -0.0893 -0.0581  0.0121 -0.0368  0.0583  0.0195
-0.0775  0.0516  0.0161  0.0913  0.0390 -0.0277  0.0335 -0.0858 -0.0022  0.0828
 0.0879 -0.0119  0.0325 -0.0712  0.0883  0.0577  0.0259 -0.0834  0.0989 -0.0610
-0.0956 -0.0691  0.0492  0.0015  0.0730  0.0047 -0.0762  0.0367  0.0097  0.0971
-0.0564  0.0322  0.0926 -0.0447 -0.0826 -0.0178 -0.0108  0.0579  0.0858 -0.0216
-0.0060  0.0961  0.1000 -0.0695 -0.0225  0.0223 -0.0245 -0.0130  0.0423  0.0120

Columns 21 to 30
-0.0487  0.0221  0.0448  0.0610 -0.0560  0.0508  0.0910 -0.0817  0.0617  0.0629
-0.0537 -0.0809 -0.0817  0.0053  0.0034 -0.0010  0.0057 -0.0148 -0.0923  0.0794
 0.0027 -0.0304 -0.0958 -0.0501 -0.0058  0.0958 -0.0581 -0.0966  0.0660 -0.0746
 0.0891  0.0326  0.0210 -0.0290 -0.0821  0.0329 -0.0654  0.0564  0.0150 -0.0379
 0.0585  0.0046 -0.0108  0.0342  0.0797 -0.0091 -0.0844 -0.0462 -0.0376  0.0261
 0.0668  0.0224 -0.0961 -0.0753 -0.0164 -0.0187 -0.0385 -0.0707  0.0458 -0.0659

Columns 31 to 40
 0.0124 -0.0051 -0.0674 -0.0168  0.0992 -0.0985  0.0837 -0.0374  0.0180  0.0241
 0.0247 -0.0635  0.0167 -0.0868  0.0634  0.0056  0.0204  0.0046  0.0489  0.0137
 0.0113 -0.0587  0.0213  0.0887 -0.0272 -0.0743 -0.0211  0.0345  0.0539  0.0567
-0.0997 -0.0228  0.0903 -0.0017 -0.0217  0.0632  0.0867 -0.0655  0.0318 -0.0447
-0.0363  0.0569  0.0256 -0.0933  0.0312  0.0988 -0.0150  0.0163  0.0567  0.0093
-0.0028  0.0020 -0.0096 -0.0861  0.0738 -0.0700  0.0377  0.0012  0.0238  0.0416

Columns 41 to 50
-0.0529  0.0049 -0.0878 -0.0505  0.0065 -0.0948  0.0614 -0.0599  0.0641  0.0961
 0.0840 -0.0004  0.0241  0.0207  0.0137  0.0353 -0.0676  0.0656 -0.0928 -0.0381
 0.0735  0.0568  0.0903  0.0896  0.0536  0.0814 -0.0639  0.0885 -0.0705 -0.0045
-0.0913  0.0464  0.0158  0.0010 -0.0228  0.0632  0.0330 -0.0363 -0.0350 -0.0145
-0.0406 -0.0359  0.0236  0.0550 -0.0047  0.0579 -0.0819  0.0214 -0.0052 -0.0996
-0.0642  0.0761 -0.0605  0.0101 -0.0064 -0.0561 -0.0979  0.0506  0.0798  0.0091

Columns 51 to 60
-0.0975  0.0970  0.0791  0.0145  0.0659 -0.0872 -0.0049  0.0538 -0.0261  0.0728
 0.0306  0.0441 -0.0123  0.0751 -0.0007 -0.0128 -0.0182  0.0964  0.0646 -0.0073
-0.0734 -0.0691  0.0685  0.0050  0.0465  0.0325 -0.0729  0.0779 -0.0217  0.0554
-0.0076 -0.0345  0.0308 -0.0673 -0.0975 -0.0916 -0.0344  0.0586  0.0422  0.0476
 0.0192 -0.0173  0.0308  0.0795 -0.0115  0.0172  0.0532  0.0580  0.0401  0.0628
 0.0233 -0.0736 -0.0472 -0.0168 -0.0551  0.0941 -0.0274  0.0629 -0.0420  0.0314

Columns 61 to 70
-0.0421  0.0493 -0.0379 -0.0930 -0.0603  0.0735 -0.0125  0.0551  0.0810 -0.0666
-0.0761  0.0060  0.0735  0.0551  0.0324 -0.0964 -0.0927  0.0366  0.0078  0.0426
-0.0973  0.0845 -0.0455  0.0913 -0.0376 -0.0220 -0.0959 -0.0513  0.0546 -0.0527
 0.0507 -0.0550  0.0010  0.0990 -0.0561 -0.0372 -0.0801  0.0624 -0.0956 -0.0313
 0.0853 -0.0149 -0.0673 -0.0429  0.0871  0.0716 -0.0273 -0.0439 -0.0333 -0.0952
-0.0127  0.0172 -0.0821  0.0015  0.0485  0.0865  0.0940  0.0145 -0.0188 -0.0254

Columns 71 to 80
-0.0356 -0.0328 -0.0728 -0.0355  0.0992 -0.0213  0.0480 -0.0495 -0.0207  0.0795
-0.0160 -0.0364 -0.0122 -0.0953 -0.0702  0.0979 -0.0538  0.0476  0.0470  0.0426
-0.0251  0.0282  0.0637  0.0065  0.0255 -0.0705 -0.0134  0.0207 -0.0722 -0.0929
 0.0531 -0.0181 -0.0401 -0.0742 -0.0042  0.0931 -0.0284 -0.0952  0.0976 -0.0049
-0.0250  0.0365 -0.0722 -0.0314  0.0493  0.0087  0.0513  0.0405  0.0777  0.0298
-0.0814 -0.0322  0.0180  0.0919  0.0209 -0.0872 -0.0740 -0.0645 -0.0789  0.0719

Columns 81 to 90
-0.0195 -0.0989  0.0805  0.0112  0.0579 -0.0945  0.0354 -0.0128 -0.0784  0.0082
-0.0901  0.0799 -0.0985  0.0631  0.0966 -0.0437 -0.0061  0.0509 -0.0437 -0.0147
-0.0097 -0.0816 -0.0278  0.0612  0.0210  0.0707  0.0727  0.0490  0.0628  0.0222
-0.0512  0.0734 -0.0572  0.0523 -0.0215 -0.0179  0.0087 -0.0557 -0.0881  0.0311
-0.0146  0.0869 -0.0567  0.0872  0.0356  0.0958  0.0542  0.0167 -0.0873 -0.0697
 0.0840 -0.0936  0.0840  0.0134  0.0284  0.0583 -0.0464  0.0280 -0.0628 -0.0269

Columns 91 to 100
 0.0130  0.0989  0.0661  0.0257 -0.0271 -0.0215  0.0552 -0.0030  0.0154  0.0472
-0.0756  0.0574 -0.0248 -0.0956 -0.0506  0.0422 -0.0705  0.0825  0.0304 -0.0691
-0.0488 -0.0516  0.0344 -0.0601 -0.0543 -0.0473 -0.0139 -0.0719  0.0206 -0.0427
 0.0892  0.0530 -0.0614  0.0663 -0.0298 -0.0800 -0.0521  0.0577  0.0877  0.0297
-0.0226  0.0265 -0.0940  0.0366  0.0828 -0.0546  0.0223 -0.0116  0.0027 -0.0735
-0.0392  0.0334 -0.0034  0.0862  0.0883 -0.0642  0.0666  0.0401  0.0181  0.0205
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:02[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:02[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:02[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 5 [batchSize = 1]
 
2016-10-13 16:47:02[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.263640721639ms 
2016-10-13 16:47:02[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:02[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0550  0.0300  0.0813 -0.0745  0.0171  0.0437 -0.0806  0.0763 -0.0743  0.0713
 0.0743  0.0334 -0.0913 -0.0743  0.0384  0.0171 -0.0355 -0.0669 -0.0890 -0.0798
-0.0733 -0.0801  0.0696 -0.0024  0.0693 -0.0933  0.0313 -0.0538  0.0509  0.0375
-0.0271 -0.0215 -0.0259  0.0109  0.0791  0.0125 -0.0112  0.0561  0.0866  0.0200
 0.0627 -0.0568  0.0452  0.0231 -0.0065 -0.0612 -0.0473  0.0801  0.0427  0.0264
 0.0049 -0.0159 -0.0635 -0.0544  0.0820  0.1001 -0.0767  0.0335  0.0962 -0.0703

Columns 11 to 20
-0.0915 -0.0505 -0.0697  0.0428 -0.0893 -0.0581  0.0121 -0.0368  0.0583  0.0195
-0.0775  0.0516  0.0161  0.0913  0.0390 -0.0276  0.0334 -0.0858 -0.0022  0.0828
 0.0879 -0.0119  0.0325 -0.0712  0.0883  0.0577  0.0258 -0.0834  0.0989 -0.0610
-0.0956 -0.0691  0.0493  0.0014  0.0730  0.0047 -0.0762  0.0367  0.0097  0.0971
-0.0564  0.0323  0.0926 -0.0447 -0.0826 -0.0178 -0.0108  0.0579  0.0858 -0.0216
-0.0060  0.0961  0.1000 -0.0695 -0.0225  0.0223 -0.0246 -0.0130  0.0423  0.0120

Columns 21 to 30
-0.0487  0.0221  0.0448  0.0610 -0.0560  0.0508  0.0910 -0.0817  0.0617  0.0629
-0.0537 -0.0809 -0.0817  0.0053  0.0034 -0.0010  0.0056 -0.0148 -0.0924  0.0793
 0.0027 -0.0304 -0.0958 -0.0501 -0.0058  0.0958 -0.0582 -0.0965  0.0660 -0.0747
 0.0891  0.0326  0.0210 -0.0290 -0.0821  0.0329 -0.0654  0.0565  0.0150 -0.0380
 0.0585  0.0046 -0.0108  0.0342  0.0797 -0.0091 -0.0845 -0.0461 -0.0376  0.0260
 0.0668  0.0224 -0.0961 -0.0753 -0.0164 -0.0187 -0.0385 -0.0707  0.0458 -0.0659

Columns 31 to 40
 0.0124 -0.0051 -0.0675 -0.0168  0.0992 -0.0985  0.0837 -0.0374  0.0180  0.0241
 0.0246 -0.0634  0.0166 -0.0869  0.0634  0.0056  0.0204  0.0047  0.0489  0.0138
 0.0113 -0.0587  0.0213  0.0887 -0.0273 -0.0743 -0.0211  0.0345  0.0539  0.0567
-0.0998 -0.0228  0.0902 -0.0017 -0.0217  0.0632  0.0867 -0.0654  0.0318 -0.0447
-0.0363  0.0569  0.0256 -0.0933  0.0312  0.0988 -0.0150  0.0164  0.0567  0.0093
-0.0029  0.0020 -0.0096 -0.0861  0.0738 -0.0700  0.0377  0.0012  0.0238  0.0416

Columns 41 to 50
-0.0529  0.0049 -0.0878 -0.0506  0.0065 -0.0948  0.0614 -0.0599  0.0641  0.0961
 0.0840 -0.0004  0.0241  0.0207  0.0137  0.0353 -0.0676  0.0656 -0.0928 -0.0382
 0.0735  0.0568  0.0903  0.0896  0.0536  0.0814 -0.0639  0.0885 -0.0705 -0.0046
-0.0912  0.0464  0.0159  0.0010 -0.0228  0.0632  0.0330 -0.0363 -0.0351 -0.0146
-0.0406 -0.0359  0.0236  0.0550 -0.0047  0.0579 -0.0819  0.0215 -0.0053 -0.0996
-0.0641  0.0761 -0.0605  0.0101 -0.0064 -0.0561 -0.0979  0.0506  0.0797  0.0091

Columns 51 to 60
-0.0975  0.0970  0.0791  0.0145  0.0659 -0.0872 -0.0049  0.0538 -0.0261  0.0728
 0.0306  0.0441 -0.0122  0.0751 -0.0007 -0.0128 -0.0183  0.0964  0.0646 -0.0073
-0.0734 -0.0691  0.0685  0.0050  0.0466  0.0325 -0.0729  0.0779 -0.0217  0.0554
-0.0076 -0.0345  0.0308 -0.0673 -0.0975 -0.0916 -0.0344  0.0586  0.0423  0.0476
 0.0192 -0.0173  0.0308  0.0795 -0.0115  0.0172  0.0532  0.0580  0.0401  0.0628
 0.0233 -0.0736 -0.0472 -0.0168 -0.0551  0.0941 -0.0274  0.0629 -0.0419  0.0314

Columns 61 to 70
-0.0420  0.0493 -0.0380 -0.0930 -0.0603  0.0735 -0.0125  0.0551  0.0810 -0.0667
-0.0761  0.0061  0.0734  0.0551  0.0324 -0.0964 -0.0927  0.0366  0.0078  0.0426
-0.0972  0.0845 -0.0455  0.0913 -0.0376 -0.0220 -0.0959 -0.0513  0.0547 -0.0527
 0.0507 -0.0549  0.0010  0.0989 -0.0561 -0.0372 -0.0801  0.0624 -0.0956 -0.0313
 0.0853 -0.0149 -0.0673 -0.0429  0.0871  0.0716 -0.0273 -0.0439 -0.0333 -0.0952
-0.0127  0.0172 -0.0821  0.0014  0.0485  0.0865  0.0940  0.0145 -0.0188 -0.0255

Columns 71 to 80
-0.0356 -0.0328 -0.0727 -0.0355  0.0993 -0.0213  0.0480 -0.0495 -0.0207  0.0795
-0.0160 -0.0364 -0.0122 -0.0953 -0.0702  0.0979 -0.0538  0.0476  0.0470  0.0426
-0.0252  0.0282  0.0637  0.0065  0.0255 -0.0705 -0.0134  0.0207 -0.0722 -0.0929
 0.0531 -0.0181 -0.0401 -0.0742 -0.0042  0.0931 -0.0284 -0.0952  0.0976 -0.0049
-0.0250  0.0365 -0.0722 -0.0314  0.0493  0.0087  0.0513  0.0405  0.0777  0.0298
-0.0814 -0.0322  0.0180  0.0920  0.0209 -0.0872 -0.0740 -0.0645 -0.0789  0.0719

Columns 81 to 90
-0.0195 -0.0989  0.0805  0.0112  0.0578 -0.0945  0.0354 -0.0128 -0.0784  0.0082
-0.0901  0.0799 -0.0985  0.0631  0.0965 -0.0437 -0.0061  0.0509 -0.0437 -0.0147
-0.0097 -0.0816 -0.0278  0.0613  0.0209  0.0707  0.0727  0.0490  0.0628  0.0222
-0.0512  0.0734 -0.0571  0.0524 -0.0215 -0.0179  0.0087 -0.0557 -0.0881  0.0311
-0.0146  0.0869 -0.0567  0.0872  0.0356  0.0958  0.0542  0.0167 -0.0873 -0.0696
 0.0840 -0.0936  0.0840  0.0135  0.0284  0.0583 -0.0464  0.0280 -0.0628 -0.0269

Columns 91 to 100
 0.0130  0.0990  0.0661  0.0257 -0.0271 -0.0215  0.0552 -0.0030  0.0154  0.0472
-0.0756  0.0574 -0.0248 -0.0957 -0.0506  0.0422 -0.0705  0.0825  0.0304 -0.0691
-0.0488 -0.0516  0.0344 -0.0601 -0.0543 -0.0473 -0.0139 -0.0719  0.0206 -0.0426
 0.0892  0.0530 -0.0614  0.0663 -0.0299 -0.0800 -0.0521  0.0576  0.0876  0.0298
-0.0226  0.0265 -0.0940  0.0366  0.0828 -0.0546  0.0223 -0.0117  0.0026 -0.0735
-0.0392  0.0334 -0.0034  0.0862  0.0883 -0.0642  0.0666  0.0401  0.0181  0.0206
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:02[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:02[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:02[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 6 [batchSize = 1]
 
2016-10-13 16:47:02[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.293363571167ms 
2016-10-13 16:47:02[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:02[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0550  0.0299  0.0813 -0.0745  0.0171  0.0437 -0.0806  0.0763 -0.0743  0.0713
 0.0743  0.0333 -0.0912 -0.0742  0.0384  0.0172 -0.0355 -0.0668 -0.0890 -0.0798
-0.0733 -0.0801  0.0696 -0.0024  0.0693 -0.0933  0.0312 -0.0537  0.0508  0.0375
-0.0270 -0.0215 -0.0259  0.0110  0.0791  0.0127 -0.0112  0.0562  0.0866  0.0200
 0.0628 -0.0568  0.0452  0.0231 -0.0065 -0.0611 -0.0473  0.0801  0.0427  0.0264
 0.0049 -0.0159 -0.0635 -0.0544  0.0820  0.1002 -0.0767  0.0335  0.0962 -0.0703

Columns 11 to 20
-0.0915 -0.0505 -0.0696  0.0428 -0.0893 -0.0580  0.0120 -0.0368  0.0583  0.0195
-0.0775  0.0516  0.0161  0.0913  0.0390 -0.0276  0.0334 -0.0858 -0.0022  0.0828
 0.0879 -0.0118  0.0325 -0.0712  0.0883  0.0577  0.0258 -0.0834  0.0989 -0.0610
-0.0956 -0.0690  0.0493  0.0014  0.0730  0.0047 -0.0763  0.0367  0.0097  0.0970
-0.0564  0.0323  0.0926 -0.0447 -0.0826 -0.0178 -0.0108  0.0578  0.0858 -0.0216
-0.0060  0.0961  0.1000 -0.0695 -0.0225  0.0223 -0.0246 -0.0130  0.0423  0.0120

Columns 21 to 30
-0.0487  0.0221  0.0448  0.0610 -0.0560  0.0508  0.0910 -0.0817  0.0617  0.0629
-0.0537 -0.0809 -0.0817  0.0053  0.0034 -0.0010  0.0056 -0.0148 -0.0924  0.0793
 0.0027 -0.0304 -0.0958 -0.0501 -0.0058  0.0957 -0.0582 -0.0965  0.0660 -0.0747
 0.0890  0.0326  0.0211 -0.0290 -0.0821  0.0329 -0.0655  0.0565  0.0150 -0.0380
 0.0585  0.0046 -0.0108  0.0342  0.0797 -0.0091 -0.0845 -0.0461 -0.0376  0.0260
 0.0667  0.0224 -0.0961 -0.0753 -0.0164 -0.0187 -0.0386 -0.0706  0.0458 -0.0660

Columns 31 to 40
 0.0124 -0.0051 -0.0675 -0.0168  0.0992 -0.0985  0.0837 -0.0373  0.0180  0.0241
 0.0246 -0.0634  0.0166 -0.0869  0.0633  0.0056  0.0204  0.0047  0.0489  0.0138
 0.0112 -0.0587  0.0213  0.0886 -0.0273 -0.0743 -0.0212  0.0346  0.0539  0.0567
-0.0999 -0.0228  0.0902 -0.0017 -0.0218  0.0632  0.0867 -0.0654  0.0318 -0.0446
-0.0364  0.0570  0.0256 -0.0933  0.0312  0.0988 -0.0150  0.0164  0.0567  0.0094
-0.0029  0.0020 -0.0096 -0.0861  0.0738 -0.0700  0.0377  0.0012  0.0238  0.0416

Columns 41 to 50
-0.0529  0.0049 -0.0878 -0.0506  0.0065 -0.0948  0.0614 -0.0599  0.0640  0.0961
 0.0840 -0.0004  0.0241  0.0207  0.0137  0.0353 -0.0676  0.0656 -0.0929 -0.0382
 0.0736  0.0568  0.0903  0.0896  0.0536  0.0814 -0.0640  0.0885 -0.0706 -0.0046
-0.0912  0.0464  0.0159  0.0009 -0.0229  0.0633  0.0330 -0.0363 -0.0351 -0.0147
-0.0406 -0.0359  0.0236  0.0549 -0.0047  0.0580 -0.0819  0.0215 -0.0053 -0.0996
-0.0641  0.0761 -0.0605  0.0100 -0.0064 -0.0561 -0.0979  0.0506  0.0797  0.0091

Columns 51 to 60
-0.0975  0.0970  0.0791  0.0145  0.0660 -0.0872 -0.0050  0.0538 -0.0261  0.0728
 0.0307  0.0441 -0.0122  0.0751 -0.0006 -0.0128 -0.0183  0.0964  0.0647 -0.0073
-0.0734 -0.0691  0.0685  0.0050  0.0466  0.0325 -0.0730  0.0779 -0.0217  0.0554
-0.0076 -0.0345  0.0309 -0.0672 -0.0974 -0.0916 -0.0344  0.0586  0.0423  0.0476
 0.0192 -0.0173  0.0309  0.0795 -0.0115  0.0172  0.0531  0.0580  0.0401  0.0628
 0.0233 -0.0735 -0.0472 -0.0168 -0.0551  0.0941 -0.0274  0.0629 -0.0419  0.0314

Columns 61 to 70
-0.0420  0.0493 -0.0379 -0.0930 -0.0603  0.0735 -0.0125  0.0551  0.0810 -0.0667
-0.0761  0.0061  0.0734  0.0551  0.0324 -0.0964 -0.0927  0.0366  0.0079  0.0426
-0.0972  0.0845 -0.0455  0.0912 -0.0376 -0.0220 -0.0959 -0.0513  0.0547 -0.0528
 0.0508 -0.0548  0.0010  0.0989 -0.0561 -0.0372 -0.0801  0.0624 -0.0955 -0.0314
 0.0853 -0.0148 -0.0673 -0.0429  0.0871  0.0717 -0.0273 -0.0439 -0.0333 -0.0952
-0.0127  0.0172 -0.0821  0.0014  0.0485  0.0865  0.0940  0.0145 -0.0188 -0.0255

Columns 71 to 80
-0.0357 -0.0328 -0.0727 -0.0354  0.0992 -0.0212  0.0480 -0.0495 -0.0207  0.0795
-0.0160 -0.0364 -0.0122 -0.0953 -0.0702  0.0979 -0.0539  0.0476  0.0470  0.0426
-0.0252  0.0282  0.0637  0.0066  0.0255 -0.0704 -0.0135  0.0207 -0.0722 -0.0929
 0.0530 -0.0181 -0.0400 -0.0742 -0.0042  0.0932 -0.0285 -0.0952  0.0976 -0.0049
-0.0250  0.0365 -0.0722 -0.0314  0.0493  0.0087  0.0512  0.0405  0.0777  0.0298
-0.0814 -0.0322  0.0180  0.0920  0.0208 -0.0871 -0.0740 -0.0645 -0.0789  0.0719

Columns 81 to 90
-0.0196 -0.0989  0.0805  0.0112  0.0578 -0.0945  0.0354 -0.0128 -0.0784  0.0082
-0.0901  0.0799 -0.0985  0.0632  0.0965 -0.0437 -0.0061  0.0509 -0.0437 -0.0146
-0.0097 -0.0816 -0.0277  0.0613  0.0209  0.0707  0.0727  0.0490  0.0628  0.0222
-0.0512  0.0734 -0.0571  0.0526 -0.0216 -0.0179  0.0087 -0.0557 -0.0881  0.0311
-0.0146  0.0869 -0.0566  0.0872  0.0356  0.0958  0.0542  0.0167 -0.0873 -0.0696
 0.0840 -0.0936  0.0840  0.0135  0.0284  0.0583 -0.0464  0.0279 -0.0628 -0.0269

Columns 91 to 100
 0.0130  0.0990  0.0661  0.0257 -0.0271 -0.0215  0.0552 -0.0031  0.0154  0.0473
-0.0756  0.0574 -0.0248 -0.0957 -0.0505  0.0422 -0.0705  0.0824  0.0303 -0.0691
-0.0488 -0.0516  0.0344 -0.0601 -0.0543 -0.0473 -0.0139 -0.0720  0.0205 -0.0426
 0.0891  0.0530 -0.0614  0.0662 -0.0298 -0.0800 -0.0520  0.0576  0.0876  0.0298
-0.0226  0.0265 -0.0940  0.0366  0.0828 -0.0546  0.0223 -0.0117  0.0026 -0.0735
-0.0392  0.0334 -0.0034  0.0862  0.0883 -0.0642  0.0666  0.0401  0.0181  0.0206
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:02[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:02[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:02[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 7 [batchSize = 1]
 
2016-10-13 16:47:02[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.19434038798ms 
2016-10-13 16:47:02[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:02[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0550  0.0299  0.0813 -0.0745  0.0171  0.0438 -0.0806  0.0763 -0.0743  0.0713
 0.0743  0.0333 -0.0912 -0.0742  0.0384  0.0172 -0.0355 -0.0668 -0.0891 -0.0798
-0.0733 -0.0802  0.0696 -0.0024  0.0694 -0.0932  0.0312 -0.0537  0.0508  0.0375
-0.0270 -0.0217 -0.0259  0.0110  0.0791  0.0128 -0.0113  0.0562  0.0866  0.0200
 0.0628 -0.0569  0.0452  0.0232 -0.0065 -0.0611 -0.0473  0.0801  0.0426  0.0264
 0.0049 -0.0159 -0.0635 -0.0544  0.0820  0.1002 -0.0767  0.0335  0.0962 -0.0702

Columns 11 to 20
-0.0915 -0.0505 -0.0696  0.0428 -0.0893 -0.0580  0.0120 -0.0368  0.0583  0.0195
-0.0775  0.0516  0.0162  0.0912  0.0390 -0.0276  0.0334 -0.0859 -0.0022  0.0827
 0.0879 -0.0118  0.0325 -0.0712  0.0883  0.0577  0.0258 -0.0834  0.0989 -0.0611
-0.0955 -0.0690  0.0494  0.0013  0.0730  0.0047 -0.0763  0.0366  0.0097  0.0970
-0.0564  0.0323  0.0926 -0.0447 -0.0826 -0.0178 -0.0109  0.0578  0.0858 -0.0216
-0.0060  0.0961  0.1000 -0.0696 -0.0225  0.0223 -0.0246 -0.0130  0.0423  0.0119

Columns 21 to 30
-0.0487  0.0221  0.0448  0.0610 -0.0560  0.0508  0.0910 -0.0817  0.0617  0.0628
-0.0537 -0.0809 -0.0817  0.0054  0.0034 -0.0010  0.0056 -0.0147 -0.0924  0.0793
 0.0026 -0.0304 -0.0958 -0.0501 -0.0058  0.0957 -0.0582 -0.0965  0.0660 -0.0747
 0.0890  0.0327  0.0211 -0.0290 -0.0821  0.0329 -0.0656  0.0566  0.0150 -0.0380
 0.0584  0.0046 -0.0108  0.0342  0.0797 -0.0091 -0.0845 -0.0461 -0.0376  0.0260
 0.0667  0.0225 -0.0960 -0.0753 -0.0164 -0.0187 -0.0386 -0.0706  0.0458 -0.0660

Columns 31 to 40
 0.0123 -0.0051 -0.0675 -0.0168  0.0992 -0.0985  0.0836 -0.0373  0.0180  0.0242
 0.0246 -0.0634  0.0166 -0.0869  0.0633  0.0057  0.0203  0.0047  0.0489  0.0138
 0.0112 -0.0586  0.0213  0.0886 -0.0273 -0.0742 -0.0212  0.0346  0.0539  0.0567
-0.0999 -0.0227  0.0901 -0.0018 -0.0218  0.0633  0.0866 -0.0654  0.0317 -0.0446
-0.0364  0.0570  0.0255 -0.0933  0.0312  0.0988 -0.0151  0.0164  0.0567  0.0094
-0.0029  0.0020 -0.0096 -0.0861  0.0738 -0.0700  0.0376  0.0012  0.0238  0.0416

Columns 41 to 50
-0.0529  0.0049 -0.0877 -0.0506  0.0065 -0.0948  0.0613 -0.0599  0.0640  0.0960
 0.0840 -0.0003  0.0242  0.0206  0.0137  0.0353 -0.0676  0.0656 -0.0929 -0.0382
 0.0736  0.0569  0.0903  0.0895  0.0536  0.0814 -0.0640  0.0885 -0.0706 -0.0047
-0.0911  0.0465  0.0159  0.0008 -0.0228  0.0633  0.0329 -0.0362 -0.0352 -0.0147
-0.0406 -0.0359  0.0236  0.0549 -0.0047  0.0580 -0.0819  0.0215 -0.0053 -0.0997
-0.0641  0.0761 -0.0605  0.0100 -0.0064 -0.0561 -0.0979  0.0506  0.0797  0.0090

Columns 51 to 60
-0.0975  0.0970  0.0791  0.0146  0.0660 -0.0872 -0.0050  0.0539 -0.0260  0.0728
 0.0307  0.0441 -0.0121  0.0752 -0.0006 -0.0128 -0.0183  0.0964  0.0647 -0.0073
-0.0734 -0.0691  0.0686  0.0051  0.0466  0.0325 -0.0730  0.0780 -0.0216  0.0554
-0.0076 -0.0344  0.0310 -0.0671 -0.0974 -0.0916 -0.0345  0.0587  0.0424  0.0476
 0.0192 -0.0173  0.0309  0.0795 -0.0115  0.0172  0.0531  0.0580  0.0402  0.0628
 0.0233 -0.0735 -0.0471 -0.0167 -0.0550  0.0941 -0.0274  0.0629 -0.0419  0.0314

Columns 61 to 70
-0.0420  0.0494 -0.0380 -0.0931 -0.0603  0.0736 -0.0125  0.0551  0.0810 -0.0667
-0.0760  0.0062  0.0734  0.0550  0.0324 -0.0964 -0.0927  0.0366  0.0079  0.0425
-0.0972  0.0846 -0.0456  0.0912 -0.0376 -0.0220 -0.0959 -0.0513  0.0547 -0.0528
 0.0509 -0.0546  0.0010  0.0988 -0.0562 -0.0372 -0.0801  0.0625 -0.0954 -0.0315
 0.0854 -0.0148 -0.0673 -0.0430  0.0871  0.0717 -0.0273 -0.0439 -0.0332 -0.0952
-0.0127  0.0173 -0.0821  0.0014  0.0485  0.0865  0.0940  0.0145 -0.0188 -0.0255

Columns 71 to 80
-0.0357 -0.0328 -0.0727 -0.0354  0.0992 -0.0212  0.0480 -0.0495 -0.0207  0.0795
-0.0160 -0.0363 -0.0121 -0.0952 -0.0703  0.0980 -0.0539  0.0476  0.0470  0.0426
-0.0252  0.0283  0.0637  0.0066  0.0254 -0.0704 -0.0135  0.0207 -0.0722 -0.0929
 0.0530 -0.0180 -0.0400 -0.0741 -0.0043  0.0933 -0.0286 -0.0952  0.0976 -0.0049
-0.0250  0.0365 -0.0722 -0.0314  0.0493  0.0088  0.0512  0.0405  0.0777  0.0298
-0.0814 -0.0321  0.0180  0.0920  0.0208 -0.0871 -0.0740 -0.0644 -0.0789  0.0719

Columns 81 to 90
-0.0196 -0.0989  0.0805  0.0113  0.0578 -0.0945  0.0354 -0.0128 -0.0784  0.0083
-0.0901  0.0799 -0.0985  0.0632  0.0965 -0.0437 -0.0061  0.0509 -0.0437 -0.0146
-0.0098 -0.0816 -0.0277  0.0614  0.0209  0.0707  0.0727  0.0490  0.0628  0.0222
-0.0513  0.0734 -0.0571  0.0527 -0.0216 -0.0179  0.0087 -0.0557 -0.0881  0.0311
-0.0146  0.0869 -0.0566  0.0873  0.0356  0.0958  0.0542  0.0167 -0.0873 -0.0696
 0.0840 -0.0936  0.0841  0.0135  0.0284  0.0583 -0.0464  0.0279 -0.0628 -0.0269

Columns 91 to 100
 0.0129  0.0990  0.0661  0.0257 -0.0271 -0.0215  0.0552 -0.0031  0.0153  0.0473
-0.0756  0.0574 -0.0248 -0.0957 -0.0505  0.0422 -0.0705  0.0824  0.0303 -0.0690
-0.0488 -0.0515  0.0344 -0.0601 -0.0543 -0.0473 -0.0139 -0.0720  0.0205 -0.0426
 0.0891  0.0531 -0.0614  0.0661 -0.0298 -0.0800 -0.0520  0.0575  0.0875  0.0299
-0.0226  0.0266 -0.0940  0.0365  0.0828 -0.0546  0.0223 -0.0117  0.0026 -0.0734
-0.0392  0.0334 -0.0034  0.0861  0.0883 -0.0642  0.0666  0.0400  0.0180  0.0206
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:02[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:03[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:03[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 8 [batchSize = 1]
 
2016-10-13 16:47:03[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.172008514404ms 
2016-10-13 16:47:03[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:03[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0550  0.0299  0.0813 -0.0744  0.0171  0.0437 -0.0806  0.0763 -0.0744  0.0713
 0.0744  0.0332 -0.0912 -0.0742  0.0384  0.0173 -0.0355 -0.0668 -0.0891 -0.0798
-0.0732 -0.0802  0.0696 -0.0023  0.0694 -0.0932  0.0312 -0.0537  0.0508  0.0375
-0.0270 -0.0217 -0.0259  0.0111  0.0791  0.0128 -0.0113  0.0562  0.0865  0.0200
 0.0628 -0.0569  0.0452  0.0232 -0.0065 -0.0610 -0.0473  0.0801  0.0426  0.0264
 0.0050 -0.0160 -0.0635 -0.0543  0.0820  0.1002 -0.0767  0.0335  0.0962 -0.0702

Columns 11 to 20
-0.0915 -0.0505 -0.0696  0.0428 -0.0893 -0.0580  0.0120 -0.0368  0.0583  0.0195
-0.0775  0.0516  0.0162  0.0912  0.0390 -0.0276  0.0334 -0.0859 -0.0022  0.0827
 0.0879 -0.0118  0.0326 -0.0712  0.0882  0.0577  0.0258 -0.0835  0.0989 -0.0611
-0.0955 -0.0690  0.0493  0.0013  0.0730  0.0047 -0.0763  0.0366  0.0097  0.0969
-0.0564  0.0323  0.0927 -0.0447 -0.0827 -0.0177 -0.0109  0.0578  0.0858 -0.0217
-0.0059  0.0962  0.1001 -0.0696 -0.0225  0.0223 -0.0246 -0.0130  0.0423  0.0119

Columns 21 to 30
-0.0487  0.0221  0.0448  0.0610 -0.0560  0.0508  0.0910 -0.0817  0.0617  0.0629
-0.0538 -0.0809 -0.0817  0.0054  0.0034 -0.0010  0.0055 -0.0147 -0.0924  0.0793
 0.0026 -0.0304 -0.0958 -0.0501 -0.0058  0.0957 -0.0583 -0.0965  0.0660 -0.0747
 0.0890  0.0327  0.0211 -0.0289 -0.0821  0.0329 -0.0656  0.0567  0.0150 -0.0380
 0.0584  0.0046 -0.0108  0.0342  0.0797 -0.0091 -0.0846 -0.0461 -0.0376  0.0260
 0.0667  0.0225 -0.0960 -0.0753 -0.0164 -0.0187 -0.0386 -0.0706  0.0458 -0.0660

Columns 31 to 40
 0.0123 -0.0050 -0.0675 -0.0168  0.0992 -0.0985  0.0836 -0.0373  0.0180  0.0242
 0.0245 -0.0634  0.0166 -0.0869  0.0633  0.0057  0.0203  0.0047  0.0489  0.0138
 0.0112 -0.0586  0.0213  0.0886 -0.0273 -0.0742 -0.0213  0.0346  0.0539  0.0568
-0.1000 -0.0226  0.0901 -0.0018 -0.0218  0.0633  0.0866 -0.0654  0.0317 -0.0446
-0.0364  0.0570  0.0255 -0.0934  0.0312  0.0988 -0.0151  0.0164  0.0567  0.0094
-0.0029  0.0021 -0.0096 -0.0861  0.0738 -0.0700  0.0376  0.0013  0.0238  0.0416

Columns 41 to 50
-0.0529  0.0049 -0.0877 -0.0506  0.0065 -0.0948  0.0613 -0.0599  0.0640  0.0960
 0.0841 -0.0003  0.0242  0.0206  0.0137  0.0353 -0.0676  0.0657 -0.0929 -0.0383
 0.0736  0.0569  0.0903  0.0895  0.0536  0.0814 -0.0640  0.0885 -0.0706 -0.0047
-0.0911  0.0465  0.0159  0.0008 -0.0228  0.0633  0.0329 -0.0362 -0.0352 -0.0147
-0.0405 -0.0359  0.0237  0.0549 -0.0047  0.0580 -0.0820  0.0215 -0.0054 -0.0997
-0.0641  0.0761 -0.0605  0.0100 -0.0064 -0.0561 -0.0979  0.0507  0.0797  0.0090

Columns 51 to 60
-0.0975  0.0970  0.0791  0.0146  0.0660 -0.0872 -0.0050  0.0539 -0.0260  0.0728
 0.0307  0.0441 -0.0121  0.0752 -0.0006 -0.0128 -0.0183  0.0964  0.0647 -0.0073
-0.0734 -0.0691  0.0686  0.0051  0.0466  0.0325 -0.0730  0.0780 -0.0216  0.0554
-0.0076 -0.0344  0.0310 -0.0671 -0.0974 -0.0916 -0.0345  0.0586  0.0424  0.0476
 0.0192 -0.0173  0.0310  0.0796 -0.0115  0.0172  0.0531  0.0580  0.0402  0.0628
 0.0233 -0.0735 -0.0471 -0.0167 -0.0550  0.0941 -0.0274  0.0630 -0.0419  0.0314

Columns 61 to 70
-0.0420  0.0494 -0.0380 -0.0931 -0.0603  0.0735 -0.0125  0.0551  0.0810 -0.0667
-0.0760  0.0062  0.0734  0.0550  0.0324 -0.0964 -0.0927  0.0366  0.0079  0.0425
-0.0971  0.0846 -0.0456  0.0912 -0.0376 -0.0220 -0.0959 -0.0513  0.0547 -0.0529
 0.0509 -0.0546  0.0010  0.0987 -0.0562 -0.0372 -0.0801  0.0625 -0.0954 -0.0315
 0.0854 -0.0148 -0.0674 -0.0430  0.0871  0.0717 -0.0273 -0.0439 -0.0332 -0.0953
-0.0126  0.0173 -0.0821  0.0013  0.0485  0.0865  0.0940  0.0145 -0.0187 -0.0255

Columns 71 to 80
-0.0357 -0.0328 -0.0727 -0.0354  0.0992 -0.0212  0.0480 -0.0495 -0.0206  0.0795
-0.0160 -0.0363 -0.0121 -0.0952 -0.0703  0.0980 -0.0539  0.0476  0.0471  0.0426
-0.0252  0.0283  0.0637  0.0066  0.0254 -0.0703 -0.0135  0.0207 -0.0722 -0.0929
 0.0530 -0.0180 -0.0400 -0.0741 -0.0043  0.0933 -0.0286 -0.0952  0.0976 -0.0049
-0.0251  0.0365 -0.0722 -0.0314  0.0492  0.0088  0.0512  0.0405  0.0778  0.0298
-0.0815 -0.0321  0.0181  0.0920  0.0208 -0.0871 -0.0740 -0.0644 -0.0789  0.0719

Columns 81 to 90
-0.0196 -0.0989  0.0806  0.0113  0.0578 -0.0945  0.0354 -0.0128 -0.0784  0.0083
-0.0901  0.0799 -0.0984  0.0633  0.0965 -0.0437 -0.0062  0.0509 -0.0438 -0.0146
-0.0098 -0.0816 -0.0277  0.0614  0.0209  0.0707  0.0727  0.0490  0.0627  0.0222
-0.0513  0.0734 -0.0571  0.0528 -0.0216 -0.0179  0.0087 -0.0557 -0.0881  0.0311
-0.0146  0.0869 -0.0566  0.0873  0.0356  0.0958  0.0542  0.0167 -0.0873 -0.0696
 0.0840 -0.0936  0.0841  0.0136  0.0284  0.0583 -0.0464  0.0279 -0.0628 -0.0269

Columns 91 to 100
 0.0129  0.0990  0.0661  0.0257 -0.0271 -0.0215  0.0552 -0.0031  0.0153  0.0473
-0.0756  0.0574 -0.0248 -0.0957 -0.0505  0.0422 -0.0705  0.0824  0.0303 -0.0690
-0.0489 -0.0515  0.0344 -0.0602 -0.0542 -0.0473 -0.0139 -0.0720  0.0205 -0.0426
 0.0890  0.0531 -0.0614  0.0661 -0.0297 -0.0800 -0.0520  0.0575  0.0875  0.0299
-0.0226  0.0266 -0.0940  0.0365  0.0828 -0.0546  0.0224 -0.0117  0.0026 -0.0734
-0.0392  0.0334 -0.0034  0.0861  0.0883 -0.0642  0.0666  0.0400  0.0180  0.0206
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:03[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:03[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:03[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 9 [batchSize = 1]
 
2016-10-13 16:47:03[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.347007751465ms 
2016-10-13 16:47:03[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:03[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0550  0.0299  0.0813 -0.0744  0.0171  0.0437 -0.0806  0.0763 -0.0744  0.0713
 0.0744  0.0332 -0.0912 -0.0741  0.0384  0.0173 -0.0355 -0.0668 -0.0891 -0.0798
-0.0732 -0.0803  0.0696 -0.0023  0.0694 -0.0932  0.0312 -0.0537  0.0508  0.0375
-0.0269 -0.0217 -0.0259  0.0111  0.0791  0.0128 -0.0113  0.0563  0.0864  0.0200
 0.0628 -0.0569  0.0452  0.0232 -0.0065 -0.0610 -0.0473  0.0802  0.0426  0.0264
 0.0050 -0.0160 -0.0635 -0.0543  0.0820  0.1002 -0.0767  0.0335  0.0962 -0.0702

Columns 11 to 20
-0.0915 -0.0505 -0.0696  0.0427 -0.0893 -0.0580  0.0120 -0.0368  0.0583  0.0195
-0.0775  0.0517  0.0162  0.0912  0.0390 -0.0276  0.0334 -0.0859 -0.0022  0.0827
 0.0879 -0.0118  0.0326 -0.0713  0.0882  0.0577  0.0258 -0.0835  0.0989 -0.0611
-0.0955 -0.0690  0.0494  0.0013  0.0730  0.0048 -0.0763  0.0365  0.0097  0.0969
-0.0564  0.0323  0.0927 -0.0447 -0.0827 -0.0177 -0.0109  0.0578  0.0858 -0.0217
-0.0059  0.0962  0.1001 -0.0696 -0.0225  0.0223 -0.0246 -0.0131  0.0423  0.0119

Columns 21 to 30
-0.0487  0.0221  0.0448  0.0610 -0.0560  0.0508  0.0910 -0.0817  0.0617  0.0628
-0.0538 -0.0809 -0.0817  0.0054  0.0034 -0.0010  0.0055 -0.0147 -0.0924  0.0792
 0.0026 -0.0304 -0.0958 -0.0501 -0.0058  0.0957 -0.0583 -0.0965  0.0660 -0.0748
 0.0890  0.0327  0.0211 -0.0289 -0.0821  0.0329 -0.0656  0.0567  0.0150 -0.0381
 0.0584  0.0046 -0.0108  0.0343  0.0797 -0.0091 -0.0846 -0.0461 -0.0377  0.0260
 0.0667  0.0225 -0.0960 -0.0753 -0.0164 -0.0188 -0.0386 -0.0706  0.0458 -0.0660

Columns 31 to 40
 0.0123 -0.0050 -0.0675 -0.0168  0.0992 -0.0985  0.0836 -0.0373  0.0180  0.0242
 0.0245 -0.0633  0.0166 -0.0870  0.0633  0.0057  0.0202  0.0048  0.0489  0.0138
 0.0111 -0.0586  0.0212  0.0885 -0.0273 -0.0742 -0.0213  0.0346  0.0538  0.0568
-0.1001 -0.0226  0.0901 -0.0019 -0.0218  0.0633  0.0865 -0.0653  0.0317 -0.0446
-0.0364  0.0570  0.0255 -0.0934  0.0312  0.0988 -0.0151  0.0165  0.0567  0.0094
-0.0029  0.0021 -0.0097 -0.0862  0.0738 -0.0700  0.0376  0.0013  0.0237  0.0416

Columns 41 to 50
-0.0529  0.0049 -0.0877 -0.0506  0.0065 -0.0948  0.0613 -0.0598  0.0640  0.0960
 0.0841 -0.0003  0.0242  0.0205  0.0137  0.0354 -0.0677  0.0657 -0.0930 -0.0383
 0.0736  0.0569  0.0904  0.0895  0.0536  0.0814 -0.0640  0.0885 -0.0707 -0.0047
-0.0910  0.0465  0.0160  0.0007 -0.0228  0.0633  0.0329 -0.0362 -0.0353 -0.0148
-0.0405 -0.0358  0.0237  0.0548 -0.0047  0.0580 -0.0820  0.0215 -0.0054 -0.0998
-0.0641  0.0761 -0.0605  0.0100 -0.0064 -0.0561 -0.0980  0.0507  0.0796  0.0090

Columns 51 to 60
-0.0975  0.0970  0.0791  0.0146  0.0660 -0.0872 -0.0050  0.0539 -0.0260  0.0728
 0.0307  0.0441 -0.0121  0.0752 -0.0006 -0.0129 -0.0183  0.0965  0.0648 -0.0073
-0.0734 -0.0690  0.0687  0.0051  0.0466  0.0325 -0.0730  0.0780 -0.0216  0.0554
-0.0076 -0.0344  0.0311 -0.0670 -0.0973 -0.0917 -0.0345  0.0587  0.0424  0.0476
 0.0192 -0.0173  0.0310  0.0796 -0.0115  0.0172  0.0531  0.0581  0.0402  0.0628
 0.0233 -0.0735 -0.0471 -0.0167 -0.0550  0.0941 -0.0274  0.0630 -0.0419  0.0314

Columns 61 to 70
-0.0420  0.0494 -0.0380 -0.0931 -0.0604  0.0736 -0.0125  0.0551  0.0810 -0.0667
-0.0760  0.0062  0.0734  0.0550  0.0324 -0.0964 -0.0927  0.0366  0.0080  0.0425
-0.0971  0.0847 -0.0456  0.0911 -0.0376 -0.0220 -0.0959 -0.0512  0.0548 -0.0529
 0.0509 -0.0545  0.0009  0.0987 -0.0562 -0.0371 -0.0801  0.0625 -0.0953 -0.0315
 0.0854 -0.0147 -0.0674 -0.0430  0.0871  0.0717 -0.0273 -0.0439 -0.0332 -0.0953
-0.0126  0.0173 -0.0822  0.0013  0.0484  0.0865  0.0940  0.0145 -0.0187 -0.0256

Columns 71 to 80
-0.0357 -0.0328 -0.0727 -0.0354  0.0992 -0.0212  0.0479 -0.0495 -0.0206  0.0795
-0.0160 -0.0363 -0.0121 -0.0952 -0.0703  0.0981 -0.0540  0.0476  0.0471  0.0426
-0.0252  0.0283  0.0637  0.0066  0.0254 -0.0703 -0.0136  0.0207 -0.0722 -0.0929
 0.0529 -0.0180 -0.0399 -0.0740 -0.0043  0.0933 -0.0286 -0.0952  0.0977 -0.0049
-0.0251  0.0366 -0.0722 -0.0313  0.0492  0.0088  0.0512  0.0405  0.0778  0.0298
-0.0815 -0.0321  0.0181  0.0920  0.0208 -0.0870 -0.0741 -0.0644 -0.0789  0.0719

Columns 81 to 90
-0.0196 -0.0989  0.0806  0.0114  0.0578 -0.0945  0.0354 -0.0128 -0.0784  0.0083
-0.0901  0.0799 -0.0984  0.0633  0.0965 -0.0437 -0.0062  0.0509 -0.0438 -0.0146
-0.0098 -0.0816 -0.0277  0.0615  0.0209  0.0707  0.0727  0.0490  0.0627  0.0222
-0.0513  0.0734 -0.0570  0.0529 -0.0217 -0.0179  0.0087 -0.0557 -0.0881  0.0311
-0.0147  0.0869 -0.0566  0.0874  0.0356  0.0958  0.0542  0.0167 -0.0873 -0.0696
 0.0840 -0.0936  0.0841  0.0136  0.0284  0.0583 -0.0464  0.0279 -0.0628 -0.0269

Columns 91 to 100
 0.0129  0.0990  0.0661  0.0257 -0.0271 -0.0215  0.0553 -0.0031  0.0153  0.0473
-0.0756  0.0575 -0.0248 -0.0958 -0.0505  0.0422 -0.0704  0.0824  0.0302 -0.0690
-0.0489 -0.0515  0.0344 -0.0602 -0.0542 -0.0473 -0.0138 -0.0720  0.0204 -0.0425
 0.0890  0.0531 -0.0614  0.0661 -0.0297 -0.0800 -0.0519  0.0575  0.0874  0.0299
-0.0226  0.0266 -0.0940  0.0365  0.0829 -0.0546  0.0224 -0.0117  0.0025 -0.0734
-0.0392  0.0334 -0.0034  0.0861  0.0883 -0.0642  0.0666  0.0400  0.0180  0.0206
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:03[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:03[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:03[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 10 [batchSize = 1]
 
2016-10-13 16:47:03[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.537345250448ms 
2016-10-13 16:47:03[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> saving network model to /home/robotec/catkin_ws/src/superchicko/farnn/src/network/glassfurnace_lstm-net.t7 
2016-10-13 16:47:03[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:03[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0550  0.0299  0.0813 -0.0744  0.0171  0.0438 -0.0806  0.0764 -0.0744  0.0713
 0.0744  0.0332 -0.0912 -0.0741  0.0384  0.0173 -0.0355 -0.0667 -0.0891 -0.0798
-0.0732 -0.0803  0.0696 -0.0022  0.0694 -0.0931  0.0312 -0.0536  0.0507  0.0375
-0.0269 -0.0218 -0.0259  0.0112  0.0791  0.0130 -0.0113  0.0563  0.0863  0.0200
 0.0628 -0.0569  0.0452  0.0233 -0.0065 -0.0610 -0.0473  0.0802  0.0426  0.0264
 0.0050 -0.0160 -0.0635 -0.0543  0.0820  0.1003 -0.0767  0.0335  0.0961 -0.0702

Columns 11 to 20
-0.0915 -0.0505 -0.0696  0.0427 -0.0893 -0.0580  0.0120 -0.0369  0.0583  0.0194
-0.0775  0.0517  0.0162  0.0912  0.0390 -0.0276  0.0334 -0.0859 -0.0022  0.0827
 0.0879 -0.0118  0.0326 -0.0713  0.0882  0.0577  0.0258 -0.0835  0.0989 -0.0611
-0.0955 -0.0689  0.0494  0.0013  0.0729  0.0048 -0.0763  0.0365  0.0097  0.0968
-0.0563  0.0323  0.0927 -0.0447 -0.0827 -0.0177 -0.0109  0.0578  0.0858 -0.0217
-0.0059  0.0962  0.1001 -0.0696 -0.0225  0.0223 -0.0246 -0.0131  0.0423  0.0119

Columns 21 to 30
-0.0487  0.0221  0.0448  0.0611 -0.0560  0.0508  0.0909 -0.0816  0.0617  0.0628
-0.0538 -0.0809 -0.0817  0.0054  0.0034 -0.0010  0.0054 -0.0147 -0.0924  0.0792
 0.0026 -0.0304 -0.0958 -0.0500 -0.0058  0.0957 -0.0583 -0.0964  0.0660 -0.0748
 0.0889  0.0327  0.0211 -0.0289 -0.0822  0.0328 -0.0657  0.0568  0.0150 -0.0381
 0.0584  0.0046 -0.0108  0.0343  0.0797 -0.0091 -0.0846 -0.0460 -0.0377  0.0259
 0.0667  0.0225 -0.0960 -0.0752 -0.0164 -0.0188 -0.0387 -0.0706  0.0458 -0.0660

Columns 31 to 40
 0.0123 -0.0050 -0.0675 -0.0169  0.0992 -0.0985  0.0836 -0.0373  0.0179  0.0242
 0.0245 -0.0633  0.0166 -0.0870  0.0633  0.0057  0.0202  0.0048  0.0489  0.0138
 0.0111 -0.0586  0.0212  0.0885 -0.0273 -0.0742 -0.0213  0.0347  0.0538  0.0568
-0.1002 -0.0226  0.0901 -0.0020 -0.0219  0.0633  0.0864 -0.0652  0.0317 -0.0445
-0.0365  0.0570  0.0255 -0.0934  0.0312  0.0988 -0.0152  0.0165  0.0567  0.0094
-0.0030  0.0021 -0.0097 -0.0862  0.0738 -0.0700  0.0376  0.0013  0.0237  0.0416

Columns 41 to 50
-0.0529  0.0049 -0.0877 -0.0507  0.0065 -0.0948  0.0613 -0.0598  0.0640  0.0960
 0.0841 -0.0003  0.0242  0.0205  0.0137  0.0354 -0.0677  0.0657 -0.0930 -0.0383
 0.0736  0.0569  0.0904  0.0894  0.0536  0.0814 -0.0640  0.0885 -0.0707 -0.0048
-0.0910  0.0465  0.0161  0.0006 -0.0228  0.0633  0.0328 -0.0362 -0.0353 -0.0149
-0.0405 -0.0358  0.0237  0.0548 -0.0047  0.0580 -0.0820  0.0215 -0.0054 -0.0998
-0.0641  0.0761 -0.0605  0.0099 -0.0064 -0.0561 -0.0980  0.0507  0.0796  0.0090

Columns 51 to 60
-0.0975  0.0970  0.0792  0.0146  0.0660 -0.0872 -0.0050  0.0539 -0.0260  0.0728
 0.0307  0.0442 -0.0120  0.0752 -0.0005 -0.0129 -0.0184  0.0965  0.0648 -0.0073
-0.0734 -0.0690  0.0687  0.0051  0.0467  0.0325 -0.0730  0.0780 -0.0215  0.0554
-0.0076 -0.0343  0.0312 -0.0669 -0.0973 -0.0917 -0.0346  0.0587  0.0425  0.0476
 0.0192 -0.0173  0.0310  0.0796 -0.0114  0.0171  0.0531  0.0581  0.0402  0.0628
 0.0233 -0.0735 -0.0471 -0.0167 -0.0550  0.0941 -0.0274  0.0630 -0.0418  0.0314

Columns 61 to 70
-0.0420  0.0494 -0.0380 -0.0931 -0.0604  0.0736 -0.0125  0.0551  0.0810 -0.0667
-0.0759  0.0063  0.0734  0.0549  0.0324 -0.0964 -0.0927  0.0366  0.0080  0.0424
-0.0971  0.0847 -0.0456  0.0911 -0.0376 -0.0220 -0.0959 -0.0512  0.0548 -0.0529
 0.0510 -0.0544  0.0009  0.0986 -0.0562 -0.0371 -0.0801  0.0625 -0.0953 -0.0316
 0.0854 -0.0147 -0.0674 -0.0430  0.0871  0.0717 -0.0273 -0.0439 -0.0332 -0.0953
-0.0126  0.0173 -0.0822  0.0013  0.0484  0.0865  0.0940  0.0145 -0.0187 -0.0256

Columns 71 to 80
-0.0357 -0.0328 -0.0727 -0.0354  0.0992 -0.0211  0.0479 -0.0495 -0.0206  0.0795
-0.0161 -0.0363 -0.0121 -0.0952 -0.0703  0.0981 -0.0540  0.0476  0.0471  0.0426
-0.0252  0.0283  0.0638  0.0066  0.0254 -0.0703 -0.0136  0.0207 -0.0721 -0.0929
 0.0529 -0.0179 -0.0399 -0.0740 -0.0044  0.0935 -0.0287 -0.0952  0.0977 -0.0050
-0.0251  0.0366 -0.0722 -0.0313  0.0492  0.0089  0.0511  0.0405  0.0778  0.0298
-0.0815 -0.0321  0.0181  0.0920  0.0208 -0.0870 -0.0741 -0.0644 -0.0789  0.0719

Columns 81 to 90
-0.0196 -0.0989  0.0806  0.0114  0.0578 -0.0945  0.0354 -0.0128 -0.0784  0.0083
-0.0901  0.0799 -0.0984  0.0633  0.0965 -0.0437 -0.0062  0.0509 -0.0438 -0.0146
-0.0098 -0.0816 -0.0277  0.0615  0.0209  0.0707  0.0726  0.0490  0.0627  0.0222
-0.0514  0.0734 -0.0570  0.0530 -0.0217 -0.0178  0.0086 -0.0558 -0.0882  0.0311
-0.0147  0.0869 -0.0566  0.0874  0.0356  0.0958  0.0541  0.0167 -0.0874 -0.0696
 0.0840 -0.0936  0.0841  0.0136  0.0284  0.0583 -0.0464  0.0279 -0.0628 -0.0269

Columns 91 to 100
 0.0129  0.0990  0.0661  0.0257 -0.0271 -0.0215  0.0553 -0.0031  0.0153  0.0473
-0.0757  0.0575 -0.0248 -0.0958 -0.0505  0.0422 -0.0704  0.0823  0.0302 -0.0690
-0.0489 -0.0515  0.0344 -0.0602 -0.0542 -0.0473 -0.0138 -0.0721  0.0204 -0.0425
 0.0890  0.0531 -0.0614  0.0661 -0.0297 -0.0800 -0.0519  0.0574  0.0873  0.0300
-0.0226  0.0266 -0.0940  0.0365  0.0829 -0.0546  0.0224 -0.0117  0.0025 -0.0734
-0.0393  0.0334 -0.0034  0.0861  0.0884 -0.0642  0.0666  0.0400  0.0180  0.0206
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:03[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:03[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:03[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 11 [batchSize = 1]
 
2016-10-13 16:47:03[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.635653177897ms 
2016-10-13 16:47:03[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:03[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0550  0.0299  0.0813 -0.0744  0.0172  0.0438 -0.0806  0.0764 -0.0744  0.0713
 0.0744  0.0332 -0.0912 -0.0741  0.0384  0.0174 -0.0355 -0.0667 -0.0892 -0.0798
-0.0732 -0.0803  0.0696 -0.0022  0.0694 -0.0931  0.0312 -0.0536  0.0507  0.0375
-0.0269 -0.0218 -0.0259  0.0112  0.0791  0.0130 -0.0113  0.0564  0.0863  0.0200
 0.0628 -0.0570  0.0452  0.0233 -0.0065 -0.0609 -0.0473  0.0802  0.0425  0.0264
 0.0050 -0.0160 -0.0635 -0.0543  0.0820  0.1003 -0.0767  0.0336  0.0961 -0.0702

Columns 11 to 20
-0.0915 -0.0505 -0.0696  0.0427 -0.0893 -0.0580  0.0120 -0.0369  0.0583  0.0194
-0.0775  0.0517  0.0163  0.0912  0.0390 -0.0276  0.0334 -0.0860 -0.0022  0.0826
 0.0880 -0.0118  0.0326 -0.0713  0.0882  0.0578  0.0258 -0.0835  0.0989 -0.0612
-0.0954 -0.0689  0.0494  0.0013  0.0729  0.0048 -0.0764  0.0364  0.0097  0.0968
-0.0563  0.0323  0.0927 -0.0448 -0.0827 -0.0177 -0.0109  0.0577  0.0858 -0.0217
-0.0059  0.0962  0.1001 -0.0696 -0.0225  0.0223 -0.0246 -0.0131  0.0423  0.0119

Columns 21 to 30
-0.0487  0.0221  0.0448  0.0611 -0.0560  0.0508  0.0909 -0.0816  0.0617  0.0628
-0.0538 -0.0809 -0.0817  0.0054  0.0034 -0.0010  0.0054 -0.0146 -0.0924  0.0792
 0.0026 -0.0304 -0.0958 -0.0500 -0.0058  0.0957 -0.0584 -0.0964  0.0660 -0.0748
 0.0889  0.0327  0.0211 -0.0289 -0.0822  0.0328 -0.0658  0.0568  0.0150 -0.0382
 0.0584  0.0046 -0.0108  0.0343  0.0797 -0.0091 -0.0846 -0.0460 -0.0377  0.0259
 0.0667  0.0225 -0.0960 -0.0752 -0.0164 -0.0188 -0.0387 -0.0706  0.0458 -0.0660

Columns 31 to 40
 0.0123 -0.0050 -0.0675 -0.0169  0.0992 -0.0985  0.0836 -0.0373  0.0179  0.0242
 0.0244 -0.0633  0.0165 -0.0870  0.0633  0.0057  0.0202  0.0048  0.0488  0.0138
 0.0111 -0.0586  0.0212  0.0885 -0.0273 -0.0742 -0.0214  0.0347  0.0538  0.0568
-0.1002 -0.0225  0.0900 -0.0020 -0.0219  0.0633  0.0864 -0.0652  0.0316 -0.0445
-0.0365  0.0571  0.0255 -0.0934  0.0312  0.0988 -0.0152  0.0165  0.0567  0.0094
-0.0030  0.0021 -0.0097 -0.0862  0.0738 -0.0699  0.0375  0.0013  0.0237  0.0416

Columns 41 to 50
-0.0529  0.0049 -0.0877 -0.0506  0.0065 -0.0948  0.0613 -0.0598  0.0640  0.0960
 0.0841 -0.0003  0.0242  0.0205  0.0137  0.0354 -0.0677  0.0657 -0.0930 -0.0384
 0.0737  0.0569  0.0904  0.0894  0.0536  0.0814 -0.0640  0.0885 -0.0707 -0.0048
-0.0910  0.0465  0.0161  0.0006 -0.0228  0.0633  0.0328 -0.0362 -0.0354 -0.0150
-0.0405 -0.0358  0.0237  0.0548 -0.0047  0.0580 -0.0820  0.0215 -0.0054 -0.0998
-0.0640  0.0761 -0.0604  0.0099 -0.0064 -0.0561 -0.0980  0.0507  0.0796  0.0089

Columns 51 to 60
-0.0975  0.0970  0.0792  0.0146  0.0660 -0.0872 -0.0050  0.0539 -0.0260  0.0728
 0.0307  0.0442 -0.0120  0.0753 -0.0005 -0.0129 -0.0184  0.0965  0.0648 -0.0073
-0.0733 -0.0690  0.0687  0.0052  0.0467  0.0325 -0.0731  0.0780 -0.0215  0.0554
-0.0076 -0.0343  0.0312 -0.0669 -0.0972 -0.0917 -0.0346  0.0588  0.0425  0.0476
 0.0192 -0.0173  0.0311  0.0796 -0.0114  0.0171  0.0530  0.0581  0.0403  0.0628
 0.0233 -0.0735 -0.0470 -0.0167 -0.0550  0.0941 -0.0274  0.0630 -0.0418  0.0314

Columns 61 to 70
-0.0420  0.0494 -0.0380 -0.0931 -0.0604  0.0736 -0.0125  0.0551  0.0810 -0.0668
-0.0759  0.0063  0.0734  0.0549  0.0324 -0.0964 -0.0927  0.0366  0.0080  0.0424
-0.0971  0.0847 -0.0456  0.0911 -0.0376 -0.0220 -0.0959 -0.0512  0.0548 -0.0529
 0.0510 -0.0545  0.0009  0.0986 -0.0562 -0.0371 -0.0801  0.0625 -0.0952 -0.0316
 0.0854 -0.0147 -0.0674 -0.0431  0.0870  0.0717 -0.0273 -0.0438 -0.0331 -0.0953
-0.0126  0.0173 -0.0822  0.0013  0.0484  0.0865  0.0940  0.0145 -0.0187 -0.0256

Columns 71 to 80
-0.0357 -0.0328 -0.0727 -0.0354  0.0992 -0.0211  0.0479 -0.0495 -0.0206  0.0795
-0.0161 -0.0363 -0.0121 -0.0952 -0.0703  0.0982 -0.0540  0.0476  0.0471  0.0425
-0.0253  0.0283  0.0638  0.0067  0.0254 -0.0702 -0.0136  0.0207 -0.0721 -0.0929
 0.0529 -0.0179 -0.0399 -0.0740 -0.0044  0.0935 -0.0287 -0.0952  0.0977 -0.0050
-0.0251  0.0366 -0.0721 -0.0313  0.0492  0.0089  0.0511  0.0405  0.0778  0.0298
-0.0815 -0.0321  0.0181  0.0920  0.0208 -0.0870 -0.0741 -0.0644 -0.0789  0.0719

Columns 81 to 90
-0.0196 -0.0989  0.0806  0.0114  0.0578 -0.0945  0.0354 -0.0128 -0.0784  0.0083
-0.0902  0.0799 -0.0984  0.0634  0.0964 -0.0437 -0.0062  0.0509 -0.0438 -0.0146
-0.0098 -0.0816 -0.0276  0.0615  0.0209  0.0707  0.0726  0.0490  0.0627  0.0222
-0.0514  0.0734 -0.0569  0.0530 -0.0217 -0.0178  0.0086 -0.0558 -0.0882  0.0312
-0.0147  0.0869 -0.0566  0.0874  0.0356  0.0958  0.0541  0.0167 -0.0874 -0.0696
 0.0839 -0.0936  0.0841  0.0136  0.0283  0.0583 -0.0464  0.0279 -0.0628 -0.0269

Columns 91 to 100
 0.0129  0.0990  0.0661  0.0257 -0.0271 -0.0215  0.0553 -0.0031  0.0153  0.0473
-0.0757  0.0575 -0.0248 -0.0958 -0.0505  0.0422 -0.0704  0.0823  0.0301 -0.0690
-0.0489 -0.0515  0.0344 -0.0602 -0.0542 -0.0473 -0.0138 -0.0721  0.0204 -0.0425
 0.0889  0.0532 -0.0614  0.0660 -0.0297 -0.0800 -0.0518  0.0574  0.0873  0.0300
-0.0226  0.0266 -0.0940  0.0365  0.0829 -0.0546  0.0224 -0.0118  0.0024 -0.0733
-0.0393  0.0335 -0.0034  0.0861  0.0884 -0.0642  0.0666  0.0400  0.0179  0.0207
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:03[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:03[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:03[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 12 [batchSize = 1]
 
2016-10-13 16:47:04[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.410347620646ms 
2016-10-13 16:47:04[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:04[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0550  0.0298  0.0813 -0.0744  0.0172  0.0438 -0.0806  0.0764 -0.0745  0.0713
 0.0744  0.0331 -0.0912 -0.0740  0.0384  0.0175 -0.0356 -0.0667 -0.0892 -0.0798
-0.0732 -0.0803  0.0696 -0.0022  0.0694 -0.0930  0.0312 -0.0536  0.0507  0.0375
-0.0269 -0.0219 -0.0259  0.0113  0.0791  0.0131 -0.0114  0.0564  0.0862  0.0201
 0.0629 -0.0570  0.0452  0.0233 -0.0065 -0.0609 -0.0474  0.0802  0.0425  0.0264
 0.0050 -0.0160 -0.0635 -0.0543  0.0820  0.1004 -0.0768  0.0336  0.0961 -0.0702

Columns 11 to 20
-0.0915 -0.0504 -0.0696  0.0427 -0.0894 -0.0580  0.0120 -0.0369  0.0583  0.0194
-0.0775  0.0517  0.0163  0.0912  0.0390 -0.0276  0.0334 -0.0860 -0.0022  0.0826
 0.0880 -0.0117  0.0327 -0.0713  0.0882  0.0578  0.0258 -0.0835  0.0989 -0.0612
-0.0954 -0.0688  0.0495  0.0013  0.0729  0.0048 -0.0764  0.0364  0.0097  0.0967
-0.0563  0.0324  0.0927 -0.0448 -0.0827 -0.0177 -0.0109  0.0577  0.0858 -0.0218
-0.0059  0.0962  0.1001 -0.0696 -0.0225  0.0223 -0.0246 -0.0131  0.0422  0.0118

Columns 21 to 30
-0.0488  0.0222  0.0448  0.0611 -0.0560  0.0508  0.0909 -0.0816  0.0617  0.0628
-0.0538 -0.0809 -0.0817  0.0054  0.0034 -0.0010  0.0054 -0.0146 -0.0924  0.0792
 0.0026 -0.0304 -0.0958 -0.0500 -0.0058  0.0957 -0.0584 -0.0964  0.0659 -0.0748
 0.0888  0.0327  0.0212 -0.0288 -0.0822  0.0328 -0.0659  0.0569  0.0150 -0.0382
 0.0584  0.0046 -0.0107  0.0343  0.0797 -0.0091 -0.0847 -0.0460 -0.0377  0.0259
 0.0667  0.0225 -0.0960 -0.0752 -0.0164 -0.0188 -0.0387 -0.0705  0.0458 -0.0661

Columns 31 to 40
 0.0123 -0.0050 -0.0675 -0.0169  0.0992 -0.0985  0.0835 -0.0372  0.0179  0.0242
 0.0244 -0.0633  0.0165 -0.0870  0.0633  0.0057  0.0201  0.0048  0.0488  0.0138
 0.0111 -0.0585  0.0212  0.0885 -0.0273 -0.0742 -0.0214  0.0347  0.0538  0.0568
-0.1002 -0.0225  0.0900 -0.0020 -0.0219  0.0634  0.0863 -0.0651  0.0316 -0.0445
-0.0365  0.0571  0.0255 -0.0934  0.0311  0.0989 -0.0152  0.0165  0.0566  0.0094
-0.0030  0.0021 -0.0097 -0.0862  0.0738 -0.0699  0.0375  0.0013  0.0237  0.0416

Columns 41 to 50
-0.0528  0.0049 -0.0877 -0.0506  0.0065 -0.0948  0.0613 -0.0598  0.0639  0.0959
 0.0841 -0.0003  0.0243  0.0205  0.0137  0.0354 -0.0677  0.0657 -0.0930 -0.0384
 0.0737  0.0569  0.0904  0.0894  0.0536  0.0814 -0.0640  0.0885 -0.0708 -0.0049
-0.0909  0.0465  0.0161  0.0006 -0.0228  0.0633  0.0328 -0.0362 -0.0354 -0.0151
-0.0405 -0.0358  0.0238  0.0548 -0.0047  0.0580 -0.0820  0.0215 -0.0055 -0.0999
-0.0640  0.0761 -0.0604  0.0099 -0.0064 -0.0561 -0.0980  0.0507  0.0796  0.0089

Columns 51 to 60
-0.0975  0.0970  0.0792  0.0146  0.0660 -0.0872 -0.0050  0.0539 -0.0260  0.0728
 0.0307  0.0442 -0.0119  0.0753 -0.0005 -0.0129 -0.0184  0.0965  0.0648 -0.0073
-0.0733 -0.0690  0.0688  0.0052  0.0467  0.0325 -0.0731  0.0781 -0.0215  0.0554
-0.0076 -0.0343  0.0313 -0.0669 -0.0972 -0.0917 -0.0346  0.0588  0.0426  0.0476
 0.0192 -0.0173  0.0311  0.0797 -0.0114  0.0171  0.0530  0.0581  0.0403  0.0628
 0.0233 -0.0735 -0.0470 -0.0166 -0.0549  0.0941 -0.0275  0.0630 -0.0418  0.0314

Columns 61 to 70
-0.0420  0.0494 -0.0380 -0.0931 -0.0604  0.0736 -0.0125  0.0551  0.0811 -0.0668
-0.0759  0.0063  0.0733  0.0549  0.0324 -0.0964 -0.0926  0.0367  0.0080  0.0424
-0.0970  0.0847 -0.0457  0.0910 -0.0376 -0.0219 -0.0959 -0.0512  0.0548 -0.0530
 0.0510 -0.0545  0.0008  0.0985 -0.0562 -0.0371 -0.0801  0.0625 -0.0951 -0.0317
 0.0855 -0.0146 -0.0674 -0.0431  0.0870  0.0717 -0.0273 -0.0438 -0.0331 -0.0954
-0.0126  0.0174 -0.0822  0.0013  0.0484  0.0865  0.0940  0.0146 -0.0187 -0.0256

Columns 71 to 80
-0.0357 -0.0328 -0.0727 -0.0354  0.0992 -0.0211  0.0479 -0.0495 -0.0206  0.0795
-0.0161 -0.0363 -0.0121 -0.0951 -0.0703  0.0982 -0.0541  0.0476  0.0471  0.0425
-0.0253  0.0283  0.0638  0.0067  0.0254 -0.0702 -0.0136  0.0207 -0.0721 -0.0929
 0.0529 -0.0179 -0.0398 -0.0739 -0.0044  0.0936 -0.0288 -0.0952  0.0977 -0.0050
-0.0251  0.0366 -0.0721 -0.0313  0.0492  0.0089  0.0511  0.0405  0.0778  0.0298
-0.0815 -0.0321  0.0181  0.0920  0.0208 -0.0870 -0.0741 -0.0644 -0.0789  0.0719

Columns 81 to 90
-0.0196 -0.0989  0.0806  0.0115  0.0578 -0.0945  0.0354 -0.0128 -0.0784  0.0083
-0.0902  0.0799 -0.0984  0.0634  0.0964 -0.0437 -0.0062  0.0509 -0.0438 -0.0146
-0.0098 -0.0816 -0.0276  0.0616  0.0208  0.0707  0.0726  0.0490  0.0627  0.0222
-0.0514  0.0734 -0.0569  0.0532 -0.0217 -0.0178  0.0086 -0.0558 -0.0882  0.0312
-0.0147  0.0869 -0.0565  0.0875  0.0355  0.0958  0.0541  0.0167 -0.0874 -0.0696
 0.0839 -0.0936  0.0841  0.0137  0.0283  0.0583 -0.0464  0.0279 -0.0628 -0.0269

Columns 91 to 100
 0.0129  0.0990  0.0661  0.0257 -0.0271 -0.0215  0.0553 -0.0031  0.0152  0.0473
-0.0757  0.0575 -0.0248 -0.0958 -0.0505  0.0422 -0.0704  0.0823  0.0301 -0.0689
-0.0489 -0.0515  0.0344 -0.0603 -0.0542 -0.0473 -0.0138 -0.0721  0.0203 -0.0425
 0.0889  0.0532 -0.0615  0.0660 -0.0296 -0.0800 -0.0518  0.0574  0.0871  0.0301
-0.0227  0.0266 -0.0940  0.0364  0.0829 -0.0546  0.0224 -0.0118  0.0024 -0.0733
-0.0393  0.0335 -0.0034  0.0861  0.0884 -0.0642  0.0667  0.0400  0.0179  0.0207
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:04[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:04[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:04[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 13 [batchSize = 1]
 
2016-10-13 16:47:04[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.315695444743ms 
2016-10-13 16:47:04[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:04[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0550  0.0298  0.0813 -0.0744  0.0172  0.0439 -0.0806  0.0764 -0.0744  0.0713
 0.0744  0.0331 -0.0912 -0.0740  0.0384  0.0175 -0.0356 -0.0667 -0.0892 -0.0798
-0.0732 -0.0804  0.0696 -0.0022  0.0694 -0.0929  0.0312 -0.0536  0.0506  0.0375
-0.0268 -0.0219 -0.0259  0.0114  0.0791  0.0132 -0.0114  0.0564  0.0862  0.0201
 0.0629 -0.0570  0.0452  0.0233 -0.0065 -0.0608 -0.0474  0.0802  0.0425  0.0264
 0.0050 -0.0160 -0.0635 -0.0542  0.0820  0.1004 -0.0768  0.0336  0.0961 -0.0702

Columns 11 to 20
-0.0915 -0.0504 -0.0696  0.0427 -0.0894 -0.0580  0.0120 -0.0369  0.0583  0.0194
-0.0775  0.0517  0.0163  0.0912  0.0390 -0.0276  0.0333 -0.0860 -0.0022  0.0826
 0.0880 -0.0117  0.0327 -0.0713  0.0882  0.0578  0.0257 -0.0836  0.0989 -0.0612
-0.0954 -0.0688  0.0496  0.0012  0.0729  0.0048 -0.0764  0.0363  0.0097  0.0967
-0.0563  0.0324  0.0928 -0.0448 -0.0827 -0.0177 -0.0109  0.0577  0.0858 -0.0218
-0.0059  0.0962  0.1001 -0.0696 -0.0225  0.0223 -0.0246 -0.0131  0.0422  0.0118

Columns 21 to 30
-0.0488  0.0222  0.0448  0.0611 -0.0560  0.0508  0.0909 -0.0816  0.0617  0.0628
-0.0538 -0.0809 -0.0817  0.0054  0.0034 -0.0010  0.0053 -0.0146 -0.0924  0.0791
 0.0026 -0.0304 -0.0957 -0.0500 -0.0058  0.0957 -0.0584 -0.0963  0.0659 -0.0748
 0.0888  0.0328  0.0212 -0.0288 -0.0822  0.0328 -0.0659  0.0570  0.0150 -0.0383
 0.0584  0.0047 -0.0107  0.0343  0.0797 -0.0091 -0.0847 -0.0459 -0.0377  0.0259
 0.0667  0.0225 -0.0960 -0.0752 -0.0164 -0.0188 -0.0387 -0.0705  0.0458 -0.0661

Columns 31 to 40
 0.0122 -0.0050 -0.0675 -0.0169  0.0992 -0.0985  0.0836 -0.0372  0.0179  0.0242
 0.0244 -0.0633  0.0165 -0.0870  0.0633  0.0057  0.0201  0.0049  0.0488  0.0139
 0.0110 -0.0585  0.0212  0.0885 -0.0273 -0.0742 -0.0214  0.0348  0.0538  0.0568
-0.1003 -0.0224  0.0900 -0.0021 -0.0219  0.0634  0.0863 -0.0650  0.0316 -0.0445
-0.0365  0.0571  0.0255 -0.0934  0.0311  0.0989 -0.0153  0.0166  0.0566  0.0094
-0.0030  0.0021 -0.0097 -0.0862  0.0738 -0.0699  0.0375  0.0013  0.0237  0.0416

Columns 41 to 50
-0.0528  0.0049 -0.0877 -0.0507  0.0065 -0.0948  0.0613 -0.0598  0.0639  0.0959
 0.0842 -0.0003  0.0243  0.0204  0.0137  0.0354 -0.0677  0.0657 -0.0931 -0.0385
 0.0737  0.0569  0.0904  0.0894  0.0536  0.0814 -0.0640  0.0886 -0.0708 -0.0049
-0.0909  0.0465  0.0162  0.0006 -0.0228  0.0633  0.0328 -0.0361 -0.0355 -0.0152
-0.0405 -0.0358  0.0238  0.0547 -0.0047  0.0580 -0.0820  0.0215 -0.0055 -0.0999
-0.0640  0.0761 -0.0604  0.0099 -0.0064 -0.0561 -0.0980  0.0507  0.0796  0.0089

Columns 51 to 60
-0.0975  0.0970  0.0792  0.0147  0.0661 -0.0872 -0.0050  0.0539 -0.0260  0.0728
 0.0307  0.0442 -0.0119  0.0753 -0.0005 -0.0129 -0.0184  0.0965  0.0649 -0.0073
-0.0733 -0.0690  0.0688  0.0052  0.0468  0.0324 -0.0731  0.0781 -0.0215  0.0554
-0.0076 -0.0343  0.0313 -0.0668 -0.0971 -0.0917 -0.0346  0.0589  0.0426  0.0475
 0.0192 -0.0172  0.0311  0.0797 -0.0113  0.0171  0.0530  0.0581  0.0403  0.0628
 0.0233 -0.0735 -0.0470 -0.0166 -0.0549  0.0941 -0.0275  0.0630 -0.0418  0.0314

Columns 61 to 70
-0.0420  0.0494 -0.0380 -0.0931 -0.0604  0.0736 -0.0125  0.0551  0.0811 -0.0668
-0.0759  0.0064  0.0733  0.0548  0.0324 -0.0964 -0.0927  0.0367  0.0081  0.0423
-0.0970  0.0848 -0.0457  0.0910 -0.0376 -0.0219 -0.0959 -0.0512  0.0549 -0.0530
 0.0511 -0.0544  0.0008  0.0985 -0.0562 -0.0371 -0.0801  0.0625 -0.0951 -0.0317
 0.0855 -0.0146 -0.0674 -0.0431  0.0870  0.0717 -0.0273 -0.0438 -0.0331 -0.0954
-0.0126  0.0174 -0.0822  0.0013  0.0484  0.0865  0.0940  0.0146 -0.0186 -0.0257

Columns 71 to 80
-0.0357 -0.0328 -0.0727 -0.0354  0.0992 -0.0211  0.0479 -0.0495 -0.0206  0.0795
-0.0161 -0.0363 -0.0120 -0.0951 -0.0703  0.0982 -0.0541  0.0476  0.0471  0.0425
-0.0253  0.0284  0.0638  0.0067  0.0254 -0.0702 -0.0137  0.0207 -0.0721 -0.0929
 0.0528 -0.0178 -0.0398 -0.0739 -0.0044  0.0936 -0.0289 -0.0952  0.0977 -0.0050
-0.0251  0.0366 -0.0721 -0.0313  0.0492  0.0090  0.0510  0.0405  0.0778  0.0298
-0.0815 -0.0321  0.0181  0.0921  0.0208 -0.0869 -0.0742 -0.0644 -0.0788  0.0719

Columns 81 to 90
-0.0196 -0.0989  0.0806  0.0115  0.0578 -0.0945  0.0354 -0.0128 -0.0784  0.0083
-0.0902  0.0799 -0.0983  0.0635  0.0964 -0.0436 -0.0062  0.0509 -0.0438 -0.0146
-0.0098 -0.0816 -0.0276  0.0616  0.0208  0.0707  0.0726  0.0490  0.0627  0.0223
-0.0514  0.0734 -0.0568  0.0532 -0.0217 -0.0178  0.0086 -0.0558 -0.0882  0.0312
-0.0147  0.0869 -0.0565  0.0875  0.0355  0.0958  0.0541  0.0167 -0.0874 -0.0696
 0.0839 -0.0936  0.0841  0.0137  0.0283  0.0583 -0.0464  0.0279 -0.0628 -0.0269

Columns 91 to 100
 0.0129  0.0990  0.0661  0.0257 -0.0271 -0.0215  0.0553 -0.0031  0.0152  0.0473
-0.0757  0.0575 -0.0248 -0.0959 -0.0504  0.0422 -0.0703  0.0823  0.0301 -0.0689
-0.0489 -0.0514  0.0344 -0.0603 -0.0542 -0.0473 -0.0137 -0.0721  0.0203 -0.0424
 0.0889  0.0532 -0.0615  0.0660 -0.0296 -0.0800 -0.0518  0.0573  0.0871  0.0301
-0.0227  0.0267 -0.0940  0.0364  0.0829 -0.0546  0.0225 -0.0118  0.0024 -0.0733
-0.0393  0.0335 -0.0034  0.0860  0.0884 -0.0642  0.0667  0.0400  0.0179  0.0207
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:04[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:04[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:04[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 14 [batchSize = 1]
 
2016-10-13 16:47:04[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.376015345256ms 
2016-10-13 16:47:04[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:04[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0550  0.0298  0.0813 -0.0744  0.0172  0.0439 -0.0806  0.0764 -0.0745  0.0713
 0.0745  0.0331 -0.0912 -0.0739  0.0384  0.0176 -0.0356 -0.0667 -0.0893 -0.0798
-0.0731 -0.0804  0.0696 -0.0021  0.0694 -0.0929  0.0311 -0.0536  0.0506  0.0375
-0.0268 -0.0219 -0.0259  0.0115  0.0791  0.0133 -0.0114  0.0565  0.0862  0.0201
 0.0629 -0.0570  0.0452  0.0234 -0.0064 -0.0607 -0.0474  0.0803  0.0424  0.0265
 0.0050 -0.0161 -0.0635 -0.0542  0.0820  0.1004 -0.0768  0.0336  0.0960 -0.0702

Columns 11 to 20
-0.0915 -0.0504 -0.0695  0.0427 -0.0894 -0.0580  0.0120 -0.0369  0.0583  0.0194
-0.0774  0.0518  0.0163  0.0911  0.0389 -0.0276  0.0333 -0.0860 -0.0022  0.0826
 0.0880 -0.0117  0.0327 -0.0713  0.0882  0.0578  0.0257 -0.0836  0.0989 -0.0612
-0.0954 -0.0688  0.0496  0.0012  0.0729  0.0048 -0.0764  0.0363  0.0097  0.0967
-0.0563  0.0324  0.0928 -0.0448 -0.0827 -0.0177 -0.0109  0.0577  0.0858 -0.0218
-0.0059  0.0962  0.1002 -0.0696 -0.0225  0.0223 -0.0246 -0.0131  0.0422  0.0118

Columns 21 to 30
-0.0488  0.0222  0.0448  0.0611 -0.0560  0.0508  0.0909 -0.0816  0.0617  0.0628
-0.0539 -0.0808 -0.0816  0.0055  0.0034 -0.0010  0.0053 -0.0145 -0.0924  0.0791
 0.0025 -0.0304 -0.0957 -0.0500 -0.0058  0.0957 -0.0585 -0.0963  0.0659 -0.0749
 0.0888  0.0327  0.0212 -0.0288 -0.0822  0.0328 -0.0659  0.0570  0.0149 -0.0383
 0.0583  0.0047 -0.0107  0.0343  0.0797 -0.0091 -0.0847 -0.0459 -0.0377  0.0258
 0.0667  0.0225 -0.0960 -0.0752 -0.0164 -0.0188 -0.0387 -0.0705  0.0458 -0.0661

Columns 31 to 40
 0.0122 -0.0050 -0.0675 -0.0169  0.0992 -0.0984  0.0835 -0.0372  0.0179  0.0242
 0.0244 -0.0632  0.0165 -0.0871  0.0633  0.0057  0.0201  0.0049  0.0488  0.0139
 0.0110 -0.0585  0.0212  0.0884 -0.0274 -0.0742 -0.0215  0.0348  0.0538  0.0568
-0.1003 -0.0224  0.0900 -0.0021 -0.0219  0.0634  0.0863 -0.0650  0.0315 -0.0445
-0.0365  0.0571  0.0254 -0.0935  0.0311  0.0989 -0.0153  0.0166  0.0566  0.0094
-0.0030  0.0022 -0.0097 -0.0862  0.0738 -0.0699  0.0375  0.0014  0.0237  0.0416

Columns 41 to 50
-0.0528  0.0049 -0.0877 -0.0507  0.0065 -0.0948  0.0613 -0.0598  0.0639  0.0959
 0.0842 -0.0003  0.0243  0.0204  0.0137  0.0354 -0.0677  0.0657 -0.0931 -0.0385
 0.0737  0.0569  0.0905  0.0893  0.0536  0.0814 -0.0640  0.0886 -0.0708 -0.0049
-0.0909  0.0465  0.0162  0.0005 -0.0228  0.0633  0.0328 -0.0361 -0.0356 -0.0153
-0.0404 -0.0358  0.0238  0.0547 -0.0047  0.0580 -0.0820  0.0215 -0.0055 -0.0999
-0.0640  0.0761 -0.0604  0.0099 -0.0064 -0.0561 -0.0980  0.0507  0.0795  0.0089

Columns 51 to 60
-0.0975  0.0970  0.0792  0.0147  0.0661 -0.0872 -0.0050  0.0539 -0.0260  0.0728
 0.0307  0.0442 -0.0119  0.0754 -0.0004 -0.0129 -0.0184  0.0966  0.0649 -0.0073
-0.0733 -0.0690  0.0689  0.0053  0.0468  0.0324 -0.0731  0.0781 -0.0214  0.0554
-0.0076 -0.0343  0.0314 -0.0668 -0.0971 -0.0917 -0.0347  0.0589  0.0427  0.0475
 0.0192 -0.0172  0.0312  0.0797 -0.0113  0.0171  0.0530  0.0582  0.0403  0.0628
 0.0233 -0.0735 -0.0470 -0.0166 -0.0549  0.0941 -0.0275  0.0630 -0.0418  0.0314

Columns 61 to 70
-0.0420  0.0495 -0.0380 -0.0931 -0.0604  0.0736 -0.0125  0.0551  0.0811 -0.0668
-0.0758  0.0064  0.0733  0.0548  0.0324 -0.0964 -0.0927  0.0367  0.0081  0.0423
-0.0970  0.0848 -0.0457  0.0910 -0.0376 -0.0219 -0.0959 -0.0512  0.0549 -0.0530
 0.0511 -0.0543  0.0008  0.0984 -0.0562 -0.0371 -0.0801  0.0625 -0.0950 -0.0317
 0.0855 -0.0145 -0.0674 -0.0431  0.0870  0.0717 -0.0273 -0.0438 -0.0331 -0.0955
-0.0125  0.0174 -0.0822  0.0012  0.0484  0.0865  0.0940  0.0146 -0.0186 -0.0257

Columns 71 to 80
-0.0357 -0.0328 -0.0727 -0.0354  0.0992 -0.0211  0.0479 -0.0495 -0.0206  0.0795
-0.0161 -0.0362 -0.0120 -0.0951 -0.0704  0.0982 -0.0541  0.0476  0.0471  0.0425
-0.0253  0.0284  0.0638  0.0067  0.0253 -0.0701 -0.0137  0.0207 -0.0721 -0.0929
 0.0528 -0.0178 -0.0398 -0.0738 -0.0045  0.0936 -0.0289 -0.0952  0.0977 -0.0050
-0.0251  0.0366 -0.0721 -0.0313  0.0492  0.0090  0.0510  0.0405  0.0778  0.0297
-0.0815 -0.0321  0.0181  0.0921  0.0208 -0.0869 -0.0742 -0.0644 -0.0788  0.0719

Columns 81 to 90
-0.0196 -0.0989  0.0806  0.0115  0.0578 -0.0945  0.0354 -0.0128 -0.0784  0.0083
-0.0902  0.0799 -0.0983  0.0635  0.0964 -0.0436 -0.0062  0.0509 -0.0438 -0.0146
-0.0098 -0.0816 -0.0276  0.0617  0.0208  0.0707  0.0726  0.0490  0.0627  0.0223
-0.0514  0.0734 -0.0568  0.0533 -0.0218 -0.0178  0.0086 -0.0558 -0.0882  0.0312
-0.0147  0.0869 -0.0565  0.0876  0.0355  0.0958  0.0541  0.0167 -0.0874 -0.0696
 0.0839 -0.0936  0.0841  0.0137  0.0283  0.0583 -0.0464  0.0279 -0.0628 -0.0269

Columns 91 to 100
 0.0129  0.0990  0.0661  0.0257 -0.0271 -0.0215  0.0553 -0.0031  0.0152  0.0473
-0.0757  0.0575 -0.0248 -0.0959 -0.0504  0.0422 -0.0703  0.0822  0.0300 -0.0689
-0.0489 -0.0514  0.0344 -0.0603 -0.0541 -0.0473 -0.0137 -0.0722  0.0202 -0.0424
 0.0888  0.0532 -0.0615  0.0659 -0.0295 -0.0800 -0.0518  0.0573  0.0870  0.0302
-0.0227  0.0267 -0.0940  0.0364  0.0829 -0.0546  0.0225 -0.0118  0.0023 -0.0733
-0.0393  0.0335 -0.0034  0.0860  0.0884 -0.0642  0.0667  0.0399  0.0178  0.0207
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:04[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:04[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:04[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 15 [batchSize = 1]
 
2016-10-13 16:47:04[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.296701431274ms 
2016-10-13 16:47:04[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:04[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0550  0.0298  0.0813 -0.0744  0.0172  0.0439 -0.0806  0.0764 -0.0745  0.0713
 0.0745  0.0331 -0.0912 -0.0739  0.0384  0.0176 -0.0356 -0.0666 -0.0893 -0.0797
-0.0731 -0.0804  0.0696 -0.0021  0.0694 -0.0929  0.0311 -0.0535  0.0506  0.0375
-0.0268 -0.0220 -0.0259  0.0115  0.0791  0.0134 -0.0114  0.0566  0.0861  0.0201
 0.0629 -0.0571  0.0452  0.0234 -0.0064 -0.0607 -0.0474  0.0803  0.0424  0.0265
 0.0050 -0.0161 -0.0635 -0.0542  0.0820  0.1005 -0.0768  0.0336  0.0960 -0.0702

Columns 11 to 20
-0.0915 -0.0504 -0.0695  0.0427 -0.0894 -0.0580  0.0120 -0.0369  0.0583  0.0194
-0.0774  0.0518  0.0164  0.0911  0.0389 -0.0275  0.0333 -0.0861 -0.0022  0.0825
 0.0880 -0.0117  0.0327 -0.0713  0.0882  0.0578  0.0257 -0.0836  0.0989 -0.0613
-0.0954 -0.0688  0.0497  0.0012  0.0729  0.0049 -0.0765  0.0363  0.0097  0.0966
-0.0563  0.0324  0.0928 -0.0448 -0.0827 -0.0177 -0.0109  0.0577  0.0858 -0.0218
-0.0059  0.0962  0.1002 -0.0696 -0.0226  0.0224 -0.0246 -0.0131  0.0422  0.0118

Columns 21 to 30
-0.0488  0.0222  0.0448  0.0611 -0.0560  0.0508  0.0909 -0.0816  0.0617  0.0628
-0.0539 -0.0808 -0.0816  0.0055  0.0034 -0.0010  0.0053 -0.0145 -0.0924  0.0791
 0.0025 -0.0304 -0.0957 -0.0500 -0.0058  0.0957 -0.0585 -0.0963  0.0659 -0.0749
 0.0888  0.0327  0.0212 -0.0287 -0.0822  0.0328 -0.0660  0.0571  0.0149 -0.0383
 0.0583  0.0047 -0.0107  0.0343  0.0797 -0.0091 -0.0848 -0.0459 -0.0377  0.0258
 0.0666  0.0225 -0.0960 -0.0752 -0.0164 -0.0188 -0.0388 -0.0705  0.0458 -0.0661

Columns 31 to 40
 0.0122 -0.0050 -0.0675 -0.0169  0.0992 -0.0984  0.0835 -0.0372  0.0179  0.0242
 0.0243 -0.0632  0.0165 -0.0871  0.0632  0.0058  0.0200  0.0049  0.0488  0.0139
 0.0110 -0.0585  0.0212  0.0884 -0.0274 -0.0742 -0.0215  0.0348  0.0538  0.0568
-0.1004 -0.0223  0.0899 -0.0022 -0.0220  0.0635  0.0862 -0.0649  0.0315 -0.0444
-0.0366  0.0571  0.0254 -0.0935  0.0311  0.0989 -0.0153  0.0166  0.0566  0.0095
-0.0030  0.0022 -0.0097 -0.0862  0.0738 -0.0699  0.0375  0.0014  0.0237  0.0417

Columns 41 to 50
-0.0528  0.0049 -0.0877 -0.0507  0.0065 -0.0948  0.0613 -0.0598  0.0639  0.0959
 0.0842 -0.0003  0.0243  0.0204  0.0137  0.0354 -0.0677  0.0657 -0.0931 -0.0385
 0.0737  0.0569  0.0905  0.0893  0.0536  0.0814 -0.0641  0.0886 -0.0708 -0.0050
-0.0908  0.0465  0.0162  0.0005 -0.0228  0.0633  0.0328 -0.0361 -0.0356 -0.0153
-0.0404 -0.0358  0.0238  0.0547 -0.0047  0.0580 -0.0820  0.0215 -0.0055 -0.1000
-0.0640  0.0761 -0.0604  0.0098 -0.0064 -0.0561 -0.0980  0.0507  0.0795  0.0088

Columns 51 to 60
-0.0975  0.0970  0.0792  0.0147  0.0661 -0.0872 -0.0050  0.0539 -0.0260  0.0728
 0.0307  0.0442 -0.0118  0.0754 -0.0004 -0.0129 -0.0184  0.0966  0.0649 -0.0073
-0.0733 -0.0690  0.0689  0.0053  0.0468  0.0324 -0.0731  0.0781 -0.0214  0.0554
-0.0076 -0.0343  0.0315 -0.0667 -0.0971 -0.0917 -0.0347  0.0589  0.0428  0.0475
 0.0193 -0.0172  0.0312  0.0797 -0.0113  0.0171  0.0530  0.0582  0.0403  0.0628
 0.0233 -0.0735 -0.0470 -0.0166 -0.0549  0.0941 -0.0275  0.0630 -0.0418  0.0314

Columns 61 to 70
-0.0419  0.0495 -0.0380 -0.0931 -0.0604  0.0736 -0.0125  0.0551  0.0811 -0.0668
-0.0758  0.0065  0.0733  0.0548  0.0324 -0.0963 -0.0926  0.0367  0.0081  0.0423
-0.0969  0.0849 -0.0457  0.0910 -0.0377 -0.0219 -0.0959 -0.0512  0.0549 -0.0531
 0.0512 -0.0542  0.0008  0.0983 -0.0562 -0.0370 -0.0801  0.0626 -0.0950 -0.0317
 0.0856 -0.0145 -0.0675 -0.0432  0.0870  0.0717 -0.0273 -0.0438 -0.0330 -0.0955
-0.0125  0.0175 -0.0822  0.0012  0.0484  0.0865  0.0940  0.0146 -0.0186 -0.0257

Columns 71 to 80
-0.0357 -0.0328 -0.0727 -0.0353  0.0992 -0.0211  0.0479 -0.0495 -0.0206  0.0795
-0.0161 -0.0362 -0.0120 -0.0951 -0.0704  0.0983 -0.0542  0.0476  0.0471  0.0425
-0.0253  0.0284  0.0638  0.0067  0.0253 -0.0701 -0.0137  0.0208 -0.0721 -0.0930
 0.0528 -0.0178 -0.0397 -0.0738 -0.0045  0.0937 -0.0290 -0.0951  0.0977 -0.0050
-0.0251  0.0367 -0.0721 -0.0312  0.0492  0.0090  0.0510  0.0405  0.0778  0.0297
-0.0815 -0.0321  0.0181  0.0921  0.0208 -0.0869 -0.0742 -0.0644 -0.0788  0.0719

Columns 81 to 90
-0.0196 -0.0989  0.0806  0.0115  0.0578 -0.0945  0.0354 -0.0128 -0.0784  0.0083
-0.0902  0.0799 -0.0983  0.0635  0.0964 -0.0436 -0.0062  0.0509 -0.0438 -0.0146
-0.0099 -0.0816 -0.0276  0.0617  0.0208  0.0707  0.0726  0.0490  0.0627  0.0223
-0.0515  0.0734 -0.0568  0.0534 -0.0218 -0.0178  0.0086 -0.0558 -0.0882  0.0312
-0.0147  0.0869 -0.0565  0.0876  0.0355  0.0958  0.0541  0.0167 -0.0874 -0.0696
 0.0839 -0.0936  0.0841  0.0137  0.0283  0.0583 -0.0464  0.0279 -0.0628 -0.0268

Columns 91 to 100
 0.0129  0.0990  0.0661  0.0256 -0.0270 -0.0215  0.0553 -0.0032  0.0152  0.0473
-0.0757  0.0575 -0.0248 -0.0959 -0.0504  0.0422 -0.0703  0.0822  0.0300 -0.0688
-0.0489 -0.0514  0.0344 -0.0603 -0.0541 -0.0473 -0.0137 -0.0722  0.0202 -0.0424
 0.0888  0.0533 -0.0615  0.0659 -0.0295 -0.0800 -0.0517  0.0572  0.0869  0.0302
-0.0227  0.0267 -0.0940  0.0363  0.0829 -0.0546  0.0225 -0.0119  0.0023 -0.0732
-0.0393  0.0335 -0.0034  0.0860  0.0884 -0.0642  0.0667  0.0399  0.0178  0.0207
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:04[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:04[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:04[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 16 [batchSize = 1]
 
2016-10-13 16:47:05[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.324993769328ms 
2016-10-13 16:47:05[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:05[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0549  0.0298  0.0813 -0.0743  0.0172  0.0439 -0.0806  0.0764 -0.0745  0.0713
 0.0745  0.0330 -0.0912 -0.0739  0.0384  0.0176 -0.0356 -0.0666 -0.0893 -0.0797
-0.0731 -0.0804  0.0696 -0.0021  0.0694 -0.0928  0.0311 -0.0535  0.0505  0.0376
-0.0267 -0.0220 -0.0259  0.0116  0.0792  0.0135 -0.0115  0.0566  0.0860  0.0201
 0.0629 -0.0571  0.0453  0.0234 -0.0064 -0.0607 -0.0474  0.0803  0.0424  0.0265
 0.0051 -0.0161 -0.0635 -0.0542  0.0820  0.1005 -0.0768  0.0336  0.0960 -0.0702

Columns 11 to 20
-0.0915 -0.0504 -0.0695  0.0427 -0.0894 -0.0580  0.0120 -0.0369  0.0583  0.0194
-0.0774  0.0518  0.0164  0.0911  0.0389 -0.0275  0.0333 -0.0861 -0.0022  0.0825
 0.0880 -0.0117  0.0328 -0.0714  0.0882  0.0578  0.0257 -0.0836  0.0989 -0.0613
-0.0954 -0.0687  0.0498  0.0011  0.0728  0.0049 -0.0765  0.0362  0.0097  0.0965
-0.0563  0.0324  0.0928 -0.0448 -0.0827 -0.0177 -0.0109  0.0576  0.0858 -0.0218
-0.0059  0.0962  0.1002 -0.0696 -0.0226  0.0224 -0.0246 -0.0131  0.0422  0.0118

Columns 21 to 30
-0.0488  0.0222  0.0448  0.0611 -0.0560  0.0508  0.0908 -0.0815  0.0617  0.0628
-0.0539 -0.0808 -0.0816  0.0055  0.0034 -0.0010  0.0053 -0.0145 -0.0924  0.0791
 0.0025 -0.0303 -0.0957 -0.0499 -0.0058  0.0957 -0.0585 -0.0963  0.0659 -0.0749
 0.0887  0.0328  0.0212 -0.0287 -0.0822  0.0328 -0.0661  0.0572  0.0149 -0.0384
 0.0583  0.0047 -0.0107  0.0344  0.0797 -0.0091 -0.0848 -0.0459 -0.0377  0.0258
 0.0666  0.0225 -0.0960 -0.0752 -0.0164 -0.0188 -0.0388 -0.0705  0.0458 -0.0661

Columns 31 to 40
 0.0122 -0.0049 -0.0675 -0.0169  0.0991 -0.0984  0.0835 -0.0372  0.0179  0.0242
 0.0243 -0.0632  0.0165 -0.0871  0.0632  0.0058  0.0200  0.0050  0.0488  0.0139
 0.0110 -0.0584  0.0211  0.0884 -0.0274 -0.0742 -0.0215  0.0348  0.0537  0.0568
-0.1005 -0.0222  0.0899 -0.0022 -0.0220  0.0635  0.0860 -0.0649  0.0315 -0.0444
-0.0366  0.0572  0.0254 -0.0935  0.0311  0.0989 -0.0153  0.0166  0.0566  0.0095
-0.0031  0.0022 -0.0097 -0.0862  0.0738 -0.0699  0.0374  0.0014  0.0237  0.0417

Columns 41 to 50
-0.0528  0.0049 -0.0877 -0.0507  0.0065 -0.0948  0.0613 -0.0598  0.0639  0.0958
 0.0842 -0.0003  0.0243  0.0203  0.0137  0.0354 -0.0677  0.0657 -0.0931 -0.0386
 0.0738  0.0569  0.0905  0.0892  0.0536  0.0815 -0.0641  0.0886 -0.0708 -0.0050
-0.0908  0.0465  0.0163  0.0004 -0.0228  0.0634  0.0327 -0.0361 -0.0357 -0.0155
-0.0404 -0.0358  0.0238  0.0546 -0.0047  0.0580 -0.0820  0.0215 -0.0056 -0.1000
-0.0640  0.0761 -0.0604  0.0098 -0.0064 -0.0561 -0.0980  0.0507  0.0795  0.0088

Columns 51 to 60
-0.0975  0.0970  0.0792  0.0147  0.0661 -0.0872 -0.0051  0.0539 -0.0259  0.0728
 0.0307  0.0442 -0.0118  0.0754 -0.0004 -0.0129 -0.0185  0.0966  0.0649 -0.0073
-0.0733 -0.0690  0.0689  0.0053  0.0468  0.0324 -0.0732  0.0781 -0.0214  0.0554
-0.0075 -0.0342  0.0316 -0.0666 -0.0970 -0.0918 -0.0348  0.0590  0.0428  0.0475
 0.0193 -0.0172  0.0312  0.0798 -0.0113  0.0171  0.0530  0.0582  0.0404  0.0628
 0.0233 -0.0735 -0.0469 -0.0166 -0.0549  0.0941 -0.0275  0.0631 -0.0417  0.0314

Columns 61 to 70
-0.0419  0.0495 -0.0380 -0.0931 -0.0604  0.0736 -0.0125  0.0551  0.0811 -0.0668
-0.0758  0.0065  0.0733  0.0548  0.0323 -0.0963 -0.0926  0.0367  0.0082  0.0422
-0.0969  0.0849 -0.0457  0.0909 -0.0377 -0.0219 -0.0959 -0.0512  0.0550 -0.0531
 0.0513 -0.0541  0.0007  0.0983 -0.0562 -0.0370 -0.0801  0.0626 -0.0949 -0.0319
 0.0856 -0.0145 -0.0675 -0.0432  0.0870  0.0717 -0.0273 -0.0438 -0.0330 -0.0955
-0.0125  0.0175 -0.0822  0.0012  0.0484  0.0865  0.0940  0.0146 -0.0186 -0.0257

Columns 71 to 80
-0.0357 -0.0328 -0.0726 -0.0353  0.0992 -0.0211  0.0478 -0.0495 -0.0206  0.0795
-0.0161 -0.0362 -0.0120 -0.0951 -0.0704  0.0983 -0.0542  0.0476  0.0471  0.0425
-0.0253  0.0284  0.0639  0.0067  0.0253 -0.0701 -0.0138  0.0208 -0.0721 -0.0930
 0.0528 -0.0177 -0.0397 -0.0738 -0.0046  0.0938 -0.0291 -0.0951  0.0978 -0.0050
-0.0252  0.0367 -0.0721 -0.0312  0.0491  0.0091  0.0510  0.0405  0.0778  0.0297
-0.0815 -0.0320  0.0181  0.0921  0.0208 -0.0869 -0.0742 -0.0644 -0.0788  0.0719

Columns 81 to 90
-0.0196 -0.0989  0.0806  0.0115  0.0578 -0.0945  0.0354 -0.0128 -0.0784  0.0083
-0.0902  0.0799 -0.0983  0.0636  0.0964 -0.0436 -0.0062  0.0509 -0.0438 -0.0146
-0.0099 -0.0816 -0.0276  0.0617  0.0208  0.0707  0.0726  0.0490  0.0627  0.0223
-0.0515  0.0734 -0.0568  0.0534 -0.0218 -0.0178  0.0086 -0.0558 -0.0882  0.0312
-0.0147  0.0869 -0.0565  0.0876  0.0355  0.0958  0.0541  0.0167 -0.0874 -0.0696
 0.0839 -0.0936  0.0842  0.0138  0.0283  0.0584 -0.0464  0.0279 -0.0628 -0.0268

Columns 91 to 100
 0.0129  0.0990  0.0661  0.0256 -0.0270 -0.0215  0.0553 -0.0032  0.0152  0.0473
-0.0757  0.0576 -0.0248 -0.0959 -0.0504  0.0422 -0.0703  0.0822  0.0300 -0.0688
-0.0490 -0.0514  0.0344 -0.0604 -0.0541 -0.0473 -0.0137 -0.0722  0.0202 -0.0424
 0.0888  0.0533 -0.0615  0.0658 -0.0294 -0.0801 -0.0517  0.0572  0.0868  0.0302
-0.0227  0.0267 -0.0940  0.0363  0.0830 -0.0546  0.0225 -0.0119  0.0023 -0.0732
-0.0393  0.0335 -0.0034  0.0860  0.0884 -0.0642  0.0667  0.0399  0.0178  0.0207
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:05[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:05[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:05[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 17 [batchSize = 1]
 
2016-10-13 16:47:05[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.461369196574ms 
2016-10-13 16:47:05[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:05[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0549  0.0298  0.0813 -0.0743  0.0172  0.0439 -0.0806  0.0764 -0.0745  0.0713
 0.0745  0.0330 -0.0912 -0.0739  0.0384  0.0176 -0.0356 -0.0666 -0.0894 -0.0797
-0.0731 -0.0805  0.0696 -0.0020  0.0694 -0.0928  0.0311 -0.0535  0.0505  0.0376
-0.0267 -0.0222 -0.0258  0.0117  0.0792  0.0135 -0.0115  0.0567  0.0859  0.0201
 0.0630 -0.0571  0.0453  0.0235 -0.0064 -0.0607 -0.0474  0.0803  0.0423  0.0265
 0.0051 -0.0161 -0.0635 -0.0542  0.0820  0.1005 -0.0768  0.0336  0.0960 -0.0702

Columns 11 to 20
-0.0915 -0.0504 -0.0695  0.0427 -0.0894 -0.0580  0.0120 -0.0369  0.0583  0.0194
-0.0774  0.0518  0.0164  0.0911  0.0389 -0.0275  0.0333 -0.0861 -0.0022  0.0825
 0.0880 -0.0117  0.0328 -0.0714  0.0882  0.0578  0.0257 -0.0837  0.0989 -0.0613
-0.0954 -0.0687  0.0498  0.0011  0.0728  0.0049 -0.0765  0.0361  0.0097  0.0965
-0.0563  0.0324  0.0928 -0.0448 -0.0827 -0.0177 -0.0110  0.0576  0.0858 -0.0219
-0.0059  0.0963  0.1002 -0.0696 -0.0226  0.0224 -0.0246 -0.0132  0.0422  0.0118

Columns 21 to 30
-0.0488  0.0222  0.0448  0.0611 -0.0560  0.0508  0.0908 -0.0815  0.0617  0.0627
-0.0539 -0.0808 -0.0816  0.0055  0.0034 -0.0011  0.0052 -0.0145 -0.0924  0.0790
 0.0025 -0.0303 -0.0957 -0.0499 -0.0058  0.0957 -0.0586 -0.0962  0.0659 -0.0750
 0.0887  0.0328  0.0212 -0.0287 -0.0822  0.0327 -0.0662  0.0572  0.0149 -0.0385
 0.0583  0.0047 -0.0107  0.0344  0.0797 -0.0092 -0.0848 -0.0459 -0.0377  0.0258
 0.0666  0.0225 -0.0960 -0.0752 -0.0164 -0.0188 -0.0388 -0.0704  0.0458 -0.0661

Columns 31 to 40
 0.0122 -0.0049 -0.0675 -0.0169  0.0991 -0.0984  0.0835 -0.0372  0.0179  0.0242
 0.0243 -0.0632  0.0164 -0.0871  0.0632  0.0058  0.0199  0.0050  0.0488  0.0139
 0.0109 -0.0584  0.0211  0.0884 -0.0274 -0.0741 -0.0216  0.0349  0.0537  0.0569
-0.1005 -0.0222  0.0898 -0.0023 -0.0220  0.0635  0.0859 -0.0648  0.0314 -0.0444
-0.0366  0.0572  0.0254 -0.0935  0.0311  0.0989 -0.0154  0.0167  0.0566  0.0095
-0.0031  0.0022 -0.0097 -0.0863  0.0738 -0.0699  0.0374  0.0014  0.0237  0.0417

Columns 41 to 50
-0.0528  0.0049 -0.0877 -0.0507  0.0065 -0.0948  0.0613 -0.0598  0.0638  0.0958
 0.0842 -0.0003  0.0244  0.0203  0.0137  0.0354 -0.0677  0.0657 -0.0932 -0.0386
 0.0738  0.0569  0.0905  0.0892  0.0536  0.0815 -0.0641  0.0886 -0.0709 -0.0051
-0.0907  0.0466  0.0163  0.0003 -0.0228  0.0634  0.0327 -0.0361 -0.0358 -0.0156
-0.0404 -0.0358  0.0238  0.0546 -0.0047  0.0580 -0.0820  0.0216 -0.0056 -0.1000
-0.0640  0.0762 -0.0604  0.0098 -0.0064 -0.0561 -0.0980  0.0507  0.0795  0.0088

Columns 51 to 60
-0.0975  0.0970  0.0792  0.0147  0.0661 -0.0872 -0.0051  0.0540 -0.0259  0.0728
 0.0307  0.0442 -0.0118  0.0754 -0.0004 -0.0129 -0.0185  0.0966  0.0649 -0.0073
-0.0733 -0.0690  0.0690  0.0054  0.0469  0.0324 -0.0732  0.0782 -0.0214  0.0554
-0.0075 -0.0342  0.0316 -0.0665 -0.0970 -0.0918 -0.0348  0.0591  0.0429  0.0475
 0.0193 -0.0172  0.0313  0.0798 -0.0113  0.0171  0.0530  0.0582  0.0404  0.0628
 0.0233 -0.0735 -0.0469 -0.0165 -0.0549  0.0941 -0.0275  0.0631 -0.0417  0.0314

Columns 61 to 70
-0.0419  0.0495 -0.0380 -0.0932 -0.0604  0.0736 -0.0125  0.0551  0.0811 -0.0668
-0.0757  0.0065  0.0733  0.0547  0.0323 -0.0963 -0.0926  0.0367  0.0082  0.0422
-0.0969  0.0849 -0.0457  0.0909 -0.0377 -0.0219 -0.0959 -0.0511  0.0550 -0.0531
 0.0514 -0.0541  0.0007  0.0982 -0.0562 -0.0370 -0.0801  0.0627 -0.0948 -0.0320
 0.0856 -0.0145 -0.0675 -0.0432  0.0870  0.0718 -0.0273 -0.0438 -0.0330 -0.0955
-0.0125  0.0175 -0.0822  0.0012  0.0484  0.0865  0.0940  0.0146 -0.0186 -0.0257

Columns 71 to 80
-0.0357 -0.0328 -0.0726 -0.0353  0.0991 -0.0211  0.0478 -0.0495 -0.0206  0.0795
-0.0162 -0.0362 -0.0120 -0.0951 -0.0704  0.0984 -0.0542  0.0476  0.0471  0.0425
-0.0253  0.0284  0.0639  0.0067  0.0253 -0.0700 -0.0138  0.0208 -0.0721 -0.0930
 0.0527 -0.0177 -0.0397 -0.0737 -0.0046  0.0938 -0.0291 -0.0951  0.0978 -0.0051
-0.0252  0.0367 -0.0720 -0.0312  0.0491  0.0091  0.0509  0.0405  0.0778  0.0297
-0.0815 -0.0320  0.0181  0.0921  0.0207 -0.0869 -0.0742 -0.0644 -0.0788  0.0718

Columns 81 to 90
-0.0196 -0.0989  0.0806  0.0115  0.0578 -0.0945  0.0354 -0.0128 -0.0784  0.0083
-0.0902  0.0799 -0.0983  0.0636  0.0964 -0.0436 -0.0062  0.0509 -0.0438 -0.0146
-0.0099 -0.0816 -0.0276  0.0618  0.0208  0.0708  0.0726  0.0490  0.0627  0.0223
-0.0515  0.0734 -0.0567  0.0535 -0.0218 -0.0178  0.0086 -0.0558 -0.0882  0.0312
-0.0147  0.0869 -0.0565  0.0876  0.0355  0.0958  0.0541  0.0167 -0.0874 -0.0696
 0.0839 -0.0936  0.0842  0.0138  0.0283  0.0584 -0.0464  0.0279 -0.0628 -0.0268

Columns 91 to 100
 0.0128  0.0991  0.0661  0.0256 -0.0270 -0.0215  0.0553 -0.0032  0.0152  0.0473
-0.0757  0.0576 -0.0248 -0.0960 -0.0504  0.0422 -0.0703  0.0822  0.0299 -0.0688
-0.0490 -0.0514  0.0344 -0.0604 -0.0541 -0.0473 -0.0137 -0.0722  0.0201 -0.0424
 0.0887  0.0534 -0.0615  0.0657 -0.0294 -0.0800 -0.0517  0.0571  0.0868  0.0303
-0.0227  0.0267 -0.0940  0.0363  0.0830 -0.0546  0.0225 -0.0119  0.0023 -0.0732
-0.0393  0.0335 -0.0035  0.0860  0.0884 -0.0642  0.0667  0.0399  0.0178  0.0207
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:05[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:05[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:05[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 18 [batchSize = 1]
 
2016-10-13 16:47:05[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.332623163859ms 
2016-10-13 16:47:05[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:05[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0549  0.0298  0.0814 -0.0743  0.0172  0.0440 -0.0807  0.0764 -0.0745  0.0713
 0.0745  0.0330 -0.0912 -0.0738  0.0385  0.0177 -0.0356 -0.0666 -0.0894 -0.0797
-0.0731 -0.0805  0.0696 -0.0020  0.0694 -0.0927  0.0311 -0.0535  0.0504  0.0376
-0.0267 -0.0222 -0.0258  0.0118  0.0792  0.0137 -0.0115  0.0568  0.0858  0.0202
 0.0630 -0.0571  0.0453  0.0235 -0.0064 -0.0606 -0.0474  0.0803  0.0423  0.0265
 0.0051 -0.0161 -0.0635 -0.0541  0.0820  0.1005 -0.0768  0.0337  0.0959 -0.0702

Columns 11 to 20
-0.0915 -0.0504 -0.0695  0.0427 -0.0894 -0.0580  0.0120 -0.0370  0.0583  0.0193
-0.0774  0.0518  0.0164  0.0911  0.0389 -0.0275  0.0333 -0.0861 -0.0022  0.0825
 0.0880 -0.0116  0.0328 -0.0714  0.0882  0.0578  0.0257 -0.0837  0.0989 -0.0613
-0.0954 -0.0687  0.0499  0.0010  0.0728  0.0049 -0.0765  0.0361  0.0097  0.0964
-0.0563  0.0324  0.0929 -0.0448 -0.0827 -0.0177 -0.0110  0.0576  0.0858 -0.0219
-0.0059  0.0963  0.1002 -0.0696 -0.0226  0.0224 -0.0247 -0.0132  0.0422  0.0117

Columns 21 to 30
-0.0488  0.0222  0.0448  0.0611 -0.0560  0.0508  0.0908 -0.0815  0.0617  0.0627
-0.0539 -0.0808 -0.0816  0.0055  0.0034 -0.0011  0.0052 -0.0144 -0.0924  0.0790
 0.0025 -0.0303 -0.0957 -0.0499 -0.0058  0.0957 -0.0586 -0.0962  0.0659 -0.0750
 0.0887  0.0328  0.0213 -0.0286 -0.0822  0.0327 -0.0663  0.0573  0.0149 -0.0386
 0.0583  0.0047 -0.0107  0.0344  0.0797 -0.0092 -0.0848 -0.0458 -0.0377  0.0258
 0.0666  0.0225 -0.0960 -0.0752 -0.0164 -0.0188 -0.0388 -0.0704  0.0458 -0.0661

Columns 31 to 40
 0.0122 -0.0049 -0.0676 -0.0170  0.0991 -0.0984  0.0834 -0.0371  0.0179  0.0242
 0.0243 -0.0631  0.0164 -0.0871  0.0632  0.0058  0.0199  0.0050  0.0487  0.0139
 0.0109 -0.0584  0.0211  0.0884 -0.0274 -0.0741 -0.0216  0.0349  0.0537  0.0569
-0.1006 -0.0221  0.0898 -0.0023 -0.0220  0.0635  0.0858 -0.0647  0.0314 -0.0444
-0.0367  0.0572  0.0254 -0.0935  0.0311  0.0989 -0.0154  0.0167  0.0566  0.0095
-0.0031  0.0022 -0.0097 -0.0863  0.0737 -0.0699  0.0374  0.0014  0.0237  0.0417

Columns 41 to 50
-0.0528  0.0049 -0.0876 -0.0507  0.0065 -0.0948  0.0613 -0.0598  0.0638  0.0958
 0.0842 -0.0003  0.0244  0.0203  0.0137  0.0354 -0.0677  0.0657 -0.0932 -0.0387
 0.0738  0.0569  0.0905  0.0892  0.0536  0.0815 -0.0641  0.0886 -0.0709 -0.0051
-0.0907  0.0466  0.0164  0.0002 -0.0228  0.0634  0.0327 -0.0361 -0.0359 -0.0157
-0.0404 -0.0358  0.0238  0.0546 -0.0047  0.0580 -0.0820  0.0216 -0.0056 -0.1001
-0.0640  0.0762 -0.0604  0.0098 -0.0064 -0.0561 -0.0980  0.0507  0.0795  0.0088

Columns 51 to 60
-0.0975  0.0970  0.0793  0.0148  0.0661 -0.0872 -0.0051  0.0540 -0.0259  0.0728
 0.0307  0.0442 -0.0117  0.0755 -0.0003 -0.0129 -0.0185  0.0967  0.0650 -0.0073
-0.0733 -0.0689  0.0690  0.0054  0.0469  0.0324 -0.0732  0.0782 -0.0213  0.0554
-0.0075 -0.0342  0.0318 -0.0665 -0.0969 -0.0918 -0.0348  0.0592  0.0429  0.0475
 0.0193 -0.0172  0.0313  0.0798 -0.0112  0.0171  0.0529  0.0582  0.0404  0.0628
 0.0233 -0.0734 -0.0469 -0.0165 -0.0549  0.0941 -0.0275  0.0631 -0.0417  0.0314

Columns 61 to 70
-0.0419  0.0495 -0.0380 -0.0932 -0.0604  0.0736 -0.0125  0.0551  0.0811 -0.0668
-0.0757  0.0066  0.0733  0.0547  0.0323 -0.0963 -0.0926  0.0367  0.0082  0.0422
-0.0969  0.0850 -0.0457  0.0909 -0.0377 -0.0219 -0.0959 -0.0511  0.0550 -0.0531
 0.0514 -0.0540  0.0007  0.0982 -0.0563 -0.0370 -0.0801  0.0627 -0.0948 -0.0320
 0.0856 -0.0144 -0.0675 -0.0432  0.0870  0.0718 -0.0273 -0.0438 -0.0329 -0.0956
-0.0125  0.0175 -0.0822  0.0012  0.0484  0.0865  0.0940  0.0146 -0.0185 -0.0257

Columns 71 to 80
-0.0357 -0.0327 -0.0726 -0.0353  0.0991 -0.0211  0.0478 -0.0495 -0.0206  0.0795
-0.0162 -0.0362 -0.0120 -0.0950 -0.0704  0.0984 -0.0542  0.0476  0.0471  0.0425
-0.0254  0.0284  0.0639  0.0068  0.0253 -0.0700 -0.0138  0.0208 -0.0721 -0.0930
 0.0527 -0.0177 -0.0396 -0.0737 -0.0046  0.0939 -0.0292 -0.0951  0.0978 -0.0051
-0.0252  0.0367 -0.0720 -0.0312  0.0491  0.0091  0.0509  0.0405  0.0778  0.0297
-0.0816 -0.0320  0.0181  0.0921  0.0207 -0.0868 -0.0742 -0.0644 -0.0788  0.0718

Columns 81 to 90
-0.0196 -0.0990  0.0806  0.0116  0.0578 -0.0945  0.0354 -0.0128 -0.0784  0.0083
-0.0902  0.0799 -0.0983  0.0636  0.0964 -0.0436 -0.0062  0.0509 -0.0438 -0.0146
-0.0099 -0.0816 -0.0276  0.0618  0.0208  0.0708  0.0726  0.0490  0.0627  0.0223
-0.0515  0.0733 -0.0567  0.0536 -0.0219 -0.0178  0.0086 -0.0558 -0.0882  0.0312
-0.0147  0.0869 -0.0565  0.0877  0.0355  0.0959  0.0541  0.0167 -0.0874 -0.0696
 0.0839 -0.0936  0.0842  0.0138  0.0283  0.0584 -0.0464  0.0279 -0.0628 -0.0268

Columns 91 to 100
 0.0128  0.0991  0.0661  0.0256 -0.0270 -0.0215  0.0553 -0.0032  0.0152  0.0474
-0.0758  0.0576 -0.0249 -0.0960 -0.0503  0.0422 -0.0703  0.0821  0.0299 -0.0688
-0.0490 -0.0514  0.0344 -0.0604 -0.0541 -0.0473 -0.0137 -0.0722  0.0201 -0.0423
 0.0887  0.0534 -0.0615  0.0657 -0.0293 -0.0800 -0.0516  0.0570  0.0867  0.0303
-0.0227  0.0267 -0.0941  0.0363  0.0830 -0.0546  0.0226 -0.0119  0.0022 -0.0732
-0.0393  0.0335 -0.0035  0.0859  0.0884 -0.0642  0.0667  0.0399  0.0178  0.0208
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:05[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:05[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:05[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 19 [batchSize = 1]
 
2016-10-13 16:47:05[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.278025309245ms 
2016-10-13 16:47:05[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:05[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0549  0.0297  0.0814 -0.0742  0.0172  0.0441 -0.0807  0.0765 -0.0746  0.0713
 0.0746  0.0329 -0.0912 -0.0738  0.0385  0.0178 -0.0357 -0.0665 -0.0895 -0.0797
-0.0730 -0.0806  0.0696 -0.0020  0.0694 -0.0927  0.0311 -0.0534  0.0504  0.0376
-0.0266 -0.0225 -0.0258  0.0119  0.0792  0.0139 -0.0116  0.0569  0.0857  0.0202
 0.0630 -0.0572  0.0453  0.0235 -0.0064 -0.0606 -0.0475  0.0804  0.0423  0.0265
 0.0051 -0.0162 -0.0635 -0.0541  0.0820  0.1006 -0.0768  0.0337  0.0959 -0.0702

Columns 11 to 20
-0.0915 -0.0504 -0.0694  0.0427 -0.0894 -0.0580  0.0119 -0.0370  0.0583  0.0193
-0.0774  0.0518  0.0165  0.0911  0.0389 -0.0275  0.0333 -0.0862 -0.0022  0.0824
 0.0880 -0.0116  0.0328 -0.0714  0.0882  0.0578  0.0257 -0.0837  0.0989 -0.0614
-0.0953 -0.0686  0.0500  0.0010  0.0728  0.0050 -0.0766  0.0360  0.0097  0.0962
-0.0563  0.0325  0.0929 -0.0449 -0.0827 -0.0176 -0.0110  0.0576  0.0858 -0.0219
-0.0059  0.0963  0.1002 -0.0697 -0.0226  0.0224 -0.0247 -0.0132  0.0422  0.0117

Columns 21 to 30
-0.0488  0.0222  0.0448  0.0611 -0.0560  0.0508  0.0907 -0.0815  0.0617  0.0627
-0.0539 -0.0808 -0.0816  0.0055  0.0034 -0.0011  0.0052 -0.0144 -0.0924  0.0790
 0.0024 -0.0303 -0.0957 -0.0499 -0.0059  0.0956 -0.0586 -0.0962  0.0659 -0.0750
 0.0886  0.0328  0.0213 -0.0286 -0.0822  0.0327 -0.0664  0.0574  0.0149 -0.0387
 0.0583  0.0047 -0.0107  0.0344  0.0797 -0.0092 -0.0849 -0.0458 -0.0377  0.0257
 0.0666  0.0225 -0.0960 -0.0751 -0.0164 -0.0188 -0.0388 -0.0704  0.0458 -0.0662

Columns 31 to 40
 0.0121 -0.0049 -0.0676 -0.0170  0.0991 -0.0984  0.0834 -0.0371  0.0178  0.0242
 0.0242 -0.0631  0.0164 -0.0872  0.0632  0.0058  0.0199  0.0051  0.0487  0.0139
 0.0109 -0.0584  0.0211  0.0883 -0.0274 -0.0741 -0.0217  0.0349  0.0537  0.0569
-0.1007 -0.0220  0.0897 -0.0024 -0.0221  0.0636  0.0857 -0.0646  0.0313 -0.0443
-0.0367  0.0572  0.0254 -0.0936  0.0311  0.0989 -0.0155  0.0167  0.0565  0.0095
-0.0031  0.0022 -0.0098 -0.0863  0.0737 -0.0699  0.0373  0.0015  0.0236  0.0417

Columns 41 to 50
-0.0527  0.0050 -0.0876 -0.0508  0.0065 -0.0948  0.0613 -0.0598  0.0638  0.0957
 0.0843 -0.0003  0.0244  0.0202  0.0137  0.0354 -0.0677  0.0658 -0.0932 -0.0387
 0.0738  0.0569  0.0906  0.0891  0.0536  0.0815 -0.0641  0.0886 -0.0709 -0.0051
-0.0906  0.0466  0.0165  0.0001 -0.0228  0.0634  0.0327 -0.0360 -0.0360 -0.0159
-0.0404 -0.0358  0.0239  0.0546 -0.0047  0.0580 -0.0820  0.0216 -0.0056 -0.1001
-0.0639  0.0762 -0.0603  0.0097 -0.0064 -0.0561 -0.0980  0.0507  0.0795  0.0087

Columns 51 to 60
-0.0975  0.0971  0.0793  0.0148  0.0661 -0.0872 -0.0051  0.0540 -0.0259  0.0728
 0.0307  0.0442 -0.0117  0.0755 -0.0003 -0.0129 -0.0185  0.0967  0.0650 -0.0073
-0.0733 -0.0689  0.0690  0.0054  0.0469  0.0324 -0.0732  0.0782 -0.0213  0.0554
-0.0075 -0.0342  0.0320 -0.0663 -0.0968 -0.0918 -0.0349  0.0592  0.0431  0.0475
 0.0193 -0.0172  0.0313  0.0799 -0.0112  0.0171  0.0529  0.0583  0.0404  0.0628
 0.0233 -0.0734 -0.0468 -0.0165 -0.0548  0.0941 -0.0275  0.0631 -0.0417  0.0314

Columns 61 to 70
-0.0418  0.0496 -0.0381 -0.0932 -0.0604  0.0736 -0.0125  0.0551  0.0812 -0.0669
-0.0757  0.0066  0.0732  0.0547  0.0323 -0.0963 -0.0926  0.0368  0.0083  0.0421
-0.0968  0.0850 -0.0457  0.0908 -0.0377 -0.0219 -0.0959 -0.0511  0.0551 -0.0532
 0.0516 -0.0538  0.0006  0.0980 -0.0563 -0.0369 -0.0801  0.0627 -0.0946 -0.0321
 0.0857 -0.0144 -0.0675 -0.0433  0.0870  0.0718 -0.0273 -0.0438 -0.0329 -0.0956
-0.0124  0.0176 -0.0823  0.0012  0.0484  0.0865  0.0940  0.0146 -0.0185 -0.0258

Columns 71 to 80
-0.0357 -0.0327 -0.0726 -0.0353  0.0991 -0.0210  0.0478 -0.0495 -0.0206  0.0794
-0.0162 -0.0361 -0.0119 -0.0950 -0.0704  0.0984 -0.0543  0.0476  0.0472  0.0425
-0.0254  0.0285  0.0639  0.0068  0.0253 -0.0699 -0.0139  0.0208 -0.0721 -0.0930
 0.0526 -0.0176 -0.0396 -0.0736 -0.0047  0.0940 -0.0293 -0.0951  0.0978 -0.0051
-0.0252  0.0367 -0.0720 -0.0312  0.0491  0.0092  0.0509  0.0405  0.0778  0.0297
-0.0816 -0.0320  0.0182  0.0921  0.0207 -0.0868 -0.0743 -0.0644 -0.0788  0.0718

Columns 81 to 90
-0.0197 -0.0990  0.0807  0.0116  0.0577 -0.0945  0.0354 -0.0128 -0.0784  0.0083
-0.0902  0.0799 -0.0983  0.0637  0.0964 -0.0436 -0.0062  0.0509 -0.0438 -0.0146
-0.0099 -0.0816 -0.0275  0.0619  0.0208  0.0708  0.0726  0.0490  0.0627  0.0223
-0.0516  0.0733 -0.0567  0.0538 -0.0220 -0.0177  0.0085 -0.0558 -0.0883  0.0313
-0.0147  0.0869 -0.0565  0.0877  0.0355  0.0959  0.0541  0.0167 -0.0874 -0.0696
 0.0839 -0.0937  0.0842  0.0138  0.0283  0.0584 -0.0464  0.0279 -0.0628 -0.0268

Columns 91 to 100
 0.0128  0.0991  0.0660  0.0255 -0.0270 -0.0215  0.0553 -0.0033  0.0151  0.0474
-0.0758  0.0576 -0.0249 -0.0960 -0.0503  0.0422 -0.0702  0.0821  0.0299 -0.0687
-0.0490 -0.0513  0.0343 -0.0604 -0.0541 -0.0473 -0.0136 -0.0723  0.0201 -0.0423
 0.0886  0.0535 -0.0616  0.0655 -0.0293 -0.0801 -0.0515  0.0569  0.0866  0.0305
-0.0227  0.0268 -0.0941  0.0363  0.0830 -0.0546  0.0226 -0.0119  0.0022 -0.0731
-0.0393  0.0335 -0.0035  0.0859  0.0884 -0.0642  0.0668  0.0399  0.0178  0.0208
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:05[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:05[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:05[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 20 [batchSize = 1]
 
2016-10-13 16:47:05[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.275641123454ms 
2016-10-13 16:47:05[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> saving network model to /home/robotec/catkin_ws/src/superchicko/farnn/src/network/glassfurnace_lstm-net.t7 
2016-10-13 16:47:06[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:06[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0549  0.0297  0.0814 -0.0742  0.0172  0.0441 -0.0807  0.0765 -0.0746  0.0713
 0.0746  0.0329 -0.0912 -0.0737  0.0385  0.0178 -0.0357 -0.0665 -0.0895 -0.0797
-0.0730 -0.0806  0.0696 -0.0019  0.0694 -0.0926  0.0311 -0.0534  0.0504  0.0376
-0.0266 -0.0225 -0.0258  0.0120  0.0792  0.0140 -0.0116  0.0569  0.0856  0.0202
 0.0630 -0.0572  0.0453  0.0236 -0.0064 -0.0605 -0.0475  0.0804  0.0422  0.0265
 0.0051 -0.0162 -0.0635 -0.0541  0.0820  0.1006 -0.0768  0.0337  0.0959 -0.0702

Columns 11 to 20
-0.0915 -0.0504 -0.0694  0.0427 -0.0894 -0.0580  0.0119 -0.0370  0.0583  0.0193
-0.0774  0.0518  0.0165  0.0910  0.0389 -0.0275  0.0333 -0.0862 -0.0022  0.0824
 0.0880 -0.0116  0.0329 -0.0714  0.0882  0.0579  0.0257 -0.0837  0.0989 -0.0614
-0.0953 -0.0686  0.0500  0.0010  0.0728  0.0050 -0.0766  0.0359  0.0097  0.0962
-0.0563  0.0325  0.0929 -0.0449 -0.0827 -0.0176 -0.0110  0.0576  0.0858 -0.0219
-0.0059  0.0963  0.1003 -0.0697 -0.0226  0.0224 -0.0247 -0.0132  0.0422  0.0117

Columns 21 to 30
-0.0488  0.0222  0.0448  0.0611 -0.0560  0.0508  0.0907 -0.0814  0.0617  0.0627
-0.0540 -0.0808 -0.0816  0.0056  0.0034 -0.0011  0.0051 -0.0144 -0.0924  0.0790
 0.0024 -0.0303 -0.0957 -0.0499 -0.0059  0.0956 -0.0587 -0.0962  0.0659 -0.0750
 0.0886  0.0328  0.0213 -0.0286 -0.0822  0.0327 -0.0665  0.0575  0.0149 -0.0387
 0.0582  0.0047 -0.0107  0.0344  0.0797 -0.0092 -0.0849 -0.0458 -0.0377  0.0257
 0.0666  0.0225 -0.0960 -0.0751 -0.0164 -0.0188 -0.0389 -0.0704  0.0458 -0.0662

Columns 31 to 40
 0.0121 -0.0049 -0.0676 -0.0170  0.0991 -0.0984  0.0834 -0.0371  0.0179  0.0242
 0.0242 -0.0631  0.0164 -0.0872  0.0632  0.0058  0.0198  0.0051  0.0487  0.0139
 0.0108 -0.0584  0.0211  0.0883 -0.0274 -0.0741 -0.0217  0.0350  0.0537  0.0569
-0.1008 -0.0220  0.0897 -0.0025 -0.0221  0.0636  0.0856 -0.0645  0.0313 -0.0443
-0.0367  0.0572  0.0253 -0.0936  0.0311  0.0989 -0.0155  0.0167  0.0565  0.0095
-0.0031  0.0022 -0.0098 -0.0863  0.0737 -0.0699  0.0373  0.0015  0.0236  0.0417

Columns 41 to 50
-0.0527  0.0050 -0.0876 -0.0508  0.0065 -0.0948  0.0613 -0.0598  0.0637  0.0957
 0.0843 -0.0003  0.0244  0.0202  0.0137  0.0354 -0.0678  0.0658 -0.0933 -0.0388
 0.0738  0.0569  0.0906  0.0891  0.0536  0.0815 -0.0641  0.0886 -0.0710 -0.0052
-0.0905  0.0466  0.0165 -0.0000 -0.0228  0.0634  0.0326 -0.0360 -0.0360 -0.0159
-0.0403 -0.0358  0.0239  0.0545 -0.0047  0.0580 -0.0821  0.0216 -0.0057 -0.1001
-0.0639  0.0762 -0.0603  0.0097 -0.0064 -0.0560 -0.0980  0.0507  0.0794  0.0087

Columns 51 to 60
-0.0975  0.0971  0.0793  0.0148  0.0661 -0.0872 -0.0051  0.0540 -0.0258  0.0728
 0.0307  0.0443 -0.0116  0.0755 -0.0003 -0.0129 -0.0185  0.0967  0.0650 -0.0073
-0.0733 -0.0689  0.0691  0.0054  0.0469  0.0324 -0.0732  0.0782 -0.0213  0.0554
-0.0075 -0.0342  0.0320 -0.0663 -0.0968 -0.0918 -0.0349  0.0593  0.0431  0.0475
 0.0193 -0.0172  0.0314  0.0799 -0.0112  0.0171  0.0529  0.0583  0.0405  0.0628
 0.0233 -0.0734 -0.0468 -0.0165 -0.0548  0.0940 -0.0275  0.0631 -0.0417  0.0314

Columns 61 to 70
-0.0418  0.0496 -0.0381 -0.0932 -0.0604  0.0736 -0.0125  0.0552  0.0812 -0.0669
-0.0757  0.0067  0.0732  0.0546  0.0323 -0.0963 -0.0926  0.0368  0.0083  0.0421
-0.0968  0.0850 -0.0457  0.0908 -0.0377 -0.0219 -0.0959 -0.0511  0.0551 -0.0532
 0.0516 -0.0537  0.0006  0.0980 -0.0563 -0.0369 -0.0801  0.0627 -0.0946 -0.0322
 0.0857 -0.0143 -0.0675 -0.0433  0.0870  0.0718 -0.0273 -0.0437 -0.0329 -0.0956
-0.0124  0.0176 -0.0823  0.0011  0.0484  0.0866  0.0940  0.0146 -0.0185 -0.0258

Columns 71 to 80
-0.0357 -0.0327 -0.0726 -0.0353  0.0991 -0.0210  0.0478 -0.0495 -0.0206  0.0794
-0.0162 -0.0361 -0.0119 -0.0950 -0.0704  0.0985 -0.0543  0.0476  0.0472  0.0425
-0.0254  0.0285  0.0639  0.0068  0.0253 -0.0699 -0.0139  0.0208 -0.0721 -0.0930
 0.0526 -0.0176 -0.0395 -0.0735 -0.0047  0.0941 -0.0293 -0.0951  0.0978 -0.0051
-0.0252  0.0367 -0.0720 -0.0311  0.0491  0.0092  0.0509  0.0405  0.0778  0.0297
-0.0816 -0.0320  0.0182  0.0921  0.0207 -0.0868 -0.0743 -0.0644 -0.0788  0.0718

Columns 81 to 90
-0.0197 -0.0990  0.0807  0.0116  0.0577 -0.0945  0.0354 -0.0128 -0.0784  0.0083
-0.0902  0.0799 -0.0983  0.0637  0.0963 -0.0436 -0.0062  0.0509 -0.0438 -0.0146
-0.0099 -0.0817 -0.0275  0.0619  0.0208  0.0708  0.0726  0.0490  0.0627  0.0223
-0.0516  0.0733 -0.0566  0.0538 -0.0220 -0.0177  0.0085 -0.0559 -0.0883  0.0313
-0.0148  0.0869 -0.0565  0.0877  0.0355  0.0959  0.0541  0.0167 -0.0874 -0.0696
 0.0839 -0.0937  0.0842  0.0139  0.0283  0.0584 -0.0465  0.0279 -0.0628 -0.0268

Columns 91 to 100
 0.0128  0.0991  0.0660  0.0255 -0.0269 -0.0215  0.0553 -0.0033  0.0151  0.0474
-0.0758  0.0576 -0.0249 -0.0960 -0.0503  0.0422 -0.0702  0.0821  0.0298 -0.0687
-0.0490 -0.0513  0.0343 -0.0604 -0.0540 -0.0473 -0.0136 -0.0723  0.0200 -0.0423
 0.0886  0.0535 -0.0616  0.0655 -0.0292 -0.0801 -0.0515  0.0569  0.0866  0.0305
-0.0227  0.0268 -0.0941  0.0362  0.0830 -0.0546  0.0226 -0.0120  0.0022 -0.0731
-0.0393  0.0336 -0.0035  0.0859  0.0885 -0.0642  0.0668  0.0398  0.0177  0.0208
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:06[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:06[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:06[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 21 [batchSize = 1]
 
2016-10-13 16:47:06[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.37736638387ms 
2016-10-13 16:47:06[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:06[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0549  0.0297  0.0813 -0.0742  0.0172  0.0441 -0.0807  0.0765 -0.0746  0.0713
 0.0746  0.0329 -0.0912 -0.0737  0.0385  0.0179 -0.0357 -0.0665 -0.0895 -0.0797
-0.0730 -0.0806  0.0696 -0.0019  0.0694 -0.0926  0.0310 -0.0534  0.0504  0.0376
-0.0265 -0.0225 -0.0258  0.0120  0.0792  0.0140 -0.0116  0.0569  0.0856  0.0202
 0.0630 -0.0572  0.0453  0.0236 -0.0064 -0.0605 -0.0475  0.0804  0.0422  0.0265
 0.0051 -0.0162 -0.0635 -0.0541  0.0820  0.1006 -0.0768  0.0337  0.0959 -0.0702

Columns 11 to 20
-0.0915 -0.0504 -0.0694  0.0427 -0.0894 -0.0580  0.0119 -0.0370  0.0583  0.0192
-0.0774  0.0519  0.0165  0.0910  0.0389 -0.0275  0.0333 -0.0862 -0.0022  0.0824
 0.0880 -0.0116  0.0329 -0.0714  0.0881  0.0579  0.0257 -0.0837  0.0989 -0.0614
-0.0953 -0.0686  0.0501  0.0010  0.0728  0.0050 -0.0766  0.0359  0.0097  0.0961
-0.0563  0.0325  0.0929 -0.0449 -0.0827 -0.0176 -0.0110  0.0576  0.0858 -0.0220
-0.0059  0.0963  0.1003 -0.0697 -0.0226  0.0224 -0.0247 -0.0132  0.0422  0.0117

Columns 21 to 30
-0.0488  0.0222  0.0448  0.0611 -0.0560  0.0508  0.0907 -0.0814  0.0617  0.0627
-0.0540 -0.0808 -0.0816  0.0056  0.0034 -0.0011  0.0051 -0.0144 -0.0924  0.0790
 0.0024 -0.0303 -0.0957 -0.0499 -0.0059  0.0956 -0.0587 -0.0961  0.0659 -0.0750
 0.0885  0.0328  0.0213 -0.0285 -0.0822  0.0327 -0.0665  0.0575  0.0149 -0.0387
 0.0582  0.0047 -0.0107  0.0344  0.0797 -0.0092 -0.0849 -0.0458 -0.0377  0.0257
 0.0666  0.0225 -0.0960 -0.0751 -0.0164 -0.0188 -0.0389 -0.0704  0.0458 -0.0662

Columns 31 to 40
 0.0121 -0.0048 -0.0676 -0.0170  0.0991 -0.0984  0.0834 -0.0371  0.0179  0.0242
 0.0242 -0.0631  0.0164 -0.0872  0.0632  0.0058  0.0198  0.0051  0.0487  0.0140
 0.0108 -0.0583  0.0210  0.0883 -0.0274 -0.0741 -0.0217  0.0350  0.0537  0.0569
-0.1008 -0.0219  0.0897 -0.0025 -0.0221  0.0636  0.0856 -0.0645  0.0313 -0.0443
-0.0367  0.0573  0.0253 -0.0936  0.0311  0.0989 -0.0155  0.0167  0.0565  0.0095
-0.0032  0.0023 -0.0098 -0.0863  0.0737 -0.0699  0.0373  0.0015  0.0236  0.0417

Columns 41 to 50
-0.0527  0.0050 -0.0876 -0.0508  0.0065 -0.0948  0.0613 -0.0598  0.0637  0.0957
 0.0843 -0.0003  0.0244  0.0202  0.0137  0.0354 -0.0678  0.0658 -0.0933 -0.0388
 0.0738  0.0569  0.0906  0.0891  0.0536  0.0815 -0.0641  0.0886 -0.0710 -0.0052
-0.0905  0.0466  0.0166 -0.0001 -0.0228  0.0634  0.0326 -0.0360 -0.0361 -0.0160
-0.0403 -0.0358  0.0239  0.0545 -0.0047  0.0580 -0.0821  0.0216 -0.0057 -0.1002
-0.0639  0.0762 -0.0603  0.0097 -0.0064 -0.0560 -0.0980  0.0507  0.0794  0.0087

Columns 51 to 60
-0.0975  0.0971  0.0793  0.0148  0.0661 -0.0872 -0.0051  0.0540 -0.0258  0.0728
 0.0307  0.0443 -0.0116  0.0756 -0.0003 -0.0129 -0.0185  0.0967  0.0651 -0.0073
-0.0733 -0.0689  0.0691  0.0055  0.0470  0.0324 -0.0732  0.0782 -0.0212  0.0554
-0.0075 -0.0341  0.0321 -0.0663 -0.0968 -0.0918 -0.0350  0.0593  0.0432  0.0475
 0.0193 -0.0172  0.0314  0.0799 -0.0112  0.0171  0.0529  0.0583  0.0405  0.0628
 0.0233 -0.0734 -0.0468 -0.0165 -0.0548  0.0940 -0.0276  0.0631 -0.0416  0.0314

Columns 61 to 70
-0.0419  0.0496 -0.0381 -0.0932 -0.0604  0.0736 -0.0125  0.0551  0.0812 -0.0669
-0.0756  0.0067  0.0732  0.0546  0.0323 -0.0963 -0.0926  0.0368  0.0083  0.0420
-0.0968  0.0851 -0.0458  0.0908 -0.0377 -0.0219 -0.0959 -0.0511  0.0551 -0.0533
 0.0516 -0.0537  0.0005  0.0980 -0.0563 -0.0369 -0.0801  0.0627 -0.0946 -0.0323
 0.0857 -0.0143 -0.0675 -0.0433  0.0870  0.0718 -0.0273 -0.0437 -0.0329 -0.0957
-0.0124  0.0176 -0.0823  0.0011  0.0484  0.0866  0.0940  0.0146 -0.0185 -0.0258

Columns 71 to 80
-0.0357 -0.0327 -0.0726 -0.0353  0.0991 -0.0210  0.0478 -0.0495 -0.0206  0.0794
-0.0162 -0.0361 -0.0119 -0.0950 -0.0705  0.0985 -0.0543  0.0476  0.0472  0.0425
-0.0254  0.0285  0.0639  0.0068  0.0253 -0.0699 -0.0139  0.0208 -0.0721 -0.0930
 0.0526 -0.0176 -0.0395 -0.0735 -0.0047  0.0941 -0.0294 -0.0951  0.0978 -0.0051
-0.0252  0.0367 -0.0720 -0.0311  0.0491  0.0092  0.0508  0.0405  0.0779  0.0297
-0.0816 -0.0320  0.0182  0.0921  0.0207 -0.0868 -0.0743 -0.0644 -0.0788  0.0718

Columns 81 to 90
-0.0197 -0.0990  0.0807  0.0116  0.0577 -0.0945  0.0354 -0.0128 -0.0784  0.0083
-0.0903  0.0799 -0.0982  0.0638  0.0963 -0.0436 -0.0062  0.0509 -0.0438 -0.0145
-0.0099 -0.0817 -0.0275  0.0619  0.0207  0.0708  0.0726  0.0490  0.0627  0.0223
-0.0516  0.0733 -0.0566  0.0539 -0.0220 -0.0177  0.0085 -0.0559 -0.0883  0.0313
-0.0148  0.0869 -0.0565  0.0878  0.0354  0.0959  0.0541  0.0167 -0.0874 -0.0696
 0.0839 -0.0937  0.0842  0.0139  0.0283  0.0584 -0.0465  0.0279 -0.0628 -0.0268

Columns 91 to 100
 0.0128  0.0991  0.0660  0.0255 -0.0269 -0.0215  0.0553 -0.0033  0.0151  0.0474
-0.0758  0.0577 -0.0249 -0.0961 -0.0503  0.0422 -0.0702  0.0821  0.0298 -0.0687
-0.0490 -0.0513  0.0343 -0.0605 -0.0540 -0.0473 -0.0136 -0.0723  0.0200 -0.0422
 0.0886  0.0536 -0.0616  0.0655 -0.0292 -0.0801 -0.0515  0.0569  0.0866  0.0305
-0.0228  0.0268 -0.0941  0.0362  0.0830 -0.0546  0.0226 -0.0120  0.0021 -0.0731
-0.0393  0.0336 -0.0035  0.0859  0.0885 -0.0642  0.0668  0.0398  0.0177  0.0208
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:06[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:06[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:06[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 22 [batchSize = 1]
 
2016-10-13 16:47:06[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.405340830485ms 
2016-10-13 16:47:06[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:06[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0549  0.0297  0.0813 -0.0742  0.0172  0.0441 -0.0807  0.0765 -0.0746  0.0713
 0.0746  0.0328 -0.0912 -0.0737  0.0385  0.0179 -0.0357 -0.0665 -0.0895 -0.0797
-0.0730 -0.0806  0.0696 -0.0019  0.0694 -0.0925  0.0310 -0.0534  0.0503  0.0376
-0.0265 -0.0226 -0.0258  0.0120  0.0792  0.0142 -0.0117  0.0570  0.0856  0.0202
 0.0630 -0.0573  0.0453  0.0236 -0.0064 -0.0604 -0.0475  0.0804  0.0422  0.0265
 0.0051 -0.0162 -0.0635 -0.0540  0.0820  0.1006 -0.0768  0.0337  0.0959 -0.0702

Columns 11 to 20
-0.0915 -0.0504 -0.0694  0.0427 -0.0894 -0.0580  0.0119 -0.0370  0.0583  0.0192
-0.0774  0.0519  0.0165  0.0910  0.0389 -0.0275  0.0332 -0.0862 -0.0022  0.0823
 0.0880 -0.0116  0.0329 -0.0714  0.0881  0.0579  0.0257 -0.0838  0.0989 -0.0615
-0.0953 -0.0685  0.0501  0.0010  0.0727  0.0050 -0.0766  0.0359  0.0097  0.0961
-0.0563  0.0325  0.0930 -0.0449 -0.0827 -0.0176 -0.0110  0.0575  0.0858 -0.0220
-0.0059  0.0963  0.1003 -0.0697 -0.0226  0.0224 -0.0247 -0.0132  0.0422  0.0117

Columns 21 to 30
-0.0488  0.0222  0.0448  0.0612 -0.0560  0.0508  0.0907 -0.0814  0.0617  0.0627
-0.0540 -0.0808 -0.0816  0.0056  0.0034 -0.0011  0.0051 -0.0143 -0.0924  0.0789
 0.0024 -0.0303 -0.0957 -0.0499 -0.0059  0.0956 -0.0587 -0.0961  0.0659 -0.0751
 0.0885  0.0329  0.0213 -0.0285 -0.0822  0.0327 -0.0666  0.0576  0.0148 -0.0388
 0.0582  0.0047 -0.0107  0.0344  0.0797 -0.0092 -0.0849 -0.0457 -0.0377  0.0257
 0.0666  0.0225 -0.0959 -0.0751 -0.0164 -0.0188 -0.0389 -0.0704  0.0458 -0.0662

Columns 31 to 40
 0.0121 -0.0048 -0.0676 -0.0170  0.0991 -0.0984  0.0834 -0.0371  0.0179  0.0242
 0.0241 -0.0630  0.0164 -0.0872  0.0632  0.0058  0.0198  0.0051  0.0487  0.0140
 0.0108 -0.0583  0.0210  0.0883 -0.0274 -0.0741 -0.0217  0.0350  0.0536  0.0569
-0.1009 -0.0218  0.0897 -0.0025 -0.0221  0.0636  0.0856 -0.0644  0.0313 -0.0443
-0.0368  0.0573  0.0253 -0.0936  0.0311  0.0989 -0.0155  0.0168  0.0565  0.0095
-0.0032  0.0023 -0.0098 -0.0863  0.0737 -0.0699  0.0373  0.0015  0.0236  0.0417

Columns 41 to 50
-0.0527  0.0050 -0.0876 -0.0508  0.0065 -0.0948  0.0613 -0.0598  0.0637  0.0957
 0.0843 -0.0002  0.0244  0.0201  0.0137  0.0354 -0.0678  0.0658 -0.0933 -0.0388
 0.0739  0.0569  0.0906  0.0890  0.0536  0.0815 -0.0641  0.0886 -0.0710 -0.0052
-0.0904  0.0466  0.0166 -0.0002 -0.0228  0.0634  0.0326 -0.0359 -0.0361 -0.0160
-0.0403 -0.0358  0.0239  0.0545 -0.0047  0.0580 -0.0821  0.0216 -0.0057 -0.1002
-0.0639  0.0762 -0.0603  0.0097 -0.0064 -0.0560 -0.0980  0.0507  0.0794  0.0087

Columns 51 to 60
-0.0975  0.0970  0.0794  0.0148  0.0661 -0.0872 -0.0051  0.0540 -0.0259  0.0728
 0.0307  0.0443 -0.0115  0.0756 -0.0002 -0.0129 -0.0186  0.0967  0.0651 -0.0073
-0.0733 -0.0689  0.0692  0.0055  0.0470  0.0324 -0.0733  0.0783 -0.0212  0.0554
-0.0075 -0.0341  0.0322 -0.0663 -0.0967 -0.0918 -0.0350  0.0594  0.0432  0.0475
 0.0193 -0.0172  0.0315  0.0799 -0.0111  0.0171  0.0529  0.0583  0.0405  0.0628
 0.0233 -0.0734 -0.0468 -0.0165 -0.0548  0.0940 -0.0276  0.0631 -0.0416  0.0314

Columns 61 to 70
-0.0418  0.0496 -0.0381 -0.0932 -0.0604  0.0736 -0.0125  0.0552  0.0812 -0.0669
-0.0756  0.0067  0.0732  0.0546  0.0323 -0.0963 -0.0926  0.0368  0.0083  0.0420
-0.0968  0.0851 -0.0458  0.0908 -0.0377 -0.0218 -0.0959 -0.0511  0.0551 -0.0533
 0.0517 -0.0536  0.0005  0.0979 -0.0563 -0.0369 -0.0801  0.0628 -0.0945 -0.0323
 0.0857 -0.0143 -0.0675 -0.0433  0.0870  0.0718 -0.0273 -0.0437 -0.0328 -0.0957
-0.0124  0.0176 -0.0823  0.0011  0.0484  0.0866  0.0940  0.0146 -0.0185 -0.0258

Columns 71 to 80
-0.0357 -0.0327 -0.0726 -0.0353  0.0991 -0.0211  0.0478 -0.0495 -0.0206  0.0794
-0.0162 -0.0361 -0.0119 -0.0950 -0.0705  0.0985 -0.0544  0.0476  0.0472  0.0425
-0.0254  0.0285  0.0639  0.0069  0.0252 -0.0699 -0.0139  0.0208 -0.0720 -0.0930
 0.0526 -0.0175 -0.0395 -0.0735 -0.0048  0.0941 -0.0294 -0.0951  0.0978 -0.0052
-0.0252  0.0368 -0.0720 -0.0311  0.0491  0.0092  0.0508  0.0406  0.0779  0.0297
-0.0816 -0.0320  0.0182  0.0922  0.0207 -0.0868 -0.0743 -0.0644 -0.0788  0.0718

Columns 81 to 90
-0.0197 -0.0990  0.0807  0.0116  0.0577 -0.0945  0.0354 -0.0128 -0.0784  0.0083
-0.0903  0.0799 -0.0982  0.0638  0.0963 -0.0436 -0.0062  0.0509 -0.0438 -0.0145
-0.0099 -0.0817 -0.0275  0.0620  0.0207  0.0708  0.0726  0.0490  0.0627  0.0223
-0.0516  0.0733 -0.0566  0.0539 -0.0220 -0.0177  0.0085 -0.0559 -0.0883  0.0313
-0.0148  0.0869 -0.0564  0.0878  0.0354  0.0959  0.0541  0.0167 -0.0874 -0.0696
 0.0839 -0.0937  0.0842  0.0139  0.0283  0.0584 -0.0465  0.0279 -0.0628 -0.0268

Columns 91 to 100
 0.0128  0.0991  0.0660  0.0255 -0.0269 -0.0215  0.0553 -0.0033  0.0151  0.0474
-0.0758  0.0577 -0.0249 -0.0961 -0.0503  0.0422 -0.0702  0.0821  0.0298 -0.0687
-0.0490 -0.0513  0.0343 -0.0605 -0.0540 -0.0473 -0.0136 -0.0723  0.0200 -0.0422
 0.0886  0.0536 -0.0616  0.0655 -0.0291 -0.0801 -0.0514  0.0568  0.0865  0.0305
-0.0228  0.0268 -0.0941  0.0362  0.0831 -0.0546  0.0226 -0.0120  0.0021 -0.0731
-0.0394  0.0336 -0.0035  0.0859  0.0885 -0.0642  0.0668  0.0398  0.0177  0.0208
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:06[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:06[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:06[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 23 [batchSize = 1]
 
2016-10-13 16:47:06[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.347723007202ms 
2016-10-13 16:47:06[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:06[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0549  0.0296  0.0814 -0.0742  0.0172  0.0442 -0.0807  0.0765 -0.0746  0.0713
 0.0746  0.0328 -0.0912 -0.0737  0.0385  0.0180 -0.0357 -0.0664 -0.0896 -0.0797
-0.0730 -0.0807  0.0697 -0.0018  0.0694 -0.0925  0.0310 -0.0534  0.0503  0.0376
-0.0264 -0.0227 -0.0258  0.0121  0.0792  0.0144 -0.0117  0.0570  0.0855  0.0203
 0.0631 -0.0573  0.0453  0.0236 -0.0064 -0.0604 -0.0475  0.0804  0.0422  0.0265
 0.0051 -0.0162 -0.0635 -0.0540  0.0820  0.1007 -0.0769  0.0337  0.0959 -0.0702

Columns 11 to 20
-0.0915 -0.0503 -0.0694  0.0427 -0.0894 -0.0579  0.0119 -0.0370  0.0583  0.0192
-0.0774  0.0519  0.0166  0.0910  0.0389 -0.0275  0.0332 -0.0862 -0.0022  0.0823
 0.0880 -0.0116  0.0329 -0.0714  0.0881  0.0579  0.0256 -0.0838  0.0989 -0.0615
-0.0953 -0.0685  0.0502  0.0009  0.0727  0.0050 -0.0767  0.0358  0.0097  0.0960
-0.0563  0.0325  0.0930 -0.0449 -0.0828 -0.0176 -0.0110  0.0575  0.0858 -0.0220
-0.0059  0.0963  0.1003 -0.0697 -0.0226  0.0224 -0.0247 -0.0132  0.0422  0.0116

Columns 21 to 30
-0.0489  0.0222  0.0448  0.0612 -0.0560  0.0508  0.0907 -0.0814  0.0617  0.0626
-0.0540 -0.0808 -0.0816  0.0056  0.0034 -0.0011  0.0050 -0.0143 -0.0924  0.0789
 0.0024 -0.0303 -0.0957 -0.0498 -0.0059  0.0956 -0.0588 -0.0961  0.0659 -0.0751
 0.0885  0.0329  0.0214 -0.0285 -0.0822  0.0327 -0.0667  0.0577  0.0148 -0.0389
 0.0582  0.0047 -0.0107  0.0344  0.0797 -0.0092 -0.0850 -0.0457 -0.0377  0.0256
 0.0666  0.0225 -0.0959 -0.0751 -0.0164 -0.0188 -0.0389 -0.0703  0.0458 -0.0662

Columns 31 to 40
 0.0121 -0.0048 -0.0676 -0.0170  0.0991 -0.0984  0.0834 -0.0370  0.0178  0.0242
 0.0241 -0.0630  0.0163 -0.0873  0.0632  0.0058  0.0198  0.0052  0.0487  0.0140
 0.0108 -0.0583  0.0210  0.0883 -0.0274 -0.0741 -0.0217  0.0350  0.0536  0.0569
-0.1009 -0.0217  0.0896 -0.0026 -0.0222  0.0637  0.0855 -0.0643  0.0312 -0.0443
-0.0368  0.0573  0.0253 -0.0936  0.0310  0.0990 -0.0155  0.0168  0.0565  0.0095
-0.0032  0.0023 -0.0098 -0.0863  0.0737 -0.0699  0.0373  0.0015  0.0236  0.0417

Columns 41 to 50
-0.0527  0.0050 -0.0876 -0.0509  0.0065 -0.0948  0.0613 -0.0598  0.0637  0.0957
 0.0844 -0.0002  0.0245  0.0201  0.0137  0.0354 -0.0678  0.0658 -0.0933 -0.0389
 0.0739  0.0570  0.0906  0.0890  0.0536  0.0815 -0.0641  0.0887 -0.0710 -0.0053
-0.0904  0.0466  0.0166 -0.0002 -0.0228  0.0635  0.0325 -0.0359 -0.0362 -0.0161
-0.0403 -0.0358  0.0239  0.0544 -0.0047  0.0580 -0.0821  0.0216 -0.0057 -0.1002
-0.0639  0.0762 -0.0603  0.0097 -0.0064 -0.0560 -0.0980  0.0507  0.0794  0.0086

Columns 51 to 60
-0.0975  0.0971  0.0794  0.0148  0.0661 -0.0872 -0.0051  0.0540 -0.0258  0.0728
 0.0307  0.0443 -0.0115  0.0756 -0.0002 -0.0130 -0.0186  0.0968  0.0651 -0.0073
-0.0733 -0.0689  0.0692  0.0055  0.0470  0.0324 -0.0733  0.0783 -0.0212  0.0554
-0.0075 -0.0341  0.0323 -0.0663 -0.0966 -0.0919 -0.0351  0.0594  0.0432  0.0475
 0.0193 -0.0171  0.0315  0.0799 -0.0111  0.0171  0.0529  0.0583  0.0405  0.0628
 0.0233 -0.0734 -0.0467 -0.0165 -0.0548  0.0940 -0.0276  0.0631 -0.0416  0.0314

Columns 61 to 70
-0.0418  0.0497 -0.0381 -0.0933 -0.0604  0.0736 -0.0125  0.0552  0.0812 -0.0669
-0.0756  0.0068  0.0732  0.0546  0.0323 -0.0963 -0.0926  0.0368  0.0084  0.0420
-0.0967  0.0851 -0.0458  0.0907 -0.0377 -0.0218 -0.0959 -0.0511  0.0552 -0.0533
 0.0517 -0.0535  0.0005  0.0978 -0.0563 -0.0369 -0.0801  0.0628 -0.0944 -0.0324
 0.0857 -0.0143 -0.0675 -0.0433  0.0870  0.0718 -0.0273 -0.0437 -0.0328 -0.0957
-0.0124  0.0176 -0.0823  0.0011  0.0484  0.0866  0.0940  0.0146 -0.0185 -0.0259

Columns 71 to 80
-0.0357 -0.0327 -0.0726 -0.0352  0.0991 -0.0210  0.0477 -0.0495 -0.0206  0.0794
-0.0163 -0.0361 -0.0119 -0.0949 -0.0705  0.0985 -0.0544  0.0476  0.0472  0.0424
-0.0254  0.0285  0.0640  0.0069  0.0252 -0.0698 -0.0140  0.0208 -0.0720 -0.0930
 0.0525 -0.0175 -0.0395 -0.0734 -0.0048  0.0942 -0.0295 -0.0951  0.0979 -0.0052
-0.0253  0.0368 -0.0720 -0.0311  0.0490  0.0093  0.0508  0.0406  0.0779  0.0297
-0.0816 -0.0320  0.0182  0.0922  0.0207 -0.0867 -0.0743 -0.0644 -0.0788  0.0718

Columns 81 to 90
-0.0197 -0.0990  0.0807  0.0116  0.0577 -0.0945  0.0354 -0.0128 -0.0784  0.0083
-0.0903  0.0799 -0.0982  0.0638  0.0963 -0.0436 -0.0062  0.0509 -0.0438 -0.0145
-0.0099 -0.0817 -0.0275  0.0620  0.0207  0.0708  0.0726  0.0490  0.0627  0.0223
-0.0517  0.0733 -0.0565  0.0540 -0.0221 -0.0177  0.0085 -0.0559 -0.0883  0.0313
-0.0148  0.0869 -0.0564  0.0878  0.0354  0.0959  0.0541  0.0167 -0.0874 -0.0695
 0.0839 -0.0937  0.0842  0.0139  0.0283  0.0584 -0.0465  0.0279 -0.0628 -0.0268

Columns 91 to 100
 0.0128  0.0991  0.0660  0.0255 -0.0269 -0.0215  0.0553 -0.0033  0.0151  0.0474
-0.0758  0.0577 -0.0249 -0.0961 -0.0502  0.0422 -0.0701  0.0820  0.0297 -0.0686
-0.0490 -0.0513  0.0343 -0.0605 -0.0540 -0.0473 -0.0136 -0.0723  0.0199 -0.0422
 0.0885  0.0537 -0.0616  0.0655 -0.0291 -0.0801 -0.0514  0.0568  0.0864  0.0305
-0.0228  0.0268 -0.0941  0.0362  0.0831 -0.0546  0.0226 -0.0120  0.0021 -0.0731
-0.0394  0.0336 -0.0035  0.0859  0.0885 -0.0642  0.0668  0.0398  0.0177  0.0208
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:06[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:06[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:06[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 24 [batchSize = 1]
 
2016-10-13 16:47:06[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.366637547811ms 
2016-10-13 16:47:06[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:06[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0549  0.0296  0.0814 -0.0742  0.0172  0.0442 -0.0807  0.0765 -0.0746  0.0713
 0.0747  0.0327 -0.0912 -0.0736  0.0385  0.0180 -0.0357 -0.0664 -0.0896 -0.0797
-0.0729 -0.0807  0.0697 -0.0018  0.0694 -0.0924  0.0310 -0.0533  0.0503  0.0376
-0.0264 -0.0227 -0.0258  0.0121  0.0793  0.0144 -0.0117  0.0570  0.0855  0.0203
 0.0631 -0.0573  0.0453  0.0237 -0.0064 -0.0603 -0.0475  0.0805  0.0421  0.0265
 0.0052 -0.0163 -0.0635 -0.0540  0.0820  0.1007 -0.0769  0.0337  0.0958 -0.0702

Columns 11 to 20
-0.0915 -0.0503 -0.0694  0.0427 -0.0894 -0.0579  0.0119 -0.0370  0.0583  0.0192
-0.0774  0.0519  0.0166  0.0910  0.0389 -0.0274  0.0332 -0.0862 -0.0022  0.0823
 0.0881 -0.0115  0.0330 -0.0714  0.0881  0.0579  0.0256 -0.0838  0.0989 -0.0615
-0.0952 -0.0684  0.0502  0.0009  0.0727  0.0050 -0.0766  0.0358  0.0097  0.0960
-0.0562  0.0325  0.0930 -0.0449 -0.0828 -0.0176 -0.0110  0.0575  0.0858 -0.0220
-0.0059  0.0963  0.1003 -0.0697 -0.0226  0.0224 -0.0247 -0.0133  0.0422  0.0116

Columns 21 to 30
-0.0489  0.0222  0.0448  0.0612 -0.0560  0.0507  0.0907 -0.0814  0.0617  0.0626
-0.0540 -0.0808 -0.0816  0.0056  0.0034 -0.0011  0.0050 -0.0143 -0.0924  0.0789
 0.0024 -0.0303 -0.0957 -0.0498 -0.0059  0.0956 -0.0588 -0.0961  0.0659 -0.0751
 0.0885  0.0329  0.0214 -0.0285 -0.0822  0.0326 -0.0667  0.0576  0.0148 -0.0389
 0.0582  0.0047 -0.0107  0.0345  0.0797 -0.0092 -0.0850 -0.0457 -0.0377  0.0256
 0.0666  0.0225 -0.0959 -0.0751 -0.0164 -0.0188 -0.0389 -0.0703  0.0458 -0.0662

Columns 31 to 40
 0.0121 -0.0048 -0.0676 -0.0170  0.0991 -0.0984  0.0834 -0.0370  0.0178  0.0242
 0.0241 -0.0630  0.0163 -0.0873  0.0631  0.0058  0.0197  0.0052  0.0487  0.0140
 0.0107 -0.0583  0.0210  0.0882 -0.0275 -0.0741 -0.0218  0.0351  0.0536  0.0569
-0.1009 -0.0217  0.0896 -0.0026 -0.0222  0.0637  0.0855 -0.0643  0.0312 -0.0443
-0.0368  0.0573  0.0253 -0.0937  0.0310  0.0990 -0.0156  0.0168  0.0565  0.0096
-0.0032  0.0023 -0.0098 -0.0864  0.0737 -0.0699  0.0373  0.0015  0.0236  0.0417

Columns 41 to 50
-0.0527  0.0050 -0.0876 -0.0509  0.0065 -0.0948  0.0613 -0.0598  0.0637  0.0957
 0.0844 -0.0002  0.0245  0.0201  0.0137  0.0355 -0.0678  0.0658 -0.0934 -0.0389
 0.0739  0.0570  0.0906  0.0890  0.0536  0.0815 -0.0641  0.0887 -0.0711 -0.0053
-0.0904  0.0466  0.0166 -0.0002 -0.0228  0.0635  0.0325 -0.0359 -0.0362 -0.0160
-0.0403 -0.0358  0.0239  0.0544 -0.0047  0.0581 -0.0821  0.0216 -0.0058 -0.1003
-0.0639  0.0762 -0.0603  0.0096 -0.0064 -0.0560 -0.0980  0.0507  0.0794  0.0086

Columns 51 to 60
-0.0975  0.0971  0.0794  0.0148  0.0661 -0.0872 -0.0051  0.0540 -0.0258  0.0728
 0.0308  0.0443 -0.0114  0.0756 -0.0002 -0.0130 -0.0186  0.0968  0.0651 -0.0073
-0.0733 -0.0689  0.0693  0.0055  0.0470  0.0324 -0.0733  0.0783 -0.0212  0.0554
-0.0075 -0.0341  0.0324 -0.0662 -0.0966 -0.0919 -0.0351  0.0594  0.0432  0.0475
 0.0193 -0.0171  0.0315  0.0799 -0.0111  0.0171  0.0528  0.0583  0.0405  0.0628
 0.0233 -0.0734 -0.0467 -0.0164 -0.0548  0.0940 -0.0276  0.0632 -0.0416  0.0314

Columns 61 to 70
-0.0418  0.0497 -0.0381 -0.0933 -0.0604  0.0736 -0.0125  0.0552  0.0813 -0.0670
-0.0756  0.0068  0.0732  0.0546  0.0323 -0.0962 -0.0926  0.0368  0.0084  0.0419
-0.0967  0.0852 -0.0458  0.0907 -0.0377 -0.0218 -0.0959 -0.0511  0.0552 -0.0534
 0.0517 -0.0535  0.0005  0.0978 -0.0563 -0.0369 -0.0801  0.0628 -0.0943 -0.0325
 0.0858 -0.0142 -0.0676 -0.0434  0.0870  0.0718 -0.0273 -0.0437 -0.0328 -0.0958
-0.0124  0.0177 -0.0823  0.0011  0.0484  0.0866  0.0940  0.0147 -0.0184 -0.0259

Columns 71 to 80
-0.0357 -0.0327 -0.0726 -0.0352  0.0991 -0.0210  0.0477 -0.0495 -0.0206  0.0794
-0.0163 -0.0361 -0.0119 -0.0949 -0.0705  0.0986 -0.0544  0.0476  0.0472  0.0424
-0.0254  0.0285  0.0640  0.0069  0.0252 -0.0698 -0.0140  0.0208 -0.0720 -0.0930
 0.0525 -0.0175 -0.0395 -0.0734 -0.0049  0.0942 -0.0295 -0.0951  0.0979 -0.0052
-0.0253  0.0368 -0.0720 -0.0311  0.0490  0.0093  0.0507  0.0406  0.0779  0.0297
-0.0816 -0.0320  0.0182  0.0922  0.0207 -0.0867 -0.0744 -0.0644 -0.0788  0.0718

Columns 81 to 90
-0.0197 -0.0990  0.0807  0.0116  0.0577 -0.0945  0.0354 -0.0128 -0.0784  0.0083
-0.0903  0.0799 -0.0982  0.0639  0.0963 -0.0436 -0.0062  0.0509 -0.0438 -0.0145
-0.0099 -0.0817 -0.0275  0.0620  0.0207  0.0708  0.0726  0.0490  0.0627  0.0223
-0.0517  0.0733 -0.0565  0.0540 -0.0221 -0.0177  0.0085 -0.0559 -0.0883  0.0313
-0.0148  0.0869 -0.0564  0.0879  0.0354  0.0959  0.0541  0.0167 -0.0874 -0.0695
 0.0839 -0.0937  0.0842  0.0139  0.0282  0.0584 -0.0465  0.0279 -0.0628 -0.0268

Columns 91 to 100
 0.0128  0.0991  0.0660  0.0255 -0.0269 -0.0215  0.0554 -0.0033  0.0151  0.0474
-0.0758  0.0577 -0.0249 -0.0961 -0.0502  0.0422 -0.0701  0.0820  0.0297 -0.0686
-0.0490 -0.0512  0.0343 -0.0605 -0.0540 -0.0473 -0.0135 -0.0724  0.0199 -0.0422
 0.0885  0.0537 -0.0616  0.0655 -0.0290 -0.0801 -0.0514  0.0568  0.0864  0.0306
-0.0228  0.0268 -0.0941  0.0362  0.0831 -0.0546  0.0227 -0.0120  0.0021 -0.0730
-0.0394  0.0336 -0.0035  0.0859  0.0885 -0.0642  0.0668  0.0398  0.0177  0.0209
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:06[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:06[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:06[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 25 [batchSize = 1]
 
2016-10-13 16:47:07[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.377048492432ms 
2016-10-13 16:47:07[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:07[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0548  0.0296  0.0814 -0.0742  0.0172  0.0441 -0.0807  0.0765 -0.0746  0.0714
 0.0747  0.0327 -0.0912 -0.0736  0.0385  0.0181 -0.0357 -0.0664 -0.0896 -0.0797
-0.0729 -0.0807  0.0697 -0.0018  0.0694 -0.0924  0.0310 -0.0533  0.0502  0.0376
-0.0264 -0.0228 -0.0258  0.0122  0.0793  0.0144 -0.0117  0.0571  0.0855  0.0203
 0.0631 -0.0573  0.0453  0.0237 -0.0064 -0.0603 -0.0475  0.0805  0.0421  0.0265
 0.0052 -0.0163 -0.0635 -0.0540  0.0820  0.1007 -0.0769  0.0338  0.0958 -0.0702

Columns 11 to 20
-0.0914 -0.0503 -0.0694  0.0427 -0.0894 -0.0579  0.0119 -0.0370  0.0583  0.0192
-0.0774  0.0519  0.0166  0.0910  0.0389 -0.0274  0.0332 -0.0863 -0.0022  0.0823
 0.0881 -0.0115  0.0330 -0.0715  0.0881  0.0579  0.0256 -0.0838  0.0989 -0.0615
-0.0952 -0.0684  0.0502  0.0009  0.0727  0.0051 -0.0767  0.0358  0.0097  0.0960
-0.0562  0.0325  0.0930 -0.0449 -0.0828 -0.0176 -0.0110  0.0575  0.0858 -0.0221
-0.0059  0.0963  0.1003 -0.0697 -0.0226  0.0224 -0.0247 -0.0133  0.0422  0.0116

Columns 21 to 30
-0.0489  0.0222  0.0449  0.0612 -0.0560  0.0507  0.0906 -0.0814  0.0617  0.0626
-0.0540 -0.0808 -0.0816  0.0056  0.0034 -0.0011  0.0050 -0.0143 -0.0924  0.0788
 0.0024 -0.0303 -0.0956 -0.0498 -0.0059  0.0956 -0.0588 -0.0961  0.0659 -0.0751
 0.0885  0.0329  0.0214 -0.0284 -0.0822  0.0326 -0.0668  0.0577  0.0148 -0.0389
 0.0582  0.0047 -0.0106  0.0345  0.0797 -0.0092 -0.0850 -0.0457 -0.0377  0.0256
 0.0665  0.0225 -0.0959 -0.0751 -0.0164 -0.0188 -0.0390 -0.0703  0.0458 -0.0663

Columns 31 to 40
 0.0121 -0.0048 -0.0676 -0.0170  0.0991 -0.0984  0.0834 -0.0370  0.0178  0.0242
 0.0241 -0.0630  0.0163 -0.0873  0.0631  0.0059  0.0197  0.0052  0.0486  0.0140
 0.0107 -0.0582  0.0210  0.0882 -0.0275 -0.0741 -0.0218  0.0351  0.0536  0.0570
-0.1010 -0.0217  0.0896 -0.0026 -0.0222  0.0637  0.0855 -0.0642  0.0312 -0.0442
-0.0368  0.0574  0.0253 -0.0937  0.0310  0.0990 -0.0156  0.0169  0.0565  0.0096
-0.0032  0.0023 -0.0098 -0.0864  0.0737 -0.0698  0.0373  0.0016  0.0236  0.0417

Columns 41 to 50
-0.0527  0.0050 -0.0876 -0.0509  0.0065 -0.0947  0.0612 -0.0598  0.0637  0.0957
 0.0844 -0.0002  0.0245  0.0200  0.0137  0.0355 -0.0678  0.0658 -0.0934 -0.0389
 0.0739  0.0570  0.0907  0.0889  0.0536  0.0815 -0.0642  0.0887 -0.0711 -0.0053
-0.0903  0.0466  0.0166 -0.0003 -0.0228  0.0635  0.0325 -0.0359 -0.0363 -0.0160
-0.0402 -0.0358  0.0240  0.0544 -0.0047  0.0581 -0.0821  0.0216 -0.0058 -0.1003
-0.0639  0.0762 -0.0603  0.0096 -0.0064 -0.0560 -0.0980  0.0507  0.0793  0.0086

Columns 51 to 60
-0.0975  0.0971  0.0794  0.0148  0.0661 -0.0872 -0.0051  0.0540 -0.0258  0.0728
 0.0308  0.0443 -0.0114  0.0756 -0.0002 -0.0130 -0.0186  0.0968  0.0651 -0.0073
-0.0733 -0.0689  0.0693  0.0055  0.0470  0.0324 -0.0733  0.0783 -0.0212  0.0554
-0.0075 -0.0341  0.0324 -0.0662 -0.0966 -0.0919 -0.0351  0.0594  0.0433  0.0475
 0.0193 -0.0171  0.0316  0.0800 -0.0111  0.0171  0.0528  0.0584  0.0405  0.0628
 0.0233 -0.0734 -0.0467 -0.0164 -0.0548  0.0940 -0.0276  0.0632 -0.0416  0.0314

Columns 61 to 70
-0.0418  0.0497 -0.0381 -0.0933 -0.0604  0.0736 -0.0125  0.0552  0.0813 -0.0670
-0.0755  0.0068  0.0732  0.0545  0.0323 -0.0962 -0.0926  0.0368  0.0084  0.0419
-0.0967  0.0852 -0.0458  0.0907 -0.0377 -0.0218 -0.0959 -0.0510  0.0552 -0.0534
 0.0518 -0.0534  0.0005  0.0978 -0.0563 -0.0369 -0.0801  0.0628 -0.0943 -0.0325
 0.0858 -0.0142 -0.0676 -0.0434  0.0870  0.0718 -0.0273 -0.0437 -0.0328 -0.0958
-0.0123  0.0177 -0.0823  0.0011  0.0484  0.0866  0.0940  0.0147 -0.0184 -0.0259

Columns 71 to 80
-0.0358 -0.0327 -0.0726 -0.0352  0.0990 -0.0210  0.0477 -0.0495 -0.0206  0.0794
-0.0163 -0.0360 -0.0119 -0.0949 -0.0705  0.0986 -0.0545  0.0476  0.0472  0.0424
-0.0255  0.0286  0.0640  0.0069  0.0252 -0.0697 -0.0140  0.0208 -0.0720 -0.0930
 0.0525 -0.0175 -0.0394 -0.0734 -0.0049  0.0943 -0.0296 -0.0951  0.0979 -0.0052
-0.0253  0.0368 -0.0719 -0.0311  0.0490  0.0093  0.0507  0.0406  0.0779  0.0297
-0.0816 -0.0319  0.0182  0.0922  0.0207 -0.0867 -0.0744 -0.0644 -0.0788  0.0718

Columns 81 to 90
-0.0197 -0.0990  0.0807  0.0117  0.0577 -0.0945  0.0354 -0.0128 -0.0784  0.0083
-0.0903  0.0799 -0.0982  0.0639  0.0963 -0.0436 -0.0062  0.0509 -0.0438 -0.0145
-0.0100 -0.0817 -0.0275  0.0621  0.0207  0.0708  0.0726  0.0490  0.0627  0.0223
-0.0517  0.0733 -0.0565  0.0541 -0.0221 -0.0177  0.0085 -0.0559 -0.0883  0.0313
-0.0148  0.0869 -0.0564  0.0879  0.0354  0.0959  0.0541  0.0167 -0.0874 -0.0695
 0.0839 -0.0937  0.0842  0.0140  0.0282  0.0584 -0.0465  0.0279 -0.0628 -0.0268

Columns 91 to 100
 0.0128  0.0991  0.0660  0.0255 -0.0269 -0.0215  0.0554 -0.0033  0.0151  0.0474
-0.0758  0.0577 -0.0249 -0.0961 -0.0502  0.0422 -0.0701  0.0820  0.0297 -0.0686
-0.0491 -0.0512  0.0343 -0.0605 -0.0539 -0.0473 -0.0135 -0.0724  0.0199 -0.0421
 0.0885  0.0537 -0.0616  0.0654 -0.0290 -0.0801 -0.0513  0.0568  0.0863  0.0307
-0.0228  0.0268 -0.0941  0.0361  0.0831 -0.0546  0.0227 -0.0121  0.0020 -0.0730
-0.0394  0.0336 -0.0035  0.0859  0.0885 -0.0642  0.0668  0.0398  0.0176  0.0209
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:07[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:07[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:07[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 26 [batchSize = 1]
 
2016-10-13 16:47:07[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 70.943355560303ms 
2016-10-13 16:47:07[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:07[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0548  0.0296  0.0814 -0.0742  0.0172  0.0441 -0.0807  0.0765 -0.0746  0.0713
 0.0747  0.0327 -0.0912 -0.0736  0.0385  0.0181 -0.0357 -0.0664 -0.0897 -0.0797
-0.0729 -0.0807  0.0696 -0.0017  0.0694 -0.0924  0.0310 -0.0533  0.0502  0.0376
-0.0264 -0.0228 -0.0258  0.0123  0.0793  0.0144 -0.0118  0.0571  0.0854  0.0203
 0.0631 -0.0574  0.0453  0.0237 -0.0064 -0.0603 -0.0475  0.0805  0.0421  0.0265
 0.0052 -0.0163 -0.0635 -0.0540  0.0820  0.1007 -0.0769  0.0338  0.0958 -0.0701

Columns 11 to 20
-0.0914 -0.0504 -0.0694  0.0426 -0.0894 -0.0579  0.0119 -0.0370  0.0583  0.0192
-0.0774  0.0519  0.0167  0.0910  0.0388 -0.0274  0.0332 -0.0863 -0.0022  0.0822
 0.0881 -0.0115  0.0330 -0.0715  0.0881  0.0579  0.0256 -0.0838  0.0989 -0.0616
-0.0952 -0.0684  0.0503  0.0009  0.0727  0.0051 -0.0767  0.0358  0.0097  0.0959
-0.0562  0.0325  0.0931 -0.0449 -0.0828 -0.0176 -0.0110  0.0575  0.0858 -0.0221
-0.0058  0.0963  0.1003 -0.0697 -0.0226  0.0224 -0.0247 -0.0133  0.0422  0.0116

Columns 21 to 30
-0.0489  0.0222  0.0449  0.0612 -0.0560  0.0507  0.0906 -0.0813  0.0617  0.0626
-0.0540 -0.0808 -0.0815  0.0056  0.0034 -0.0011  0.0049 -0.0143 -0.0924  0.0788
 0.0024 -0.0303 -0.0956 -0.0498 -0.0059  0.0956 -0.0589 -0.0960  0.0659 -0.0752
 0.0884  0.0329  0.0214 -0.0284 -0.0822  0.0326 -0.0669  0.0578  0.0148 -0.0389
 0.0582  0.0047 -0.0106  0.0345  0.0797 -0.0092 -0.0851 -0.0457 -0.0377  0.0256
 0.0665  0.0225 -0.0959 -0.0751 -0.0164 -0.0188 -0.0390 -0.0703  0.0458 -0.0663

Columns 31 to 40
 0.0120 -0.0048 -0.0676 -0.0170  0.0991 -0.0984  0.0833 -0.0370  0.0178  0.0242
 0.0240 -0.0629  0.0163 -0.0873  0.0631  0.0059  0.0197  0.0053  0.0486  0.0140
 0.0107 -0.0582  0.0210  0.0882 -0.0275 -0.0740 -0.0218  0.0351  0.0536  0.0570
-0.1011 -0.0216  0.0896 -0.0026 -0.0222  0.0637  0.0855 -0.0642  0.0312 -0.0442
-0.0368  0.0574  0.0253 -0.0937  0.0310  0.0990 -0.0156  0.0169  0.0565  0.0096
-0.0032  0.0023 -0.0098 -0.0864  0.0737 -0.0698  0.0372  0.0016  0.0236  0.0417

Columns 41 to 50
-0.0527  0.0050 -0.0876 -0.0509  0.0065 -0.0947  0.0612 -0.0598  0.0636  0.0957
 0.0844 -0.0002  0.0245  0.0200  0.0137  0.0355 -0.0678  0.0658 -0.0935 -0.0390
 0.0739  0.0570  0.0907  0.0889  0.0536  0.0815 -0.0642  0.0887 -0.0711 -0.0054
-0.0903  0.0466  0.0167 -0.0004 -0.0228  0.0635  0.0325 -0.0359 -0.0364 -0.0161
-0.0402 -0.0357  0.0240  0.0543 -0.0047  0.0581 -0.0821  0.0216 -0.0058 -0.1003
-0.0639  0.0762 -0.0603  0.0096 -0.0064 -0.0560 -0.0981  0.0508  0.0793  0.0086

Columns 51 to 60
-0.0975  0.0971  0.0794  0.0148  0.0662 -0.0872 -0.0051  0.0540 -0.0258  0.0728
 0.0308  0.0443 -0.0114  0.0757 -0.0001 -0.0130 -0.0186  0.0968  0.0651 -0.0073
-0.0733 -0.0689  0.0693  0.0056  0.0471  0.0324 -0.0733  0.0783 -0.0212  0.0554
-0.0075 -0.0341  0.0325 -0.0662 -0.0965 -0.0919 -0.0351  0.0595  0.0433  0.0475
 0.0193 -0.0171  0.0316  0.0800 -0.0111  0.0170  0.0528  0.0584  0.0405  0.0628
 0.0233 -0.0734 -0.0466 -0.0164 -0.0547  0.0940 -0.0276  0.0632 -0.0416  0.0314

Columns 61 to 70
-0.0418  0.0497 -0.0381 -0.0933 -0.0604  0.0736 -0.0125  0.0552  0.0813 -0.0670
-0.0755  0.0069  0.0732  0.0545  0.0323 -0.0962 -0.0926  0.0369  0.0085  0.0418
-0.0967  0.0852 -0.0458  0.0907 -0.0377 -0.0218 -0.0959 -0.0510  0.0552 -0.0534
 0.0518 -0.0533  0.0005  0.0977 -0.0564 -0.0368 -0.0801  0.0629 -0.0942 -0.0326
 0.0858 -0.0142 -0.0676 -0.0434  0.0870  0.0718 -0.0273 -0.0437 -0.0327 -0.0958
-0.0123  0.0177 -0.0823  0.0010  0.0484  0.0866  0.0940  0.0147 -0.0184 -0.0259

Columns 71 to 80
-0.0358 -0.0327 -0.0726 -0.0352  0.0990 -0.0210  0.0477 -0.0495 -0.0206  0.0794
-0.0163 -0.0360 -0.0118 -0.0949 -0.0705  0.0987 -0.0545  0.0476  0.0472  0.0424
-0.0255  0.0286  0.0640  0.0069  0.0252 -0.0697 -0.0141  0.0208 -0.0720 -0.0930
 0.0525 -0.0174 -0.0394 -0.0733 -0.0049  0.0944 -0.0297 -0.0951  0.0979 -0.0052
-0.0253  0.0368 -0.0719 -0.0310  0.0490  0.0094  0.0507  0.0406  0.0779  0.0297
-0.0816 -0.0319  0.0182  0.0922  0.0207 -0.0867 -0.0744 -0.0644 -0.0788  0.0718

Columns 81 to 90
-0.0197 -0.0990  0.0807  0.0117  0.0577 -0.0945  0.0354 -0.0128 -0.0784  0.0083
-0.0903  0.0799 -0.0982  0.0639  0.0963 -0.0436 -0.0062  0.0508 -0.0439 -0.0145
-0.0100 -0.0817 -0.0274  0.0621  0.0207  0.0708  0.0726  0.0490  0.0626  0.0223
-0.0517  0.0733 -0.0565  0.0542 -0.0221 -0.0177  0.0085 -0.0559 -0.0883  0.0313
-0.0148  0.0869 -0.0564  0.0879  0.0354  0.0959  0.0541  0.0167 -0.0874 -0.0695
 0.0839 -0.0937  0.0842  0.0140  0.0282  0.0584 -0.0465  0.0279 -0.0628 -0.0268

Columns 91 to 100
 0.0128  0.0991  0.0660  0.0255 -0.0269 -0.0215  0.0554 -0.0033  0.0151  0.0474
-0.0759  0.0577 -0.0249 -0.0962 -0.0502  0.0422 -0.0701  0.0820  0.0296 -0.0685
-0.0491 -0.0512  0.0343 -0.0606 -0.0539 -0.0473 -0.0135 -0.0724  0.0199 -0.0421
 0.0885  0.0537 -0.0616  0.0654 -0.0290 -0.0801 -0.0513  0.0567  0.0863  0.0307
-0.0228  0.0268 -0.0941  0.0361  0.0831 -0.0546  0.0227 -0.0121  0.0020 -0.0730
-0.0394  0.0336 -0.0035  0.0858  0.0885 -0.0642  0.0668  0.0398  0.0176  0.0209
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:07[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:07[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:07[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 27 [batchSize = 1]
 
2016-10-13 16:47:07[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.341683069865ms 
2016-10-13 16:47:07[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:07[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0548  0.0296  0.0814 -0.0741  0.0172  0.0442 -0.0807  0.0765 -0.0746  0.0714
 0.0747  0.0326 -0.0912 -0.0735  0.0385  0.0181 -0.0358 -0.0663 -0.0897 -0.0797
-0.0729 -0.0808  0.0697 -0.0017  0.0694 -0.0923  0.0310 -0.0533  0.0502  0.0376
-0.0263 -0.0230 -0.0258  0.0124  0.0793  0.0147 -0.0118  0.0572  0.0854  0.0203
 0.0631 -0.0574  0.0453  0.0238 -0.0064 -0.0602 -0.0475  0.0805  0.0421  0.0265
 0.0052 -0.0163 -0.0635 -0.0539  0.0820  0.1008 -0.0769  0.0338  0.0958 -0.0701

Columns 11 to 20
-0.0914 -0.0503 -0.0694  0.0426 -0.0894 -0.0579  0.0119 -0.0370  0.0583  0.0192
-0.0773  0.0519  0.0167  0.0910  0.0388 -0.0274  0.0332 -0.0863 -0.0022  0.0822
 0.0881 -0.0115  0.0331 -0.0715  0.0881  0.0579  0.0256 -0.0838  0.0989 -0.0616
-0.0952 -0.0684  0.0503  0.0009  0.0727  0.0051 -0.0767  0.0358  0.0097  0.0958
-0.0562  0.0326  0.0931 -0.0449 -0.0828 -0.0176 -0.0111  0.0574  0.0858 -0.0221
-0.0058  0.0964  0.1004 -0.0697 -0.0226  0.0224 -0.0247 -0.0133  0.0422  0.0116

Columns 21 to 30
-0.0489  0.0222  0.0449  0.0612 -0.0560  0.0507  0.0906 -0.0813  0.0617  0.0626
-0.0540 -0.0808 -0.0815  0.0057  0.0034 -0.0011  0.0049 -0.0142 -0.0924  0.0788
 0.0023 -0.0303 -0.0956 -0.0498 -0.0059  0.0956 -0.0589 -0.0960  0.0659 -0.0752
 0.0884  0.0329  0.0214 -0.0283 -0.0822  0.0326 -0.0670  0.0578  0.0148 -0.0390
 0.0582  0.0047 -0.0106  0.0345  0.0797 -0.0092 -0.0851 -0.0457 -0.0377  0.0256
 0.0665  0.0225 -0.0959 -0.0751 -0.0164 -0.0188 -0.0390 -0.0703  0.0458 -0.0663

Columns 31 to 40
 0.0120 -0.0047 -0.0676 -0.0171  0.0991 -0.0983  0.0833 -0.0370  0.0178  0.0242
 0.0240 -0.0629  0.0163 -0.0873  0.0631  0.0059  0.0196  0.0053  0.0486  0.0140
 0.0107 -0.0582  0.0210  0.0882 -0.0275 -0.0740 -0.0219  0.0352  0.0536  0.0570
-0.1011 -0.0215  0.0895 -0.0027 -0.0223  0.0637  0.0853 -0.0641  0.0311 -0.0442
-0.0369  0.0574  0.0253 -0.0937  0.0310  0.0990 -0.0157  0.0169  0.0564  0.0096
-0.0032  0.0023 -0.0098 -0.0864  0.0737 -0.0698  0.0372  0.0016  0.0236  0.0417

Columns 41 to 50
-0.0526  0.0050 -0.0875 -0.0509  0.0065 -0.0947  0.0612 -0.0597  0.0636  0.0957
 0.0844 -0.0002  0.0245  0.0199  0.0137  0.0355 -0.0678  0.0658 -0.0935 -0.0390
 0.0740  0.0570  0.0907  0.0889  0.0536  0.0815 -0.0642  0.0887 -0.0712 -0.0054
-0.0902  0.0467  0.0168 -0.0004 -0.0228  0.0635  0.0325 -0.0358 -0.0365 -0.0161
-0.0402 -0.0357  0.0240  0.0543 -0.0047  0.0581 -0.0821  0.0216 -0.0059 -0.1004
-0.0638  0.0762 -0.0603  0.0096 -0.0064 -0.0560 -0.0981  0.0508  0.0793  0.0086

Columns 51 to 60
-0.0975  0.0971  0.0795  0.0148  0.0662 -0.0872 -0.0052  0.0541 -0.0258  0.0728
 0.0308  0.0443 -0.0113  0.0757 -0.0001 -0.0130 -0.0187  0.0968  0.0652 -0.0073
-0.0733 -0.0689  0.0694  0.0056  0.0471  0.0324 -0.0733  0.0784 -0.0212  0.0554
-0.0074 -0.0340  0.0326 -0.0661 -0.0965 -0.0919 -0.0352  0.0595  0.0434  0.0475
 0.0193 -0.0171  0.0317  0.0800 -0.0110  0.0170  0.0528  0.0584  0.0406  0.0628
 0.0233 -0.0734 -0.0466 -0.0164 -0.0547  0.0940 -0.0276  0.0632 -0.0416  0.0314

Columns 61 to 70
-0.0417  0.0497 -0.0381 -0.0933 -0.0604  0.0736 -0.0125  0.0552  0.0813 -0.0670
-0.0755  0.0069  0.0731  0.0545  0.0323 -0.0962 -0.0926  0.0369  0.0085  0.0418
-0.0966  0.0852 -0.0458  0.0906 -0.0377 -0.0218 -0.0959 -0.0510  0.0553 -0.0535
 0.0519 -0.0533  0.0004  0.0977 -0.0564 -0.0368 -0.0801  0.0629 -0.0941 -0.0327
 0.0858 -0.0142 -0.0676 -0.0434  0.0870  0.0719 -0.0273 -0.0437 -0.0327 -0.0959
-0.0123  0.0177 -0.0823  0.0010  0.0484  0.0866  0.0940  0.0147 -0.0184 -0.0260

Columns 71 to 80
-0.0358 -0.0326 -0.0725 -0.0352  0.0990 -0.0209  0.0476 -0.0495 -0.0206  0.0794
-0.0163 -0.0360 -0.0118 -0.0948 -0.0705  0.0987 -0.0545  0.0476  0.0472  0.0424
-0.0255  0.0286  0.0640  0.0070  0.0252 -0.0697 -0.0141  0.0208 -0.0720 -0.0931
 0.0524 -0.0174 -0.0393 -0.0733 -0.0050  0.0945 -0.0298 -0.0951  0.0979 -0.0052
-0.0253  0.0368 -0.0719 -0.0310  0.0490  0.0094  0.0507  0.0406  0.0779  0.0296
-0.0816 -0.0319  0.0182  0.0922  0.0207 -0.0866 -0.0744 -0.0644 -0.0788  0.0718

Columns 81 to 90
-0.0197 -0.0990  0.0807  0.0117  0.0577 -0.0945  0.0354 -0.0128 -0.0784  0.0083
-0.0903  0.0799 -0.0981  0.0640  0.0962 -0.0436 -0.0063  0.0508 -0.0439 -0.0145
-0.0100 -0.0817 -0.0274  0.0621  0.0207  0.0708  0.0726  0.0490  0.0626  0.0223
-0.0518  0.0733 -0.0564  0.0542 -0.0221 -0.0177  0.0085 -0.0559 -0.0884  0.0313
-0.0148  0.0869 -0.0564  0.0879  0.0354  0.0959  0.0541  0.0166 -0.0874 -0.0695
 0.0839 -0.0937  0.0842  0.0140  0.0282  0.0584 -0.0465  0.0279 -0.0628 -0.0268

Columns 91 to 100
 0.0128  0.0992  0.0660  0.0255 -0.0269 -0.0215  0.0554 -0.0034  0.0150  0.0475
-0.0759  0.0577 -0.0249 -0.0962 -0.0501  0.0422 -0.0701  0.0819  0.0296 -0.0685
-0.0491 -0.0512  0.0343 -0.0606 -0.0539 -0.0473 -0.0135 -0.0724  0.0198 -0.0421
 0.0885  0.0538 -0.0616  0.0653 -0.0289 -0.0801 -0.0512  0.0566  0.0862  0.0308
-0.0228  0.0269 -0.0941  0.0361  0.0832 -0.0546  0.0227 -0.0121  0.0020 -0.0729
-0.0394  0.0336 -0.0035  0.0858  0.0885 -0.0642  0.0669  0.0397  0.0176  0.0209
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:07[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:07[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:07[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 28 [batchSize = 1]
 
2016-10-13 16:47:07[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.364650726318ms 
2016-10-13 16:47:07[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:07[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0548  0.0295  0.0814 -0.0741  0.0172  0.0443 -0.0808  0.0765 -0.0746  0.0714
 0.0747  0.0326 -0.0912 -0.0735  0.0385  0.0182 -0.0358 -0.0663 -0.0897 -0.0796
-0.0729 -0.0808  0.0697 -0.0017  0.0694 -0.0922  0.0310 -0.0533  0.0502  0.0376
-0.0262 -0.0230 -0.0258  0.0124  0.0793  0.0148 -0.0118  0.0572  0.0853  0.0203
 0.0631 -0.0574  0.0453  0.0238 -0.0064 -0.0602 -0.0475  0.0805  0.0420  0.0265
 0.0052 -0.0163 -0.0635 -0.0539  0.0820  0.1008 -0.0769  0.0338  0.0958 -0.0701

Columns 11 to 20
-0.0914 -0.0503 -0.0693  0.0426 -0.0894 -0.0579  0.0119 -0.0371  0.0583  0.0192
-0.0773  0.0520  0.0167  0.0910  0.0388 -0.0274  0.0332 -0.0863 -0.0022  0.0822
 0.0881 -0.0115  0.0331 -0.0715  0.0881  0.0579  0.0256 -0.0839  0.0989 -0.0616
-0.0951 -0.0683  0.0504  0.0008  0.0726  0.0052 -0.0768  0.0357  0.0097  0.0958
-0.0562  0.0326  0.0931 -0.0450 -0.0828 -0.0176 -0.0111  0.0574  0.0858 -0.0221
-0.0058  0.0964  0.1004 -0.0697 -0.0226  0.0224 -0.0247 -0.0133  0.0422  0.0116

Columns 21 to 30
-0.0489  0.0222  0.0449  0.0612 -0.0560  0.0507  0.0905 -0.0813  0.0617  0.0626
-0.0541 -0.0808 -0.0815  0.0057  0.0034 -0.0011  0.0048 -0.0142 -0.0924  0.0788
 0.0023 -0.0303 -0.0956 -0.0498 -0.0059  0.0956 -0.0589 -0.0960  0.0659 -0.0752
 0.0884  0.0329  0.0215 -0.0283 -0.0823  0.0326 -0.0671  0.0579  0.0148 -0.0391
 0.0582  0.0047 -0.0106  0.0345  0.0796 -0.0092 -0.0851 -0.0456 -0.0377  0.0255
 0.0665  0.0225 -0.0959 -0.0751 -0.0164 -0.0188 -0.0390 -0.0703  0.0458 -0.0663

Columns 31 to 40
 0.0120 -0.0047 -0.0676 -0.0171  0.0991 -0.0983  0.0832 -0.0369  0.0178  0.0243
 0.0240 -0.0629  0.0163 -0.0874  0.0631  0.0059  0.0196  0.0053  0.0486  0.0141
 0.0106 -0.0581  0.0210  0.0882 -0.0275 -0.0740 -0.0219  0.0352  0.0536  0.0570
-0.1012 -0.0214  0.0895 -0.0028 -0.0223  0.0638  0.0852 -0.0641  0.0311 -0.0442
-0.0369  0.0574  0.0252 -0.0937  0.0310  0.0990 -0.0157  0.0169  0.0564  0.0096
-0.0033  0.0024 -0.0098 -0.0864  0.0737 -0.0698  0.0372  0.0016  0.0236  0.0418

Columns 41 to 50
-0.0526  0.0050 -0.0875 -0.0510  0.0065 -0.0947  0.0612 -0.0597  0.0636  0.0956
 0.0845 -0.0002  0.0246  0.0199  0.0137  0.0355 -0.0678  0.0658 -0.0935 -0.0390
 0.0740  0.0570  0.0907  0.0888  0.0536  0.0815 -0.0642  0.0887 -0.0712 -0.0054
-0.0902  0.0467  0.0168 -0.0005 -0.0228  0.0635  0.0325 -0.0358 -0.0366 -0.0162
-0.0402 -0.0357  0.0240  0.0543 -0.0047  0.0581 -0.0821  0.0217 -0.0059 -0.1004
-0.0638  0.0762 -0.0602  0.0095 -0.0064 -0.0560 -0.0981  0.0508  0.0792  0.0085

Columns 51 to 60
-0.0975  0.0971  0.0795  0.0149  0.0662 -0.0873 -0.0052  0.0541 -0.0258  0.0728
 0.0308  0.0443 -0.0113  0.0757 -0.0001 -0.0130 -0.0187  0.0969  0.0652 -0.0073
-0.0733 -0.0688  0.0694  0.0056  0.0471  0.0323 -0.0734  0.0784 -0.0211  0.0554
-0.0074 -0.0340  0.0327 -0.0661 -0.0964 -0.0920 -0.0353  0.0596  0.0435  0.0475
 0.0193 -0.0171  0.0317  0.0800 -0.0110  0.0170  0.0528  0.0584  0.0406  0.0628
 0.0233 -0.0734 -0.0466 -0.0164 -0.0547  0.0940 -0.0276  0.0632 -0.0416  0.0314

Columns 61 to 70
-0.0417  0.0497 -0.0381 -0.0933 -0.0604  0.0736 -0.0125  0.0552  0.0813 -0.0671
-0.0754  0.0069  0.0731  0.0544  0.0323 -0.0962 -0.0926  0.0369  0.0085  0.0418
-0.0966  0.0853 -0.0458  0.0906 -0.0377 -0.0218 -0.0959 -0.0510  0.0553 -0.0535
 0.0520 -0.0532  0.0004  0.0976 -0.0564 -0.0368 -0.0801  0.0629 -0.0940 -0.0328
 0.0859 -0.0141 -0.0676 -0.0435  0.0870  0.0719 -0.0273 -0.0436 -0.0327 -0.0959
-0.0123  0.0177 -0.0823  0.0010  0.0484  0.0866  0.0940  0.0147 -0.0184 -0.0260

Columns 71 to 80
-0.0358 -0.0326 -0.0725 -0.0352  0.0990 -0.0209  0.0476 -0.0495 -0.0206  0.0794
-0.0163 -0.0360 -0.0118 -0.0948 -0.0706  0.0988 -0.0546  0.0477  0.0472  0.0424
-0.0255  0.0286  0.0640  0.0070  0.0252 -0.0696 -0.0141  0.0208 -0.0720 -0.0931
 0.0524 -0.0173 -0.0392 -0.0732 -0.0050  0.0946 -0.0298 -0.0951  0.0979 -0.0052
-0.0253  0.0369 -0.0719 -0.0310  0.0490  0.0094  0.0506  0.0406  0.0779  0.0296
-0.0816 -0.0319  0.0183  0.0922  0.0206 -0.0866 -0.0744 -0.0644 -0.0788  0.0718

Columns 81 to 90
-0.0197 -0.0990  0.0807  0.0117  0.0577 -0.0945  0.0354 -0.0128 -0.0784  0.0083
-0.0903  0.0799 -0.0981  0.0640  0.0962 -0.0436 -0.0063  0.0508 -0.0439 -0.0145
-0.0100 -0.0817 -0.0274  0.0622  0.0207  0.0708  0.0726  0.0490  0.0626  0.0223
-0.0518  0.0733 -0.0564  0.0543 -0.0222 -0.0177  0.0084 -0.0559 -0.0884  0.0313
-0.0148  0.0869 -0.0563  0.0880  0.0354  0.0959  0.0541  0.0166 -0.0874 -0.0695
 0.0838 -0.0937  0.0843  0.0140  0.0282  0.0584 -0.0465  0.0279 -0.0629 -0.0268

Columns 91 to 100
 0.0128  0.0992  0.0660  0.0255 -0.0269 -0.0215  0.0554 -0.0034  0.0150  0.0475
-0.0759  0.0578 -0.0249 -0.0962 -0.0501  0.0422 -0.0700  0.0819  0.0296 -0.0685
-0.0491 -0.0512  0.0343 -0.0606 -0.0539 -0.0473 -0.0134 -0.0725  0.0198 -0.0420
 0.0884  0.0538 -0.0617  0.0653 -0.0289 -0.0801 -0.0512  0.0566  0.0861  0.0310
-0.0228  0.0269 -0.0941  0.0361  0.0832 -0.0546  0.0227 -0.0121  0.0020 -0.0729
-0.0394  0.0336 -0.0035  0.0858  0.0886 -0.0642  0.0669  0.0397  0.0176  0.0210
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:07[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:07[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:07[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 29 [batchSize = 1]
 
2016-10-13 16:47:08[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.413606007894ms 
2016-10-13 16:47:08[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:08[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0548  0.0295  0.0814 -0.0741  0.0172  0.0443 -0.0808  0.0765 -0.0746  0.0714
 0.0747  0.0326 -0.0912 -0.0734  0.0385  0.0182 -0.0358 -0.0663 -0.0897 -0.0796
-0.0729 -0.0808  0.0697 -0.0016  0.0694 -0.0922  0.0310 -0.0532  0.0501  0.0376
-0.0262 -0.0231 -0.0258  0.0125  0.0793  0.0148 -0.0119  0.0572  0.0852  0.0203
 0.0631 -0.0575  0.0453  0.0238 -0.0064 -0.0601 -0.0476  0.0806  0.0420  0.0266
 0.0052 -0.0164 -0.0635 -0.0539  0.0820  0.1008 -0.0769  0.0338  0.0958 -0.0701

Columns 11 to 20
-0.0914 -0.0503 -0.0693  0.0426 -0.0894 -0.0579  0.0119 -0.0371  0.0583  0.0192
-0.0773  0.0520  0.0167  0.0909  0.0388 -0.0274  0.0332 -0.0863 -0.0022  0.0821
 0.0881 -0.0115  0.0331 -0.0715  0.0881  0.0579  0.0256 -0.0839  0.0989 -0.0616
-0.0951 -0.0683  0.0504  0.0008  0.0726  0.0052 -0.0768  0.0356  0.0097  0.0957
-0.0562  0.0326  0.0931 -0.0450 -0.0828 -0.0176 -0.0111  0.0574  0.0858 -0.0222
-0.0058  0.0964  0.1004 -0.0697 -0.0226  0.0225 -0.0247 -0.0133  0.0422  0.0116

Columns 21 to 30
-0.0489  0.0222  0.0449  0.0612 -0.0560  0.0507  0.0905 -0.0813  0.0617  0.0626
-0.0541 -0.0808 -0.0815  0.0057  0.0034 -0.0011  0.0048 -0.0142 -0.0925  0.0787
 0.0023 -0.0303 -0.0956 -0.0498 -0.0059  0.0956 -0.0589 -0.0960  0.0659 -0.0752
 0.0883  0.0330  0.0215 -0.0283 -0.0823  0.0325 -0.0671  0.0579  0.0148 -0.0391
 0.0581  0.0047 -0.0106  0.0345  0.0796 -0.0092 -0.0851 -0.0456 -0.0377  0.0255
 0.0665  0.0225 -0.0959 -0.0751 -0.0164 -0.0188 -0.0390 -0.0703  0.0458 -0.0663

Columns 31 to 40
 0.0120 -0.0047 -0.0676 -0.0171  0.0991 -0.0983  0.0832 -0.0369  0.0178  0.0242
 0.0240 -0.0628  0.0163 -0.0874  0.0631  0.0059  0.0196  0.0053  0.0486  0.0141
 0.0106 -0.0581  0.0209  0.0881 -0.0275 -0.0740 -0.0219  0.0352  0.0536  0.0570
-0.1012 -0.0214  0.0895 -0.0028 -0.0223  0.0638  0.0851 -0.0640  0.0311 -0.0442
-0.0369  0.0574  0.0252 -0.0937  0.0310  0.0990 -0.0157  0.0169  0.0564  0.0096
-0.0033  0.0024 -0.0098 -0.0864  0.0737 -0.0698  0.0372  0.0016  0.0236  0.0418

Columns 41 to 50
-0.0526  0.0050 -0.0875 -0.0510  0.0065 -0.0947  0.0612 -0.0597  0.0636  0.0956
 0.0845 -0.0002  0.0246  0.0199  0.0137  0.0355 -0.0678  0.0659 -0.0936 -0.0391
 0.0740  0.0570  0.0907  0.0888  0.0536  0.0815 -0.0642  0.0887 -0.0712 -0.0055
-0.0901  0.0467  0.0168 -0.0006 -0.0228  0.0635  0.0324 -0.0358 -0.0366 -0.0163
-0.0402 -0.0357  0.0240  0.0543 -0.0047  0.0581 -0.0821  0.0217 -0.0059 -0.1004
-0.0638  0.0762 -0.0602  0.0095 -0.0064 -0.0560 -0.0981  0.0508  0.0792  0.0085

Columns 51 to 60
-0.0975  0.0971  0.0795  0.0149  0.0662 -0.0873 -0.0052  0.0541 -0.0258  0.0728
 0.0308  0.0443 -0.0112  0.0757 -0.0001 -0.0130 -0.0187  0.0969  0.0652 -0.0073
-0.0733 -0.0688  0.0694  0.0056  0.0471  0.0323 -0.0734  0.0784 -0.0211  0.0554
-0.0074 -0.0340  0.0328 -0.0660 -0.0964 -0.0920 -0.0353  0.0596  0.0435  0.0475
 0.0193 -0.0171  0.0317  0.0800 -0.0110  0.0170  0.0528  0.0584  0.0406  0.0628
 0.0233 -0.0734 -0.0466 -0.0164 -0.0547  0.0940 -0.0276  0.0632 -0.0416  0.0314

Columns 61 to 70
-0.0417  0.0497 -0.0381 -0.0933 -0.0604  0.0736 -0.0125  0.0552  0.0813 -0.0671
-0.0754  0.0070  0.0731  0.0544  0.0323 -0.0962 -0.0926  0.0369  0.0085  0.0418
-0.0966  0.0853 -0.0458  0.0906 -0.0377 -0.0218 -0.0959 -0.0510  0.0553 -0.0535
 0.0521 -0.0532  0.0003  0.0975 -0.0564 -0.0368 -0.0801  0.0630 -0.0940 -0.0328
 0.0859 -0.0141 -0.0676 -0.0435  0.0870  0.0719 -0.0273 -0.0436 -0.0327 -0.0959
-0.0123  0.0177 -0.0823  0.0010  0.0484  0.0866  0.0940  0.0147 -0.0184 -0.0260

Columns 71 to 80
-0.0358 -0.0326 -0.0725 -0.0352  0.0990 -0.0209  0.0476 -0.0495 -0.0206  0.0794
-0.0164 -0.0360 -0.0118 -0.0948 -0.0706  0.0988 -0.0546  0.0477  0.0472  0.0424
-0.0255  0.0286  0.0641  0.0070  0.0251 -0.0696 -0.0141  0.0208 -0.0720 -0.0931
 0.0523 -0.0173 -0.0392 -0.0732 -0.0050  0.0947 -0.0299 -0.0951  0.0980 -0.0052
-0.0253  0.0369 -0.0719 -0.0310  0.0490  0.0095  0.0506  0.0406  0.0779  0.0296
-0.0816 -0.0319  0.0183  0.0923  0.0206 -0.0866 -0.0744 -0.0644 -0.0788  0.0718

Columns 81 to 90
-0.0197 -0.0990  0.0807  0.0117  0.0577 -0.0945  0.0354 -0.0128 -0.0785  0.0083
-0.0903  0.0799 -0.0981  0.0640  0.0962 -0.0435 -0.0063  0.0508 -0.0439 -0.0145
-0.0100 -0.0817 -0.0274  0.0622  0.0206  0.0708  0.0725  0.0490  0.0626  0.0223
-0.0518  0.0733 -0.0563  0.0544 -0.0222 -0.0176  0.0084 -0.0559 -0.0884  0.0313
-0.0148  0.0869 -0.0563  0.0880  0.0354  0.0959  0.0541  0.0166 -0.0874 -0.0695
 0.0838 -0.0937  0.0843  0.0140  0.0282  0.0584 -0.0465  0.0279 -0.0629 -0.0268

Columns 91 to 100
 0.0128  0.0992  0.0660  0.0255 -0.0269 -0.0215  0.0554 -0.0034  0.0150  0.0475
-0.0759  0.0578 -0.0249 -0.0962 -0.0501  0.0422 -0.0700  0.0819  0.0296 -0.0684
-0.0491 -0.0512  0.0343 -0.0607 -0.0539 -0.0473 -0.0134 -0.0725  0.0198 -0.0420
 0.0884  0.0538 -0.0617  0.0652 -0.0289 -0.0801 -0.0512  0.0565  0.0860  0.0310
-0.0228  0.0269 -0.0941  0.0360  0.0832 -0.0546  0.0227 -0.0121  0.0019 -0.0729
-0.0394  0.0336 -0.0035  0.0858  0.0886 -0.0642  0.0669  0.0397  0.0176  0.0210
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:08[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:08[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:08[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 30 [batchSize = 1]
 
2016-10-13 16:47:08[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.454296112061ms 
2016-10-13 16:47:08[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> saving network model to /home/robotec/catkin_ws/src/superchicko/farnn/src/network/glassfurnace_lstm-net.t7 
2016-10-13 16:47:08[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:08[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0548  0.0295  0.0814 -0.0740  0.0172  0.0443 -0.0808  0.0765 -0.0747  0.0714
 0.0748  0.0325 -0.0912 -0.0734  0.0385  0.0183 -0.0358 -0.0663 -0.0898 -0.0796
-0.0728 -0.0809  0.0697 -0.0016  0.0695 -0.0921  0.0309 -0.0532  0.0501  0.0377
-0.0262 -0.0232 -0.0258  0.0126  0.0793  0.0150 -0.0119  0.0573  0.0851  0.0204
 0.0632 -0.0575  0.0453  0.0238 -0.0064 -0.0601 -0.0476  0.0806  0.0420  0.0266
 0.0052 -0.0164 -0.0635 -0.0539  0.0821  0.1009 -0.0769  0.0338  0.0957 -0.0701

Columns 11 to 20
-0.0914 -0.0503 -0.0693  0.0426 -0.0894 -0.0579  0.0119 -0.0371  0.0583  0.0191
-0.0773  0.0520  0.0168  0.0909  0.0388 -0.0274  0.0331 -0.0864 -0.0022  0.0821
 0.0881 -0.0114  0.0331 -0.0715  0.0881  0.0579  0.0256 -0.0839  0.0989 -0.0617
-0.0951 -0.0682  0.0505  0.0007  0.0726  0.0052 -0.0768  0.0355  0.0097  0.0956
-0.0562  0.0326  0.0932 -0.0450 -0.0828 -0.0175 -0.0111  0.0574  0.0858 -0.0222
-0.0058  0.0964  0.1004 -0.0697 -0.0226  0.0225 -0.0247 -0.0133  0.0422  0.0115

Columns 21 to 30
-0.0489  0.0222  0.0449  0.0612 -0.0560  0.0507  0.0905 -0.0813  0.0617  0.0625
-0.0541 -0.0807 -0.0815  0.0057  0.0034 -0.0012  0.0048 -0.0142 -0.0925  0.0787
 0.0023 -0.0303 -0.0956 -0.0497 -0.0059  0.0956 -0.0590 -0.0959  0.0659 -0.0753
 0.0883  0.0330  0.0215 -0.0282 -0.0823  0.0325 -0.0673  0.0580  0.0148 -0.0393
 0.0581  0.0047 -0.0106  0.0345  0.0796 -0.0092 -0.0852 -0.0456 -0.0377  0.0255
 0.0665  0.0225 -0.0959 -0.0750 -0.0164 -0.0188 -0.0391 -0.0703  0.0458 -0.0663

Columns 31 to 40
 0.0119 -0.0047 -0.0677 -0.0171  0.0991 -0.0983  0.0832 -0.0369  0.0178  0.0243
 0.0239 -0.0628  0.0162 -0.0874  0.0631  0.0059  0.0195  0.0054  0.0486  0.0141
 0.0106 -0.0581  0.0209  0.0881 -0.0275 -0.0740 -0.0220  0.0352  0.0535  0.0570
-0.1014 -0.0213  0.0894 -0.0029 -0.0223  0.0638  0.0849 -0.0639  0.0310 -0.0441
-0.0369  0.0575  0.0252 -0.0937  0.0310  0.0990 -0.0158  0.0170  0.0564  0.0096
-0.0033  0.0024 -0.0099 -0.0864  0.0737 -0.0698  0.0371  0.0017  0.0236  0.0418

Columns 41 to 50
-0.0526  0.0050 -0.0875 -0.0510  0.0065 -0.0947  0.0612 -0.0597  0.0636  0.0956
 0.0845 -0.0002  0.0246  0.0198  0.0137  0.0355 -0.0679  0.0659 -0.0936 -0.0391
 0.0740  0.0570  0.0908  0.0888  0.0536  0.0815 -0.0642  0.0887 -0.0713 -0.0055
-0.0900  0.0467  0.0169 -0.0007 -0.0228  0.0636  0.0324 -0.0357 -0.0366 -0.0165
-0.0402 -0.0357  0.0240  0.0542 -0.0047  0.0581 -0.0821  0.0217 -0.0059 -0.1004
-0.0638  0.0762 -0.0602  0.0095 -0.0064 -0.0560 -0.0981  0.0508  0.0792  0.0085

Columns 51 to 60
-0.0975  0.0971  0.0796  0.0149  0.0662 -0.0873 -0.0052  0.0541 -0.0257  0.0728
 0.0308  0.0444 -0.0112  0.0758 -0.0000 -0.0130 -0.0187  0.0969  0.0652 -0.0073
-0.0733 -0.0688  0.0695  0.0057  0.0472  0.0323 -0.0734  0.0784 -0.0211  0.0554
-0.0074 -0.0339  0.0330 -0.0659 -0.0962 -0.0920 -0.0354  0.0596  0.0436  0.0475
 0.0193 -0.0171  0.0318  0.0801 -0.0110  0.0170  0.0527  0.0584  0.0406  0.0628
 0.0234 -0.0734 -0.0465 -0.0163 -0.0547  0.0940 -0.0277  0.0632 -0.0415  0.0314

Columns 61 to 70
-0.0416  0.0498 -0.0381 -0.0934 -0.0604  0.0736 -0.0125  0.0552  0.0814 -0.0671
-0.0754  0.0070  0.0731  0.0544  0.0323 -0.0962 -0.0926  0.0369  0.0086  0.0417
-0.0965  0.0853 -0.0459  0.0906 -0.0377 -0.0217 -0.0959 -0.0510  0.0553 -0.0536
 0.0522 -0.0531  0.0003  0.0974 -0.0565 -0.0367 -0.0801  0.0630 -0.0938 -0.0330
 0.0859 -0.0141 -0.0676 -0.0435  0.0869  0.0719 -0.0273 -0.0436 -0.0326 -0.0959
-0.0122  0.0178 -0.0823  0.0010  0.0484  0.0866  0.0940  0.0147 -0.0183 -0.0260

Columns 71 to 80
-0.0358 -0.0326 -0.0725 -0.0352  0.0990 -0.0208  0.0475 -0.0495 -0.0206  0.0794
-0.0164 -0.0360 -0.0118 -0.0948 -0.0706  0.0988 -0.0546  0.0477  0.0473  0.0424
-0.0255  0.0286  0.0641  0.0070  0.0251 -0.0696 -0.0142  0.0208 -0.0720 -0.0931
 0.0522 -0.0172 -0.0391 -0.0731 -0.0051  0.0948 -0.0301 -0.0951  0.0980 -0.0053
-0.0254  0.0369 -0.0719 -0.0310  0.0490  0.0095  0.0506  0.0406  0.0779  0.0296
-0.0817 -0.0319  0.0183  0.0923  0.0206 -0.0866 -0.0745 -0.0644 -0.0788  0.0718

Columns 81 to 90
-0.0197 -0.0990  0.0808  0.0118  0.0577 -0.0945  0.0354 -0.0129 -0.0785  0.0083
-0.0903  0.0799 -0.0981  0.0641  0.0962 -0.0435 -0.0063  0.0508 -0.0439 -0.0145
-0.0100 -0.0817 -0.0274  0.0622  0.0206  0.0708  0.0725  0.0490  0.0626  0.0224
-0.0518  0.0733 -0.0562  0.0546 -0.0222 -0.0176  0.0084 -0.0559 -0.0884  0.0314
-0.0148  0.0869 -0.0563  0.0880  0.0353  0.0959  0.0541  0.0166 -0.0875 -0.0695
 0.0838 -0.0937  0.0843  0.0141  0.0282  0.0584 -0.0465  0.0279 -0.0629 -0.0268

Columns 91 to 100
 0.0127  0.0992  0.0660  0.0254 -0.0269 -0.0215  0.0555 -0.0034  0.0149  0.0476
-0.0759  0.0578 -0.0249 -0.0963 -0.0501  0.0422 -0.0700  0.0819  0.0295 -0.0684
-0.0491 -0.0512  0.0343 -0.0607 -0.0539 -0.0473 -0.0134 -0.0725  0.0198 -0.0419
 0.0883  0.0539 -0.0617  0.0651 -0.0288 -0.0801 -0.0511  0.0565  0.0859  0.0312
-0.0228  0.0269 -0.0941  0.0360  0.0832 -0.0546  0.0228 -0.0121  0.0019 -0.0728
-0.0394  0.0336 -0.0035  0.0858  0.0886 -0.0642  0.0669  0.0397  0.0176  0.0210
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:08[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:08[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:08[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 31 [batchSize = 1]
 
2016-10-13 16:47:08[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.56865755717ms 
2016-10-13 16:47:08[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:08[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0548  0.0295  0.0814 -0.0741  0.0172  0.0443 -0.0808  0.0765 -0.0747  0.0714
 0.0748  0.0325 -0.0912 -0.0734  0.0385  0.0183 -0.0358 -0.0663 -0.0898 -0.0796
-0.0728 -0.0809  0.0697 -0.0016  0.0695 -0.0921  0.0309 -0.0532  0.0501  0.0377
-0.0262 -0.0233 -0.0258  0.0126  0.0793  0.0150 -0.0119  0.0573  0.0851  0.0204
 0.0632 -0.0575  0.0453  0.0239 -0.0064 -0.0601 -0.0476  0.0806  0.0420  0.0266
 0.0052 -0.0164 -0.0635 -0.0538  0.0821  0.1009 -0.0769  0.0338  0.0957 -0.0701

Columns 11 to 20
-0.0914 -0.0503 -0.0693  0.0426 -0.0894 -0.0579  0.0119 -0.0371  0.0583  0.0191
-0.0773  0.0520  0.0168  0.0909  0.0388 -0.0274  0.0331 -0.0864 -0.0022  0.0821
 0.0881 -0.0114  0.0331 -0.0715  0.0881  0.0580  0.0256 -0.0839  0.0989 -0.0617
-0.0951 -0.0682  0.0505  0.0007  0.0726  0.0052 -0.0768  0.0355  0.0097  0.0956
-0.0562  0.0326  0.0932 -0.0450 -0.0828 -0.0175 -0.0111  0.0574  0.0858 -0.0222
-0.0058  0.0964  0.1004 -0.0698 -0.0226  0.0225 -0.0248 -0.0133  0.0422  0.0115

Columns 21 to 30
-0.0489  0.0222  0.0449  0.0612 -0.0560  0.0507  0.0905 -0.0813  0.0617  0.0625
-0.0541 -0.0807 -0.0815  0.0057  0.0034 -0.0012  0.0048 -0.0141 -0.0925  0.0787
 0.0023 -0.0303 -0.0956 -0.0497 -0.0059  0.0956 -0.0590 -0.0959  0.0659 -0.0753
 0.0883  0.0330  0.0215 -0.0282 -0.0823  0.0325 -0.0673  0.0580  0.0148 -0.0393
 0.0581  0.0047 -0.0106  0.0346  0.0796 -0.0092 -0.0852 -0.0456 -0.0377  0.0255
 0.0665  0.0225 -0.0959 -0.0750 -0.0165 -0.0189 -0.0391 -0.0702  0.0458 -0.0663

Columns 31 to 40
 0.0119 -0.0047 -0.0676 -0.0171  0.0991 -0.0983  0.0832 -0.0369  0.0178  0.0243
 0.0239 -0.0628  0.0162 -0.0874  0.0631  0.0059  0.0195  0.0054  0.0486  0.0141
 0.0105 -0.0581  0.0209  0.0881 -0.0275 -0.0740 -0.0220  0.0353  0.0535  0.0570
-0.1014 -0.0213  0.0894 -0.0029 -0.0223  0.0638  0.0849 -0.0639  0.0310 -0.0441
-0.0370  0.0575  0.0252 -0.0938  0.0310  0.0990 -0.0158  0.0170  0.0564  0.0096
-0.0033  0.0024 -0.0099 -0.0864  0.0737 -0.0698  0.0371  0.0017  0.0235  0.0418

Columns 41 to 50
-0.0526  0.0050 -0.0875 -0.0510  0.0065 -0.0947  0.0612 -0.0597  0.0636  0.0956
 0.0845 -0.0002  0.0246  0.0198  0.0137  0.0355 -0.0679  0.0659 -0.0936 -0.0392
 0.0740  0.0570  0.0908  0.0887  0.0536  0.0815 -0.0642  0.0887 -0.0713 -0.0056
-0.0900  0.0467  0.0169 -0.0007 -0.0228  0.0636  0.0324 -0.0357 -0.0366 -0.0165
-0.0401 -0.0357  0.0241  0.0542 -0.0047  0.0581 -0.0822  0.0217 -0.0060 -0.1005
-0.0638  0.0762 -0.0602  0.0095 -0.0064 -0.0560 -0.0981  0.0508  0.0792  0.0085

Columns 51 to 60
-0.0975  0.0971  0.0796  0.0149  0.0662 -0.0873 -0.0052  0.0541 -0.0257  0.0728
 0.0308  0.0444 -0.0111  0.0758 -0.0000 -0.0130 -0.0187  0.0969  0.0653 -0.0073
-0.0732 -0.0688  0.0695  0.0057  0.0472  0.0323 -0.0734  0.0784 -0.0211  0.0554
-0.0074 -0.0339  0.0330 -0.0659 -0.0962 -0.0920 -0.0354  0.0597  0.0436  0.0475
 0.0193 -0.0171  0.0318  0.0801 -0.0110  0.0170  0.0527  0.0584  0.0406  0.0628
 0.0234 -0.0734 -0.0465 -0.0163 -0.0547  0.0940 -0.0277  0.0632 -0.0415  0.0314

Columns 61 to 70
-0.0416  0.0497 -0.0381 -0.0934 -0.0604  0.0736 -0.0125  0.0552  0.0814 -0.0671
-0.0753  0.0070  0.0731  0.0543  0.0322 -0.0962 -0.0926  0.0369  0.0086  0.0417
-0.0965  0.0853 -0.0459  0.0905 -0.0378 -0.0217 -0.0959 -0.0510  0.0554 -0.0536
 0.0522 -0.0531  0.0003  0.0974 -0.0565 -0.0367 -0.0801  0.0630 -0.0938 -0.0330
 0.0859 -0.0141 -0.0676 -0.0435  0.0869  0.0719 -0.0273 -0.0436 -0.0326 -0.0960
-0.0122  0.0178 -0.0823  0.0009  0.0484  0.0866  0.0940  0.0147 -0.0183 -0.0261

Columns 71 to 80
-0.0358 -0.0326 -0.0725 -0.0352  0.0990 -0.0208  0.0475 -0.0495 -0.0206  0.0794
-0.0164 -0.0359 -0.0117 -0.0948 -0.0706  0.0989 -0.0546  0.0477  0.0473  0.0424
-0.0256  0.0287  0.0641  0.0070  0.0251 -0.0695 -0.0142  0.0208 -0.0720 -0.0931
 0.0522 -0.0173 -0.0392 -0.0731 -0.0051  0.0948 -0.0301 -0.0951  0.0980 -0.0053
-0.0254  0.0369 -0.0718 -0.0310  0.0490  0.0095  0.0505  0.0406  0.0779  0.0296
-0.0817 -0.0319  0.0183  0.0923  0.0206 -0.0865 -0.0745 -0.0644 -0.0787  0.0718

Columns 81 to 90
-0.0197 -0.0990  0.0808  0.0118  0.0577 -0.0945  0.0354 -0.0129 -0.0785  0.0083
-0.0904  0.0799 -0.0981  0.0641  0.0962 -0.0435 -0.0063  0.0508 -0.0439 -0.0145
-0.0100 -0.0817 -0.0274  0.0622  0.0206  0.0708  0.0725  0.0490  0.0626  0.0224
-0.0518  0.0733 -0.0563  0.0546 -0.0222 -0.0176  0.0084 -0.0559 -0.0884  0.0314
-0.0149  0.0869 -0.0563  0.0880  0.0353  0.0959  0.0541  0.0166 -0.0875 -0.0695
 0.0838 -0.0937  0.0843  0.0141  0.0282  0.0584 -0.0465  0.0279 -0.0629 -0.0268

Columns 91 to 100
 0.0127  0.0992  0.0660  0.0255 -0.0269 -0.0215  0.0555 -0.0034  0.0149  0.0476
-0.0759  0.0578 -0.0249 -0.0963 -0.0501  0.0422 -0.0700  0.0819  0.0295 -0.0683
-0.0491 -0.0511  0.0343 -0.0607 -0.0538 -0.0473 -0.0134 -0.0725  0.0197 -0.0419
 0.0884  0.0539 -0.0617  0.0651 -0.0288 -0.0801 -0.0511  0.0565  0.0859  0.0312
-0.0229  0.0269 -0.0941  0.0360  0.0832 -0.0546  0.0228 -0.0122  0.0019 -0.0728
-0.0394  0.0337 -0.0035  0.0858  0.0886 -0.0642  0.0669  0.0397  0.0175  0.0210
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:08[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:08[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:08[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 32 [batchSize = 1]
 
2016-10-13 16:47:08[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.599016189575ms 
2016-10-13 16:47:08[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:08[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 6
-5.4750e-02  2.9443e-02  8.1371e-02 -7.3996e-02  1.7219e-02  4.4373e-02
 7.4802e-02  3.2477e-02 -9.1214e-02 -7.3322e-02  3.8521e-02  1.8364e-02
-7.2804e-02 -8.0934e-02  6.9658e-02 -1.5252e-03  6.9462e-02 -9.2072e-02
-2.6091e-02 -2.3418e-02 -2.5776e-02  1.2774e-02  7.9334e-02  1.5197e-02
 6.3199e-02 -5.7539e-02  4.5265e-02  2.3905e-02 -6.3705e-03 -6.0025e-02
 5.2534e-03 -1.6413e-02 -6.3521e-02 -5.3824e-02  8.2058e-02  1.0090e-01

Columns 7 to 12
-8.0779e-02  7.6561e-02 -7.4726e-02  7.1398e-02 -9.1387e-02 -5.0271e-02
-3.5823e-02 -6.6265e-02 -8.9827e-02 -7.9609e-02 -7.7287e-02  5.2023e-02
 3.0917e-02 -5.3187e-02  5.0043e-02  3.7670e-02  8.8130e-02 -1.1428e-02
-1.1982e-02  5.7368e-02  8.4941e-02  2.0420e-02 -9.5040e-02 -6.8181e-02
-4.7584e-02  8.0596e-02  4.1937e-02  2.6579e-02 -5.6176e-02  3.2631e-02
-7.6917e-02  3.3817e-02  9.5694e-02 -7.0113e-02 -5.8081e-03  9.6399e-02

Columns 13 to 18
-6.9280e-02  4.2585e-02 -8.9449e-02 -5.7882e-02  1.1883e-02 -3.7100e-02
 1.6827e-02  9.0895e-02  3.8789e-02 -2.7365e-02  3.3136e-02 -8.6399e-02
 3.3171e-02 -7.1547e-02  8.8056e-02  5.7970e-02  2.5552e-02 -8.3953e-02
 5.0643e-02  6.5574e-04  7.2549e-02  5.2321e-03 -7.6828e-02  3.5465e-02
 9.3190e-02 -4.5004e-02 -8.2828e-02 -1.7523e-02 -1.1096e-02  5.7350e-02
 1.0044e-01 -6.9763e-02 -2.2643e-02  2.2474e-02 -2.4755e-02 -1.3346e-02

Columns 19 to 24
 5.8298e-02  1.9094e-02 -4.8922e-02  2.2220e-02  4.4903e-02  6.1258e-02
-2.2274e-03  8.2054e-02 -5.4119e-02 -8.0732e-02 -8.1493e-02  5.7247e-03
 9.8878e-02 -6.1712e-02  2.2785e-03 -3.0246e-02 -9.5583e-02 -4.9719e-02
 9.6660e-03  9.5499e-02  8.8236e-02  3.3026e-02  2.1557e-02 -2.8165e-02
 8.5804e-02 -2.2227e-02  5.8099e-02  4.7530e-03 -1.0591e-02  3.4563e-02
 4.2230e-02  1.1504e-02  6.6479e-02  2.2552e-02 -9.5894e-02 -7.5023e-02

Columns 25 to 30
-5.6018e-02  5.0698e-02  9.0450e-02 -8.1251e-02  6.1670e-02  6.2491e-02
 3.3509e-03 -1.1726e-03  4.7318e-03 -1.4112e-02 -9.2467e-02  7.8656e-02
-5.8971e-03  9.5553e-02 -5.9030e-02 -9.5898e-02  6.5869e-02 -7.5300e-02
-8.2281e-02  3.2465e-02 -6.7422e-02  5.8123e-02  1.4757e-02 -3.9406e-02
 7.9635e-02 -9.2503e-03 -8.5214e-02 -4.5558e-02 -3.7730e-02  2.5445e-02
-1.6457e-02 -1.8858e-02 -3.9093e-02 -7.0232e-02  4.5764e-02 -6.6354e-02

Columns 31 to 36
 1.1903e-02 -4.6623e-03 -6.7679e-02 -1.7141e-02  9.9041e-02 -9.8294e-02
 2.3850e-02 -6.2792e-02  1.6189e-02 -8.7430e-02  6.3066e-02  5.9432e-03
 1.0515e-02 -5.8070e-02  2.0887e-02  8.8087e-02 -2.7529e-02 -7.3976e-02
-1.0152e-01 -2.1226e-02  8.9305e-02 -2.9408e-03 -2.2389e-02  6.3878e-02
-3.6999e-02  5.7498e-02  2.5180e-02 -9.3778e-02  3.0958e-02  9.9041e-02
-3.3378e-03  2.4113e-03 -9.8831e-03 -8.6452e-02  7.3656e-02 -6.9786e-02

Columns 37 to 42
 8.3110e-02 -3.6861e-02  1.7775e-02  2.4265e-02 -5.2560e-02  4.9873e-03
 1.9420e-02  5.4266e-03  4.8550e-02  1.4096e-02  8.4531e-02 -1.8295e-04
-2.2046e-02  3.5281e-02  5.3528e-02  5.7023e-02  7.4055e-02  5.7008e-02
 8.4761e-02 -6.3808e-02  3.0980e-02 -4.4079e-02 -8.9954e-02  4.6740e-02
-1.5816e-02  1.7017e-02  5.6393e-02  9.6385e-03 -4.0129e-02 -3.5717e-02
 3.7074e-02  1.6913e-03  2.3541e-02  4.1777e-02 -6.3780e-02  7.6209e-02

Columns 43 to 48
-8.7460e-02 -5.1071e-02  6.5312e-03 -9.4708e-02  6.1205e-02 -5.9704e-02
 2.4645e-02  1.9772e-02  1.3729e-02  3.5507e-02 -6.7881e-02  6.5882e-02
 9.0791e-02  8.8711e-02  5.3581e-02  8.1550e-02 -6.4220e-02  8.8726e-02
 1.7012e-02 -8.8576e-04 -2.2823e-02  6.3589e-02  3.2358e-02 -3.5699e-02
 2.4080e-02  5.4170e-02 -4.6991e-03  5.8098e-02 -8.2170e-02  2.1685e-02
-6.0186e-02  9.4543e-03 -6.4148e-03 -5.5996e-02 -9.8092e-02  5.0792e-02

Columns 49 to 54
 6.3539e-02  9.5538e-02 -9.7457e-02  9.7122e-02  7.9669e-02  1.4932e-02
-9.3645e-02 -3.9188e-02  3.0792e-02  4.4382e-02 -1.1060e-02  7.5832e-02
-7.1323e-02 -5.5867e-03 -7.3243e-02 -6.8803e-02  6.9601e-02  5.7179e-03
-3.6763e-02 -1.6617e-02 -7.3783e-03 -3.3885e-02  3.3275e-02 -6.5783e-02
-6.0020e-03 -1.0051e-01  1.9333e-02 -1.7066e-02  3.1862e-02  8.0127e-02
 7.9186e-02  8.4338e-03  2.3365e-02 -7.3355e-02 -4.6471e-02 -1.6288e-02

Columns 55 to 60
 6.6225e-02 -8.7273e-02 -5.2377e-03  5.4138e-02 -2.5690e-02  7.2757e-02
-2.4308e-06 -1.3024e-02 -1.8752e-02  9.6935e-02  6.5287e-02 -7.2565e-03
 4.7186e-02  3.2324e-02 -7.3432e-02  7.8450e-02 -2.1041e-02  5.5362e-02
-9.6151e-02 -9.2039e-02 -3.5462e-02  5.9770e-02  4.3733e-02  4.7511e-02
-1.0941e-02  1.7007e-02  5.2714e-02  5.8471e-02  4.0667e-02  6.2810e-02
-5.4670e-02  9.3992e-02 -2.7684e-02  6.3245e-02 -4.1518e-02  3.1354e-02

Columns 61 to 66
-4.1616e-02  4.9797e-02 -3.8162e-02 -9.3406e-02 -6.0422e-02  7.3658e-02
-7.5311e-02  7.0319e-03  7.3075e-02  5.4318e-02  3.2241e-02 -9.6148e-02
-9.6474e-02  8.5366e-02 -4.5887e-02  9.0500e-02 -3.7758e-02 -2.1727e-02
 5.2303e-02 -5.2924e-02  2.4044e-04  9.7284e-02 -5.6484e-02 -3.6674e-02
 8.5962e-02 -1.4027e-02 -6.7644e-02 -4.3559e-02  8.6935e-02  7.1908e-02
-1.2198e-02  1.7790e-02 -8.2342e-02  9.2988e-04  4.8358e-02  8.6638e-02

Columns 67 to 72
-1.2528e-02  5.5256e-02  8.1448e-02 -6.7194e-02 -3.5860e-02 -3.2610e-02
-9.2636e-02  3.6933e-02  8.6129e-03  4.1616e-02 -1.6418e-02 -3.5915e-02
-9.5880e-02 -5.0951e-02  5.5383e-02 -5.3644e-02 -2.5582e-02  2.8680e-02
-8.0096e-02  6.3110e-02 -9.3664e-02 -3.3205e-02  5.2163e-02 -1.7202e-02
-2.7302e-02 -4.3598e-02 -3.2602e-02 -9.5995e-02 -2.5395e-02  3.6914e-02
 9.4003e-02  1.4723e-02 -1.8315e-02 -2.6083e-02 -8.1685e-02 -3.1868e-02

Columns 73 to 78
-7.2480e-02 -3.5124e-02  9.8985e-02 -2.0805e-02  4.7513e-02 -4.9482e-02
-1.1722e-02 -9.4738e-02 -7.0595e-02  9.8885e-02 -5.4677e-02  4.7670e-02
 6.4107e-02  7.0536e-03  2.5121e-02 -6.9491e-02 -1.4230e-02  2.0813e-02
-3.9087e-02 -7.3007e-02 -5.0951e-03  9.4924e-02 -3.0194e-02 -9.5041e-02
-7.1824e-02 -3.0940e-02  4.8956e-02  9.5558e-03  5.0524e-02  4.0588e-02
 1.8295e-02  9.2292e-02  2.0625e-02 -8.6508e-02 -7.4504e-02 -6.4385e-02

Columns 79 to 84
-2.0558e-02  7.9401e-02 -1.9754e-02 -9.8975e-02  8.0770e-02  1.1869e-02
 4.7274e-02  4.2369e-02 -9.0369e-02  7.9866e-02 -9.8062e-02  6.4136e-02
-7.1956e-02 -9.3096e-02 -1.0031e-02 -8.1671e-02 -2.7344e-02  6.2280e-02
 9.8035e-02 -5.2901e-03 -5.1897e-02  7.3271e-02 -5.6197e-02  5.4801e-02
 7.7943e-02  2.9609e-02 -1.4873e-02  8.6895e-02 -5.6289e-02  8.8085e-02
-7.8742e-02  7.1762e-02  8.3815e-02 -9.3664e-02  8.4289e-02  1.4110e-02

Columns 85 to 90
 5.7660e-02 -9.4457e-02  3.5354e-02 -1.2857e-02 -7.8463e-02  8.3138e-03
 9.6184e-02 -4.3524e-02 -6.2812e-03  5.0825e-02 -4.3895e-02 -1.4482e-02
 2.0611e-02  7.0842e-02  7.2532e-02  4.8956e-02  6.2608e-02  2.2363e-02
-2.2244e-02 -1.7574e-02  8.3885e-03 -5.5948e-02 -8.8429e-02  3.1381e-02
 3.5322e-02  9.5933e-02  5.4045e-02  1.6631e-02 -8.7466e-02 -6.9498e-02
 2.8185e-02  5.8412e-02 -4.6486e-02  2.7892e-02 -6.2866e-02 -2.6785e-02

Columns 91 to 96
 1.2748e-02  9.9215e-02  6.6001e-02  2.5418e-02 -2.6840e-02 -2.1478e-02
-7.5909e-02  5.7836e-02 -2.4929e-02 -9.6345e-02 -5.0066e-02  4.2152e-02
-4.9116e-02 -5.1131e-02  3.4283e-02 -6.0739e-02 -5.3826e-02 -4.7337e-02
 8.8342e-02  5.4003e-02 -6.1720e-02  6.4952e-02 -2.8752e-02 -8.0105e-02
-2.2858e-02  2.6940e-02 -9.4122e-02  3.5962e-02  8.3221e-02 -5.4621e-02
-3.9406e-02  3.3669e-02 -3.5045e-03  8.5731e-02  8.8595e-02 -6.4182e-02

Columns 97 to 100
 5.5473e-02 -3.4256e-03  1.4946e-02  4.7599e-02
-6.9963e-02  8.1825e-02  2.9495e-02 -6.8320e-02
-1.3384e-02 -7.2538e-02  1.9739e-02 -4.1900e-02
-5.1004e-02  5.6381e-02  8.5904e-02  3.1288e-02
 2.2793e-02 -1.2193e-02  1.8811e-03 -7.2775e-02
 6.6908e-02  3.9667e-02  1.7545e-02  2.1042e-02
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:08[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:08[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:08[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 33 [batchSize = 1]
 
2016-10-13 16:47:09[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.429977416992ms 
2016-10-13 16:47:09[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:09[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0547  0.0295  0.0814 -0.0740  0.0172  0.0444 -0.0808  0.0766 -0.0747  0.0714
 0.0748  0.0324 -0.0912 -0.0733  0.0385  0.0184 -0.0358 -0.0662 -0.0898 -0.0796
-0.0728 -0.0810  0.0697 -0.0015  0.0695 -0.0920  0.0309 -0.0532  0.0500  0.0377
-0.0261 -0.0234 -0.0258  0.0128  0.0793  0.0152 -0.0120  0.0574  0.0849  0.0204
 0.0632 -0.0576  0.0453  0.0239 -0.0064 -0.0600 -0.0476  0.0806  0.0419  0.0266
 0.0053 -0.0164 -0.0635 -0.0538  0.0821  0.1009 -0.0769  0.0338  0.0957 -0.0701

Columns 11 to 20
-0.0914 -0.0503 -0.0693  0.0426 -0.0894 -0.0579  0.0119 -0.0371  0.0583  0.0191
-0.0773  0.0520  0.0169  0.0909  0.0388 -0.0274  0.0331 -0.0864 -0.0022  0.0820
 0.0881 -0.0114  0.0332 -0.0716  0.0880  0.0580  0.0255 -0.0840  0.0989 -0.0617
-0.0951 -0.0682  0.0507  0.0006  0.0726  0.0052 -0.0768  0.0355  0.0097  0.0955
-0.0562  0.0326  0.0932 -0.0450 -0.0828 -0.0175 -0.0111  0.0573  0.0858 -0.0222
-0.0058  0.0964  0.1005 -0.0698 -0.0227  0.0225 -0.0248 -0.0134  0.0422  0.0115

Columns 21 to 30
-0.0489  0.0222  0.0449  0.0613 -0.0560  0.0507  0.0905 -0.0812  0.0617  0.0625
-0.0541 -0.0807 -0.0815  0.0057  0.0033 -0.0012  0.0047 -0.0141 -0.0925  0.0786
 0.0023 -0.0302 -0.0956 -0.0497 -0.0059  0.0955 -0.0591 -0.0959  0.0659 -0.0753
 0.0882  0.0330  0.0216 -0.0282 -0.0823  0.0325 -0.0674  0.0581  0.0148 -0.0394
 0.0581  0.0048 -0.0106  0.0346  0.0796 -0.0093 -0.0852 -0.0455 -0.0377  0.0254
 0.0665  0.0226 -0.0959 -0.0750 -0.0165 -0.0189 -0.0391 -0.0702  0.0458 -0.0664

Columns 31 to 40
 0.0119 -0.0047 -0.0677 -0.0171  0.0990 -0.0983  0.0831 -0.0369  0.0178  0.0243
 0.0238 -0.0628  0.0162 -0.0875  0.0631  0.0060  0.0194  0.0055  0.0485  0.0141
 0.0105 -0.0580  0.0209  0.0881 -0.0275 -0.0740 -0.0221  0.0353  0.0535  0.0570
-0.1015 -0.0212  0.0893 -0.0029 -0.0224  0.0639  0.0848 -0.0638  0.0310 -0.0441
-0.0370  0.0575  0.0252 -0.0938  0.0309  0.0991 -0.0158  0.0170  0.0564  0.0096
-0.0034  0.0024 -0.0099 -0.0865  0.0736 -0.0698  0.0371  0.0017  0.0235  0.0418

Columns 41 to 50
-0.0526  0.0050 -0.0875 -0.0511  0.0065 -0.0947  0.0612 -0.0597  0.0635  0.0955
 0.0846 -0.0002  0.0247  0.0197  0.0137  0.0355 -0.0679  0.0659 -0.0937 -0.0392
 0.0741  0.0570  0.0908  0.0887  0.0536  0.0816 -0.0642  0.0887 -0.0714 -0.0056
-0.0900  0.0467  0.0170 -0.0009 -0.0228  0.0636  0.0324 -0.0357 -0.0368 -0.0167
-0.0401 -0.0357  0.0241  0.0541 -0.0047  0.0581 -0.0822  0.0217 -0.0060 -0.1005
-0.0638  0.0762 -0.0602  0.0094 -0.0064 -0.0560 -0.0981  0.0508  0.0792  0.0084

Columns 51 to 60
-0.0975  0.0971  0.0797  0.0149  0.0662 -0.0873 -0.0052  0.0541 -0.0257  0.0728
 0.0308  0.0444 -0.0110  0.0759  0.0000 -0.0130 -0.0188  0.0970  0.0653 -0.0073
-0.0732 -0.0688  0.0696  0.0058  0.0472  0.0323 -0.0735  0.0785 -0.0210  0.0554
-0.0074 -0.0339  0.0333 -0.0658 -0.0961 -0.0920 -0.0355  0.0598  0.0438  0.0475
 0.0193 -0.0171  0.0319  0.0802 -0.0109  0.0170  0.0527  0.0585  0.0407  0.0628
 0.0234 -0.0733 -0.0465 -0.0163 -0.0547  0.0940 -0.0277  0.0633 -0.0415  0.0314

Columns 61 to 70
-0.0416  0.0498 -0.0382 -0.0934 -0.0604  0.0737 -0.0125  0.0553  0.0814 -0.0672
-0.0753  0.0071  0.0731  0.0543  0.0322 -0.0961 -0.0926  0.0369  0.0086  0.0416
-0.0965  0.0854 -0.0459  0.0905 -0.0378 -0.0217 -0.0959 -0.0509  0.0554 -0.0537
 0.0523 -0.0529  0.0002  0.0973 -0.0565 -0.0367 -0.0801  0.0631 -0.0937 -0.0333
 0.0860 -0.0140 -0.0677 -0.0436  0.0869  0.0719 -0.0273 -0.0436 -0.0326 -0.0960
-0.0122  0.0178 -0.0824  0.0009  0.0484  0.0866  0.0940  0.0147 -0.0183 -0.0261

Columns 71 to 80
-0.0359 -0.0326 -0.0725 -0.0351  0.0990 -0.0208  0.0475 -0.0495 -0.0206  0.0794
-0.0164 -0.0359 -0.0117 -0.0947 -0.0706  0.0989 -0.0547  0.0477  0.0473  0.0424
-0.0256  0.0287  0.0641  0.0071  0.0251 -0.0695 -0.0143  0.0208 -0.0719 -0.0931
 0.0522 -0.0172 -0.0391 -0.0730 -0.0051  0.0950 -0.0302 -0.0950  0.0980 -0.0053
-0.0254  0.0369 -0.0718 -0.0309  0.0489  0.0096  0.0505  0.0406  0.0780  0.0296
-0.0817 -0.0319  0.0183  0.0923  0.0206 -0.0865 -0.0745 -0.0644 -0.0787  0.0718

Columns 81 to 90
-0.0198 -0.0990  0.0808  0.0119  0.0577 -0.0945  0.0354 -0.0129 -0.0785  0.0083
-0.0904  0.0799 -0.0981  0.0642  0.0962 -0.0435 -0.0063  0.0508 -0.0439 -0.0145
-0.0100 -0.0817 -0.0273  0.0623  0.0206  0.0708  0.0725  0.0490  0.0626  0.0224
-0.0519  0.0733 -0.0562  0.0548 -0.0222 -0.0176  0.0084 -0.0559 -0.0884  0.0314
-0.0149  0.0869 -0.0563  0.0881  0.0353  0.0959  0.0540  0.0166 -0.0875 -0.0695
 0.0838 -0.0937  0.0843  0.0141  0.0282  0.0584 -0.0465  0.0279 -0.0629 -0.0268

Columns 91 to 100
 0.0127  0.0992  0.0660  0.0254 -0.0268 -0.0215  0.0555 -0.0034  0.0149  0.0476
-0.0759  0.0579 -0.0249 -0.0964 -0.0500  0.0422 -0.0699  0.0818  0.0295 -0.0683
-0.0491 -0.0511  0.0343 -0.0608 -0.0538 -0.0473 -0.0134 -0.0726  0.0197 -0.0419
 0.0883  0.0540 -0.0617  0.0650 -0.0288 -0.0801 -0.0510  0.0564  0.0859  0.0313
-0.0229  0.0270 -0.0941  0.0359  0.0832 -0.0546  0.0228 -0.0122  0.0019 -0.0728
-0.0394  0.0337 -0.0035  0.0857  0.0886 -0.0642  0.0669  0.0397  0.0175  0.0211
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:09[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:09[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:09[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 34 [batchSize = 1]
 
2016-10-13 16:47:09[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.403354008993ms 
2016-10-13 16:47:09[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:09[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0547  0.0294  0.0814 -0.0740  0.0172  0.0444 -0.0808  0.0766 -0.0747  0.0714
 0.0749  0.0324 -0.0912 -0.0733  0.0385  0.0185 -0.0358 -0.0662 -0.0899 -0.0796
-0.0728 -0.0810  0.0697 -0.0015  0.0695 -0.0920  0.0309 -0.0532  0.0500  0.0377
-0.0260 -0.0235 -0.0258  0.0128  0.0793  0.0153 -0.0120  0.0574  0.0849  0.0204
 0.0632 -0.0576  0.0453  0.0240 -0.0064 -0.0599 -0.0476  0.0806  0.0419  0.0266
 0.0053 -0.0164 -0.0635 -0.0538  0.0821  0.1010 -0.0769  0.0338  0.0957 -0.0701

Columns 11 to 20
-0.0914 -0.0503 -0.0693  0.0426 -0.0894 -0.0579  0.0119 -0.0371  0.0583  0.0191
-0.0773  0.0520  0.0169  0.0909  0.0388 -0.0273  0.0331 -0.0864 -0.0022  0.0820
 0.0881 -0.0114  0.0332 -0.0716  0.0880  0.0580  0.0255 -0.0840  0.0989 -0.0618
-0.0950 -0.0682  0.0507  0.0006  0.0725  0.0053 -0.0769  0.0354  0.0097  0.0955
-0.0562  0.0327  0.0932 -0.0450 -0.0828 -0.0175 -0.0111  0.0573  0.0858 -0.0223
-0.0058  0.0964  0.1005 -0.0698 -0.0227  0.0225 -0.0248 -0.0134  0.0422  0.0115

Columns 21 to 30
-0.0489  0.0222  0.0449  0.0612 -0.0560  0.0507  0.0904 -0.0812  0.0617  0.0625
-0.0541 -0.0807 -0.0815  0.0057  0.0033 -0.0012  0.0047 -0.0141 -0.0925  0.0786
 0.0023 -0.0302 -0.0956 -0.0497 -0.0059  0.0955 -0.0591 -0.0959  0.0659 -0.0754
 0.0882  0.0330  0.0216 -0.0282 -0.0823  0.0325 -0.0675  0.0582  0.0147 -0.0394
 0.0581  0.0048 -0.0106  0.0346  0.0796 -0.0093 -0.0853 -0.0455 -0.0377  0.0254
 0.0665  0.0226 -0.0959 -0.0750 -0.0165 -0.0189 -0.0391 -0.0702  0.0458 -0.0664

Columns 31 to 40
 0.0119 -0.0047 -0.0677 -0.0172  0.0990 -0.0983  0.0831 -0.0369  0.0178  0.0243
 0.0238 -0.0627  0.0162 -0.0875  0.0630  0.0060  0.0194  0.0055  0.0485  0.0141
 0.0105 -0.0580  0.0209  0.0880 -0.0276 -0.0740 -0.0221  0.0353  0.0535  0.0570
-0.1016 -0.0212  0.0892 -0.0030 -0.0224  0.0639  0.0848 -0.0637  0.0309 -0.0441
-0.0370  0.0575  0.0252 -0.0938  0.0309  0.0991 -0.0159  0.0171  0.0564  0.0097
-0.0034  0.0024 -0.0099 -0.0865  0.0736 -0.0698  0.0371  0.0017  0.0235  0.0418

Columns 41 to 50
-0.0525  0.0050 -0.0875 -0.0511  0.0065 -0.0947  0.0612 -0.0597  0.0635  0.0955
 0.0846 -0.0002  0.0247  0.0197  0.0137  0.0355 -0.0679  0.0659 -0.0937 -0.0393
 0.0741  0.0570  0.0908  0.0887  0.0536  0.0816 -0.0642  0.0887 -0.0714 -0.0056
-0.0899  0.0468  0.0171 -0.0010 -0.0228  0.0636  0.0323 -0.0357 -0.0368 -0.0167
-0.0401 -0.0357  0.0241  0.0541 -0.0047  0.0581 -0.0822  0.0217 -0.0061 -0.1006
-0.0637  0.0762 -0.0602  0.0094 -0.0064 -0.0560 -0.0981  0.0508  0.0791  0.0084

Columns 51 to 60
-0.0975  0.0971  0.0797  0.0150  0.0662 -0.0873 -0.0052  0.0541 -0.0257  0.0728
 0.0308  0.0444 -0.0110  0.0759  0.0000 -0.0130 -0.0188  0.0970  0.0653 -0.0073
-0.0732 -0.0688  0.0696  0.0058  0.0472  0.0323 -0.0735  0.0785 -0.0210  0.0554
-0.0074 -0.0339  0.0334 -0.0657 -0.0961 -0.0921 -0.0355  0.0598  0.0438  0.0475
 0.0193 -0.0170  0.0319  0.0802 -0.0109  0.0170  0.0527  0.0585  0.0407  0.0628
 0.0234 -0.0733 -0.0464 -0.0162 -0.0546  0.0940 -0.0277  0.0633 -0.0415  0.0314

Columns 61 to 70
-0.0416  0.0498 -0.0382 -0.0934 -0.0604  0.0737 -0.0125  0.0553  0.0814 -0.0672
-0.0753  0.0071  0.0730  0.0543  0.0322 -0.0961 -0.0926  0.0370  0.0087  0.0415
-0.0964  0.0854 -0.0459  0.0904 -0.0378 -0.0217 -0.0959 -0.0509  0.0554 -0.0537
 0.0523 -0.0529  0.0002  0.0972 -0.0565 -0.0366 -0.0801  0.0631 -0.0937 -0.0333
 0.0860 -0.0140 -0.0677 -0.0436  0.0869  0.0719 -0.0273 -0.0436 -0.0326 -0.0961
-0.0122  0.0178 -0.0824  0.0009  0.0484  0.0867  0.0940  0.0147 -0.0183 -0.0261

Columns 71 to 80
-0.0359 -0.0326 -0.0725 -0.0351  0.0990 -0.0208  0.0475 -0.0495 -0.0206  0.0794
-0.0164 -0.0359 -0.0117 -0.0947 -0.0706  0.0990 -0.0547  0.0477  0.0473  0.0424
-0.0256  0.0287  0.0641  0.0071  0.0251 -0.0694 -0.0143  0.0208 -0.0719 -0.0931
 0.0521 -0.0172 -0.0391 -0.0729 -0.0051  0.0950 -0.0302 -0.0950  0.0981 -0.0053
-0.0254  0.0369 -0.0718 -0.0309  0.0489  0.0096  0.0505  0.0406  0.0780  0.0296
-0.0817 -0.0318  0.0183  0.0923  0.0206 -0.0865 -0.0745 -0.0644 -0.0787  0.0718

Columns 81 to 90
-0.0198 -0.0990  0.0808  0.0119  0.0577 -0.0945  0.0354 -0.0129 -0.0785  0.0083
-0.0904  0.0799 -0.0981  0.0642  0.0962 -0.0435 -0.0063  0.0508 -0.0439 -0.0145
-0.0100 -0.0817 -0.0273  0.0624  0.0206  0.0709  0.0725  0.0489  0.0626  0.0224
-0.0519  0.0733 -0.0562  0.0549 -0.0223 -0.0176  0.0084 -0.0560 -0.0884  0.0314
-0.0149  0.0869 -0.0563  0.0882  0.0353  0.0959  0.0540  0.0166 -0.0875 -0.0695
 0.0838 -0.0937  0.0843  0.0142  0.0282  0.0584 -0.0465  0.0279 -0.0629 -0.0268

Columns 91 to 100
 0.0127  0.0992  0.0660  0.0254 -0.0268 -0.0215  0.0555 -0.0034  0.0149  0.0476
-0.0759  0.0579 -0.0249 -0.0964 -0.0500  0.0422 -0.0699  0.0818  0.0294 -0.0683
-0.0491 -0.0511  0.0343 -0.0608 -0.0538 -0.0473 -0.0134 -0.0726  0.0197 -0.0419
 0.0883  0.0540 -0.0617  0.0649 -0.0288 -0.0801 -0.0510  0.0563  0.0858  0.0314
-0.0229  0.0270 -0.0941  0.0359  0.0832 -0.0546  0.0228 -0.0122  0.0018 -0.0727
-0.0394  0.0337 -0.0035  0.0857  0.0886 -0.0642  0.0669  0.0397  0.0175  0.0211
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:09[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:09[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:09[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 35 [batchSize = 1]
 
2016-10-13 16:47:09[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.402638753255ms 
2016-10-13 16:47:09[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:09[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0547  0.0294  0.0814 -0.0740  0.0172  0.0444 -0.0808  0.0766 -0.0747  0.0714
 0.0749  0.0324 -0.0912 -0.0732  0.0385  0.0185 -0.0358 -0.0662 -0.0899 -0.0796
-0.0727 -0.0810  0.0697 -0.0014  0.0695 -0.0919  0.0309 -0.0531  0.0500  0.0377
-0.0260 -0.0236 -0.0258  0.0129  0.0793  0.0153 -0.0120  0.0574  0.0849  0.0204
 0.0633 -0.0576  0.0453  0.0240 -0.0064 -0.0599 -0.0476  0.0806  0.0419  0.0266
 0.0053 -0.0165 -0.0635 -0.0538  0.0821  0.1010 -0.0769  0.0339  0.0957 -0.0701

Columns 11 to 20
-0.0914 -0.0503 -0.0692  0.0426 -0.0895 -0.0579  0.0119 -0.0371  0.0583  0.0191
-0.0773  0.0521  0.0169  0.0909  0.0388 -0.0273  0.0331 -0.0865 -0.0022  0.0820
 0.0882 -0.0114  0.0333 -0.0716  0.0880  0.0580  0.0255 -0.0840  0.0989 -0.0618
-0.0950 -0.0681  0.0508  0.0006  0.0725  0.0053 -0.0769  0.0354  0.0097  0.0954
-0.0561  0.0327  0.0933 -0.0450 -0.0829 -0.0175 -0.0111  0.0573  0.0858 -0.0223
-0.0058  0.0964  0.1005 -0.0698 -0.0227  0.0225 -0.0248 -0.0134  0.0422  0.0114

Columns 21 to 30
-0.0489  0.0222  0.0449  0.0612 -0.0560  0.0507  0.0904 -0.0812  0.0617  0.0625
-0.0542 -0.0807 -0.0815  0.0058  0.0033 -0.0012  0.0047 -0.0141 -0.0925  0.0786
 0.0022 -0.0302 -0.0956 -0.0497 -0.0059  0.0955 -0.0591 -0.0958  0.0659 -0.0754
 0.0882  0.0331  0.0216 -0.0282 -0.0823  0.0325 -0.0675  0.0582  0.0147 -0.0395
 0.0581  0.0048 -0.0106  0.0346  0.0796 -0.0093 -0.0853 -0.0455 -0.0377  0.0254
 0.0665  0.0226 -0.0959 -0.0750 -0.0165 -0.0189 -0.0391 -0.0702  0.0458 -0.0664

Columns 31 to 40
 0.0119 -0.0046 -0.0677 -0.0172  0.0990 -0.0983  0.0831 -0.0368  0.0178  0.0243
 0.0238 -0.0627  0.0161 -0.0875  0.0630  0.0060  0.0194  0.0055  0.0485  0.0141
 0.0104 -0.0580  0.0208  0.0880 -0.0276 -0.0739 -0.0221  0.0354  0.0535  0.0571
-0.1016 -0.0211  0.0892 -0.0030 -0.0224  0.0639  0.0847 -0.0637  0.0309 -0.0440
-0.0371  0.0576  0.0251 -0.0938  0.0309  0.0991 -0.0159  0.0171  0.0564  0.0097
-0.0034  0.0025 -0.0099 -0.0865  0.0736 -0.0698  0.0370  0.0017  0.0235  0.0418

Columns 41 to 50
-0.0525  0.0050 -0.0874 -0.0511  0.0065 -0.0947  0.0612 -0.0597  0.0635  0.0955
 0.0846 -0.0002  0.0247  0.0197  0.0137  0.0355 -0.0679  0.0659 -0.0937 -0.0393
 0.0741  0.0570  0.0908  0.0886  0.0536  0.0816 -0.0642  0.0887 -0.0714 -0.0057
-0.0899  0.0468  0.0171 -0.0010 -0.0228  0.0636  0.0323 -0.0357 -0.0369 -0.0169
-0.0401 -0.0357  0.0241  0.0541 -0.0047  0.0581 -0.0822  0.0217 -0.0061 -0.1006
-0.0637  0.0762 -0.0602  0.0094 -0.0064 -0.0560 -0.0981  0.0508  0.0791  0.0084

Columns 51 to 60
-0.0975  0.0971  0.0797  0.0150  0.0662 -0.0873 -0.0052  0.0542 -0.0257  0.0728
 0.0308  0.0444 -0.0110  0.0759  0.0001 -0.0130 -0.0188  0.0970  0.0653 -0.0073
-0.0732 -0.0688  0.0697  0.0058  0.0472  0.0323 -0.0735  0.0785 -0.0210  0.0554
-0.0074 -0.0338  0.0334 -0.0657 -0.0961 -0.0921 -0.0355  0.0599  0.0438  0.0475
 0.0193 -0.0170  0.0319  0.0802 -0.0109  0.0170  0.0527  0.0585  0.0407  0.0628
 0.0234 -0.0733 -0.0464 -0.0162 -0.0546  0.0940 -0.0277  0.0633 -0.0415  0.0314

Columns 61 to 70
-0.0416  0.0498 -0.0382 -0.0934 -0.0604  0.0737 -0.0125  0.0553  0.0815 -0.0673
-0.0752  0.0071  0.0730  0.0542  0.0322 -0.0961 -0.0926  0.0370  0.0087  0.0415
-0.0964  0.0854 -0.0459  0.0904 -0.0378 -0.0217 -0.0959 -0.0509  0.0555 -0.0537
 0.0524 -0.0528  0.0002  0.0972 -0.0565 -0.0366 -0.0801  0.0632 -0.0936 -0.0334
 0.0860 -0.0139 -0.0677 -0.0436  0.0869  0.0719 -0.0273 -0.0436 -0.0325 -0.0961
-0.0121  0.0178 -0.0824  0.0009  0.0483  0.0867  0.0940  0.0147 -0.0183 -0.0261

Columns 71 to 80
-0.0359 -0.0326 -0.0725 -0.0351  0.0989 -0.0207  0.0475 -0.0495 -0.0206  0.0794
-0.0165 -0.0359 -0.0117 -0.0947 -0.0706  0.0990 -0.0547  0.0477  0.0473  0.0424
-0.0256  0.0287  0.0642  0.0071  0.0251 -0.0694 -0.0143  0.0208 -0.0719 -0.0931
 0.0521 -0.0171 -0.0390 -0.0729 -0.0052  0.0951 -0.0303 -0.0950  0.0981 -0.0053
-0.0254  0.0370 -0.0718 -0.0309  0.0489  0.0096  0.0505  0.0406  0.0780  0.0296
-0.0817 -0.0318  0.0183  0.0923  0.0206 -0.0864 -0.0745 -0.0644 -0.0787  0.0718

Columns 81 to 90
-0.0198 -0.0990  0.0808  0.0119  0.0577 -0.0945  0.0353 -0.0129 -0.0785  0.0083
-0.0904  0.0799 -0.0980  0.0643  0.0961 -0.0435 -0.0063  0.0508 -0.0439 -0.0145
-0.0101 -0.0817 -0.0273  0.0624  0.0206  0.0709  0.0725  0.0489  0.0626  0.0224
-0.0519  0.0733 -0.0561  0.0550 -0.0223 -0.0176  0.0084 -0.0560 -0.0885  0.0314
-0.0149  0.0869 -0.0563  0.0882  0.0353  0.0959  0.0540  0.0166 -0.0875 -0.0695
 0.0838 -0.0937  0.0843  0.0142  0.0282  0.0584 -0.0465  0.0279 -0.0629 -0.0268

Columns 91 to 100
 0.0127  0.0992  0.0660  0.0254 -0.0268 -0.0215  0.0555 -0.0034  0.0149  0.0476
-0.0759  0.0579 -0.0249 -0.0964 -0.0500  0.0422 -0.0699  0.0818  0.0294 -0.0682
-0.0492 -0.0511  0.0343 -0.0608 -0.0538 -0.0473 -0.0133 -0.0726  0.0197 -0.0418
 0.0883  0.0541 -0.0617  0.0648 -0.0287 -0.0801 -0.0510  0.0563  0.0857  0.0314
-0.0229  0.0270 -0.0941  0.0359  0.0833 -0.0546  0.0228 -0.0122  0.0018 -0.0727
-0.0394  0.0337 -0.0035  0.0857  0.0886 -0.0642  0.0669  0.0396  0.0175  0.0211
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:09[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:09[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:09[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 36 [batchSize = 1]
 
2016-10-13 16:47:09[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.307668685913ms 
2016-10-13 16:47:09[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:09[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0547  0.0294  0.0814 -0.0740  0.0172  0.0445 -0.0808  0.0766 -0.0747  0.0714
 0.0749  0.0323 -0.0912 -0.0732  0.0385  0.0186 -0.0359 -0.0662 -0.0899 -0.0796
-0.0727 -0.0811  0.0697 -0.0014  0.0695 -0.0919  0.0309 -0.0531  0.0499  0.0377
-0.0259 -0.0237 -0.0258  0.0129  0.0794  0.0155 -0.0121  0.0574  0.0848  0.0205
 0.0633 -0.0577  0.0453  0.0240 -0.0064 -0.0599 -0.0476  0.0807  0.0418  0.0266
 0.0053 -0.0165 -0.0635 -0.0537  0.0821  0.1010 -0.0769  0.0339  0.0956 -0.0701

Columns 11 to 20
-0.0914 -0.0502 -0.0692  0.0426 -0.0895 -0.0579  0.0119 -0.0371  0.0583  0.0191
-0.0773  0.0521  0.0170  0.0908  0.0388 -0.0273  0.0331 -0.0865 -0.0022  0.0819
 0.0882 -0.0114  0.0333 -0.0716  0.0880  0.0580  0.0255 -0.0840  0.0989 -0.0618
-0.0950 -0.0681  0.0509  0.0006  0.0725  0.0053 -0.0769  0.0353  0.0097  0.0954
-0.0561  0.0327  0.0933 -0.0450 -0.0829 -0.0175 -0.0111  0.0573  0.0858 -0.0223
-0.0058  0.0964  0.1005 -0.0698 -0.0227  0.0225 -0.0248 -0.0134  0.0422  0.0114

Columns 21 to 30
-0.0489  0.0222  0.0449  0.0613 -0.0560  0.0507  0.0904 -0.0812  0.0617  0.0625
-0.0542 -0.0807 -0.0815  0.0058  0.0033 -0.0012  0.0046 -0.0140 -0.0925  0.0785
 0.0022 -0.0302 -0.0956 -0.0497 -0.0059  0.0955 -0.0591 -0.0958  0.0659 -0.0754
 0.0882  0.0331  0.0216 -0.0281 -0.0823  0.0324 -0.0675  0.0583  0.0147 -0.0395
 0.0581  0.0048 -0.0106  0.0346  0.0796 -0.0093 -0.0853 -0.0455 -0.0377  0.0253
 0.0664  0.0226 -0.0959 -0.0750 -0.0165 -0.0189 -0.0392 -0.0702  0.0458 -0.0664

Columns 31 to 40
 0.0119 -0.0046 -0.0677 -0.0172  0.0990 -0.0983  0.0831 -0.0368  0.0178  0.0243
 0.0237 -0.0627  0.0161 -0.0875  0.0630  0.0060  0.0193  0.0055  0.0485  0.0141
 0.0104 -0.0580  0.0208  0.0880 -0.0276 -0.0739 -0.0221  0.0354  0.0535  0.0571
-0.1017 -0.0210  0.0891 -0.0030 -0.0225  0.0639  0.0847 -0.0636  0.0309 -0.0440
-0.0371  0.0576  0.0251 -0.0939  0.0309  0.0991 -0.0159  0.0171  0.0563  0.0097
-0.0034  0.0025 -0.0099 -0.0865  0.0736 -0.0698  0.0370  0.0018  0.0235  0.0418

Columns 41 to 50
-0.0525  0.0050 -0.0874 -0.0511  0.0065 -0.0947  0.0612 -0.0597  0.0635  0.0954
 0.0846 -0.0002  0.0247  0.0197  0.0137  0.0355 -0.0679  0.0659 -0.0938 -0.0393
 0.0741  0.0570  0.0909  0.0886  0.0536  0.0816 -0.0643  0.0888 -0.0714 -0.0057
-0.0898  0.0468  0.0172 -0.0011 -0.0228  0.0636  0.0323 -0.0357 -0.0370 -0.0169
-0.0401 -0.0357  0.0241  0.0541 -0.0047  0.0581 -0.0822  0.0217 -0.0061 -0.1006
-0.0637  0.0762 -0.0601  0.0094 -0.0064 -0.0560 -0.0981  0.0508  0.0791  0.0083

Columns 51 to 60
-0.0975  0.0971  0.0797  0.0150  0.0662 -0.0873 -0.0052  0.0542 -0.0256  0.0728
 0.0308  0.0444 -0.0109  0.0759  0.0001 -0.0131 -0.0188  0.0970  0.0654 -0.0073
-0.0732 -0.0688  0.0697  0.0058  0.0473  0.0323 -0.0735  0.0785 -0.0210  0.0554
-0.0074 -0.0338  0.0335 -0.0657 -0.0960 -0.0921 -0.0355  0.0599  0.0439  0.0475
 0.0194 -0.0170  0.0320  0.0802 -0.0109  0.0170  0.0527  0.0586  0.0407  0.0628
 0.0234 -0.0733 -0.0464 -0.0162 -0.0546  0.0940 -0.0277  0.0633 -0.0415  0.0314

Columns 61 to 70
-0.0416  0.0499 -0.0382 -0.0935 -0.0604  0.0737 -0.0125  0.0553  0.0815 -0.0673
-0.0752  0.0072  0.0730  0.0542  0.0322 -0.0961 -0.0926  0.0370  0.0087  0.0415
-0.0964  0.0855 -0.0459  0.0904 -0.0378 -0.0217 -0.0959 -0.0509  0.0555 -0.0538
 0.0525 -0.0527  0.0001  0.0971 -0.0565 -0.0366 -0.0801  0.0632 -0.0935 -0.0335
 0.0861 -0.0139 -0.0677 -0.0437  0.0869  0.0720 -0.0273 -0.0436 -0.0325 -0.0961
-0.0121  0.0179 -0.0824  0.0009  0.0483  0.0867  0.0940  0.0148 -0.0182 -0.0262

Columns 71 to 80
-0.0359 -0.0326 -0.0724 -0.0351  0.0989 -0.0207  0.0475 -0.0495 -0.0205  0.0794
-0.0165 -0.0359 -0.0116 -0.0947 -0.0707  0.0990 -0.0548  0.0477  0.0473  0.0423
-0.0256  0.0287  0.0642  0.0071  0.0251 -0.0694 -0.0143  0.0208 -0.0719 -0.0931
 0.0521 -0.0171 -0.0390 -0.0729 -0.0052  0.0952 -0.0304 -0.0950  0.0981 -0.0053
-0.0254  0.0370 -0.0718 -0.0309  0.0489  0.0097  0.0504  0.0406  0.0780  0.0296
-0.0817 -0.0318  0.0183  0.0923  0.0206 -0.0864 -0.0746 -0.0644 -0.0787  0.0717

Columns 81 to 90
-0.0198 -0.0990  0.0808  0.0119  0.0577 -0.0945  0.0353 -0.0129 -0.0785  0.0083
-0.0904  0.0799 -0.0980  0.0643  0.0961 -0.0435 -0.0063  0.0508 -0.0439 -0.0145
-0.0101 -0.0817 -0.0273  0.0624  0.0206  0.0709  0.0725  0.0489  0.0626  0.0224
-0.0519  0.0733 -0.0561  0.0551 -0.0223 -0.0176  0.0084 -0.0560 -0.0885  0.0314
-0.0149  0.0869 -0.0562  0.0882  0.0353  0.0960  0.0540  0.0166 -0.0875 -0.0695
 0.0838 -0.0937  0.0843  0.0142  0.0281  0.0584 -0.0465  0.0279 -0.0629 -0.0268

Columns 91 to 100
 0.0127  0.0993  0.0660  0.0254 -0.0268 -0.0215  0.0555 -0.0034  0.0149  0.0477
-0.0760  0.0579 -0.0249 -0.0965 -0.0500  0.0422 -0.0699  0.0818  0.0294 -0.0682
-0.0492 -0.0511  0.0343 -0.0608 -0.0538 -0.0473 -0.0133 -0.0726  0.0196 -0.0418
 0.0882  0.0541 -0.0618  0.0648 -0.0287 -0.0801 -0.0509  0.0563  0.0856  0.0315
-0.0229  0.0270 -0.0941  0.0359  0.0833 -0.0546  0.0229 -0.0122  0.0018 -0.0727
-0.0394  0.0337 -0.0035  0.0857  0.0886 -0.0642  0.0669  0.0396  0.0175  0.0211
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:09[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:09[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:09[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 37 [batchSize = 1]
 
2016-10-13 16:47:10[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.325709025065ms 
2016-10-13 16:47:10[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:10[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0547  0.0294  0.0814 -0.0739  0.0172  0.0445 -0.0808  0.0766 -0.0748  0.0714
 0.0749  0.0323 -0.0912 -0.0732  0.0385  0.0186 -0.0359 -0.0662 -0.0900 -0.0796
-0.0727 -0.0811  0.0697 -0.0014  0.0695 -0.0918  0.0309 -0.0531  0.0499  0.0377
-0.0259 -0.0236 -0.0258  0.0130  0.0794  0.0156 -0.0121  0.0575  0.0847  0.0205
 0.0633 -0.0577  0.0453  0.0240 -0.0064 -0.0598 -0.0476  0.0807  0.0418  0.0266
 0.0053 -0.0165 -0.0635 -0.0537  0.0821  0.1011 -0.0769  0.0339  0.0956 -0.0701

Columns 11 to 20
-0.0914 -0.0502 -0.0692  0.0425 -0.0895 -0.0579  0.0118 -0.0372  0.0583  0.0191
-0.0772  0.0521  0.0170  0.0908  0.0387 -0.0273  0.0331 -0.0865 -0.0022  0.0819
 0.0882 -0.0114  0.0333 -0.0716  0.0880  0.0580  0.0255 -0.0841  0.0989 -0.0618
-0.0950 -0.0681  0.0509  0.0005  0.0725  0.0053 -0.0769  0.0353  0.0097  0.0953
-0.0561  0.0327  0.0933 -0.0451 -0.0829 -0.0175 -0.0111  0.0573  0.0858 -0.0223
-0.0058  0.0965  0.1005 -0.0698 -0.0227  0.0225 -0.0248 -0.0134  0.0422  0.0114

Columns 21 to 30
-0.0489  0.0223  0.0449  0.0613 -0.0560  0.0507  0.0904 -0.0812  0.0617  0.0624
-0.0542 -0.0807 -0.0815  0.0058  0.0033 -0.0012  0.0046 -0.0140 -0.0925  0.0785
 0.0022 -0.0302 -0.0956 -0.0497 -0.0059  0.0955 -0.0592 -0.0958  0.0658 -0.0754
 0.0882  0.0331  0.0216 -0.0281 -0.0823  0.0324 -0.0675  0.0583  0.0147 -0.0396
 0.0580  0.0048 -0.0106  0.0346  0.0796 -0.0093 -0.0853 -0.0455 -0.0377  0.0253
 0.0664  0.0226 -0.0959 -0.0750 -0.0165 -0.0189 -0.0392 -0.0702  0.0458 -0.0664

Columns 31 to 40
 0.0119 -0.0046 -0.0677 -0.0172  0.0990 -0.0983  0.0831 -0.0368  0.0177  0.0243
 0.0237 -0.0626  0.0161 -0.0875  0.0630  0.0060  0.0193  0.0056  0.0485  0.0141
 0.0104 -0.0579  0.0208  0.0880 -0.0276 -0.0739 -0.0222  0.0354  0.0534  0.0571
-0.1017 -0.0210  0.0891 -0.0031 -0.0225  0.0640  0.0846 -0.0636  0.0309 -0.0440
-0.0371  0.0576  0.0251 -0.0939  0.0309  0.0991 -0.0159  0.0171  0.0563  0.0097
-0.0034  0.0025 -0.0099 -0.0865  0.0736 -0.0698  0.0370  0.0018  0.0235  0.0418

Columns 41 to 50
-0.0525  0.0050 -0.0874 -0.0511  0.0065 -0.0947  0.0612 -0.0597  0.0634  0.0954
 0.0846 -0.0002  0.0247  0.0196  0.0137  0.0355 -0.0679  0.0659 -0.0938 -0.0394
 0.0742  0.0570  0.0909  0.0886  0.0536  0.0816 -0.0643  0.0888 -0.0715 -0.0058
-0.0898  0.0468  0.0172 -0.0011 -0.0228  0.0636  0.0323 -0.0356 -0.0370 -0.0171
-0.0400 -0.0357  0.0242  0.0540 -0.0047  0.0581 -0.0822  0.0217 -0.0061 -0.1007
-0.0637  0.0762 -0.0601  0.0094 -0.0064 -0.0560 -0.0981  0.0508  0.0791  0.0083

Columns 51 to 60
-0.0975  0.0971  0.0797  0.0150  0.0663 -0.0873 -0.0053  0.0542 -0.0256  0.0728
 0.0308  0.0444 -0.0109  0.0760  0.0001 -0.0131 -0.0188  0.0970  0.0654 -0.0073
-0.0732 -0.0688  0.0697  0.0058  0.0473  0.0323 -0.0735  0.0786 -0.0209  0.0554
-0.0074 -0.0338  0.0335 -0.0657 -0.0960 -0.0921 -0.0356  0.0600  0.0439  0.0475
 0.0194 -0.0170  0.0320  0.0802 -0.0108  0.0170  0.0526  0.0586  0.0408  0.0628
 0.0234 -0.0733 -0.0464 -0.0162 -0.0546  0.0940 -0.0277  0.0633 -0.0414  0.0314

Columns 61 to 70
-0.0416  0.0499 -0.0382 -0.0935 -0.0604  0.0737 -0.0125  0.0553  0.0815 -0.0673
-0.0752  0.0072  0.0730  0.0542  0.0322 -0.0961 -0.0926  0.0370  0.0088  0.0414
-0.0963  0.0855 -0.0460  0.0904 -0.0378 -0.0217 -0.0959 -0.0509  0.0555 -0.0538
 0.0525 -0.0525  0.0001  0.0971 -0.0565 -0.0366 -0.0801  0.0632 -0.0934 -0.0335
 0.0861 -0.0139 -0.0677 -0.0437  0.0869  0.0720 -0.0273 -0.0435 -0.0325 -0.0961
-0.0121  0.0179 -0.0824  0.0008  0.0483  0.0867  0.0940  0.0148 -0.0182 -0.0262

Columns 71 to 80
-0.0359 -0.0326 -0.0724 -0.0351  0.0989 -0.0207  0.0474 -0.0495 -0.0205  0.0794
-0.0165 -0.0358 -0.0116 -0.0946 -0.0707  0.0990 -0.0548  0.0477  0.0473  0.0423
-0.0256  0.0287  0.0642  0.0071  0.0251 -0.0693 -0.0144  0.0208 -0.0719 -0.0931
 0.0521 -0.0171 -0.0389 -0.0728 -0.0053  0.0953 -0.0304 -0.0950  0.0981 -0.0053
-0.0255  0.0370 -0.0717 -0.0309  0.0489  0.0097  0.0504  0.0406  0.0780  0.0296
-0.0817 -0.0318  0.0184  0.0924  0.0206 -0.0864 -0.0746 -0.0644 -0.0787  0.0717

Columns 81 to 90
-0.0198 -0.0990  0.0808  0.0120  0.0576 -0.0945  0.0353 -0.0129 -0.0785  0.0083
-0.0904  0.0799 -0.0980  0.0643  0.0961 -0.0435 -0.0063  0.0508 -0.0439 -0.0145
-0.0101 -0.0817 -0.0273  0.0625  0.0205  0.0709  0.0725  0.0489  0.0626  0.0224
-0.0519  0.0732 -0.0561  0.0551 -0.0223 -0.0176  0.0084 -0.0560 -0.0885  0.0314
-0.0149  0.0869 -0.0562  0.0883  0.0353  0.0960  0.0540  0.0166 -0.0875 -0.0695
 0.0838 -0.0937  0.0843  0.0143  0.0281  0.0584 -0.0465  0.0279 -0.0629 -0.0268

Columns 91 to 100
 0.0127  0.0993  0.0660  0.0253 -0.0268 -0.0215  0.0555 -0.0035  0.0149  0.0477
-0.0760  0.0579 -0.0250 -0.0965 -0.0500  0.0421 -0.0699  0.0817  0.0294 -0.0682
-0.0492 -0.0510  0.0343 -0.0609 -0.0537 -0.0473 -0.0133 -0.0726  0.0196 -0.0418
 0.0882  0.0541 -0.0618  0.0647 -0.0287 -0.0801 -0.0509  0.0562  0.0856  0.0315
-0.0229  0.0270 -0.0941  0.0358  0.0833 -0.0546  0.0229 -0.0123  0.0018 -0.0726
-0.0395  0.0337 -0.0035  0.0856  0.0887 -0.0642  0.0670  0.0396  0.0175  0.0211
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:10[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:10[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:10[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 38 [batchSize = 1]
 
2016-10-13 16:47:10[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.331351598104ms 
2016-10-13 16:47:10[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:10[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0547  0.0294  0.0814 -0.0739  0.0172  0.0446 -0.0808  0.0766 -0.0748  0.0714
 0.0749  0.0323 -0.0912 -0.0731  0.0385  0.0187 -0.0359 -0.0661 -0.0900 -0.0796
-0.0727 -0.0811  0.0697 -0.0014  0.0695 -0.0918  0.0309 -0.0531  0.0499  0.0377
-0.0259 -0.0237 -0.0257  0.0130  0.0794  0.0158 -0.0121  0.0575  0.0847  0.0205
 0.0633 -0.0577  0.0453  0.0241 -0.0064 -0.0598 -0.0476  0.0807  0.0418  0.0266
 0.0053 -0.0165 -0.0635 -0.0537  0.0821  0.1011 -0.0770  0.0339  0.0956 -0.0701

Columns 11 to 20
-0.0914 -0.0502 -0.0692  0.0425 -0.0895 -0.0578  0.0118 -0.0372  0.0583  0.0190
-0.0772  0.0521  0.0170  0.0908  0.0387 -0.0273  0.0331 -0.0865 -0.0022  0.0819
 0.0882 -0.0113  0.0334 -0.0716  0.0880  0.0580  0.0255 -0.0841  0.0989 -0.0619
-0.0950 -0.0680  0.0510  0.0005  0.0725  0.0053 -0.0770  0.0352  0.0096  0.0953
-0.0561  0.0327  0.0934 -0.0451 -0.0829 -0.0175 -0.0112  0.0572  0.0858 -0.0224
-0.0058  0.0965  0.1006 -0.0698 -0.0227  0.0225 -0.0248 -0.0134  0.0422  0.0114

Columns 21 to 30
-0.0489  0.0223  0.0449  0.0613 -0.0560  0.0507  0.0904 -0.0812  0.0616  0.0624
-0.0542 -0.0807 -0.0815  0.0058  0.0033 -0.0012  0.0046 -0.0140 -0.0925  0.0785
 0.0022 -0.0302 -0.0955 -0.0496 -0.0059  0.0955 -0.0592 -0.0958  0.0658 -0.0754
 0.0881  0.0331  0.0216 -0.0281 -0.0823  0.0324 -0.0677  0.0583  0.0147 -0.0396
 0.0580  0.0048 -0.0106  0.0346  0.0796 -0.0093 -0.0854 -0.0454 -0.0378  0.0253
 0.0664  0.0226 -0.0959 -0.0750 -0.0165 -0.0189 -0.0392 -0.0701  0.0457 -0.0665

Columns 31 to 40
 0.0118 -0.0046 -0.0677 -0.0172  0.0990 -0.0983  0.0831 -0.0368  0.0177  0.0243
 0.0237 -0.0626  0.0161 -0.0876  0.0630  0.0060  0.0193  0.0056  0.0485  0.0142
 0.0104 -0.0579  0.0208  0.0880 -0.0276 -0.0739 -0.0222  0.0354  0.0534  0.0571
-0.1018 -0.0209  0.0891 -0.0031 -0.0225  0.0640  0.0846 -0.0636  0.0308 -0.0440
-0.0371  0.0576  0.0251 -0.0939  0.0309  0.0991 -0.0160  0.0172  0.0563  0.0097
-0.0034  0.0025 -0.0100 -0.0865  0.0736 -0.0697  0.0370  0.0018  0.0235  0.0418

Columns 41 to 50
-0.0525  0.0050 -0.0874 -0.0511  0.0065 -0.0947  0.0612 -0.0597  0.0634  0.0953
 0.0847 -0.0001  0.0248  0.0196  0.0137  0.0355 -0.0680  0.0659 -0.0938 -0.0394
 0.0742  0.0570  0.0909  0.0885  0.0536  0.0816 -0.0643  0.0888 -0.0715 -0.0058
-0.0897  0.0468  0.0172 -0.0011 -0.0228  0.0636  0.0323 -0.0356 -0.0371 -0.0172
-0.0400 -0.0357  0.0242  0.0540 -0.0047  0.0581 -0.0822  0.0217 -0.0062 -0.1007
-0.0637  0.0762 -0.0601  0.0093 -0.0064 -0.0560 -0.0981  0.0508  0.0791  0.0083

Columns 51 to 60
-0.0974  0.0971  0.0797  0.0150  0.0663 -0.0873 -0.0053  0.0542 -0.0256  0.0728
 0.0308  0.0444 -0.0108  0.0760  0.0001 -0.0131 -0.0189  0.0971  0.0654 -0.0073
-0.0732 -0.0687  0.0698  0.0059  0.0473  0.0323 -0.0735  0.0786 -0.0209  0.0554
-0.0073 -0.0338  0.0336 -0.0656 -0.0959 -0.0921 -0.0356  0.0600  0.0439  0.0475
 0.0194 -0.0170  0.0321  0.0803 -0.0108  0.0170  0.0526  0.0586  0.0408  0.0628
 0.0234 -0.0733 -0.0463 -0.0162 -0.0546  0.0940 -0.0277  0.0633 -0.0414  0.0314

Columns 61 to 70
-0.0416  0.0499 -0.0382 -0.0935 -0.0604  0.0737 -0.0125  0.0553  0.0815 -0.0673
-0.0752  0.0072  0.0730  0.0541  0.0322 -0.0961 -0.0926  0.0370  0.0088  0.0414
-0.0963  0.0856 -0.0460  0.0903 -0.0378 -0.0216 -0.0959 -0.0509  0.0555 -0.0538
 0.0525 -0.0525  0.0001  0.0970 -0.0565 -0.0365 -0.0801  0.0632 -0.0934 -0.0336
 0.0861 -0.0138 -0.0677 -0.0437  0.0869  0.0720 -0.0273 -0.0435 -0.0324 -0.0962
-0.0121  0.0179 -0.0824  0.0008  0.0483  0.0867  0.0940  0.0148 -0.0182 -0.0262

Columns 71 to 80
-0.0359 -0.0326 -0.0724 -0.0351  0.0989 -0.0207  0.0474 -0.0495 -0.0205  0.0794
-0.0165 -0.0358 -0.0116 -0.0946 -0.0707  0.0991 -0.0548  0.0477  0.0473  0.0423
-0.0257  0.0288  0.0642  0.0072  0.0250 -0.0693 -0.0144  0.0208 -0.0719 -0.0931
 0.0520 -0.0171 -0.0389 -0.0728 -0.0053  0.0953 -0.0305 -0.0950  0.0981 -0.0053
-0.0255  0.0370 -0.0717 -0.0308  0.0489  0.0097  0.0504  0.0406  0.0780  0.0296
-0.0817 -0.0318  0.0184  0.0924  0.0206 -0.0864 -0.0746 -0.0644 -0.0787  0.0717

Columns 81 to 90
-0.0198 -0.0990  0.0808  0.0120  0.0576 -0.0944  0.0353 -0.0129 -0.0785  0.0083
-0.0904  0.0799 -0.0980  0.0644  0.0961 -0.0435 -0.0063  0.0508 -0.0439 -0.0145
-0.0101 -0.0817 -0.0273  0.0625  0.0205  0.0709  0.0725  0.0489  0.0626  0.0224
-0.0520  0.0732 -0.0560  0.0553 -0.0224 -0.0176  0.0083 -0.0560 -0.0885  0.0314
-0.0149  0.0869 -0.0562  0.0883  0.0352  0.0960  0.0540  0.0166 -0.0875 -0.0695
 0.0838 -0.0937  0.0843  0.0143  0.0281  0.0584 -0.0465  0.0279 -0.0629 -0.0268

Columns 91 to 100
 0.0127  0.0993  0.0660  0.0253 -0.0268 -0.0215  0.0555 -0.0035  0.0149  0.0477
-0.0760  0.0579 -0.0250 -0.0965 -0.0499  0.0421 -0.0699  0.0817  0.0293 -0.0681
-0.0492 -0.0510  0.0343 -0.0609 -0.0537 -0.0473 -0.0133 -0.0726  0.0196 -0.0417
 0.0882  0.0541 -0.0618  0.0646 -0.0286 -0.0801 -0.0509  0.0562  0.0856  0.0316
-0.0229  0.0270 -0.0941  0.0358  0.0833 -0.0546  0.0229 -0.0123  0.0017 -0.0726
-0.0395  0.0337 -0.0035  0.0856  0.0887 -0.0642  0.0670  0.0396  0.0174  0.0212
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:10[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:10[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:10[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 39 [batchSize = 1]
 
2016-10-13 16:47:10[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.313947041829ms 
2016-10-13 16:47:10[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:10[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0547  0.0293  0.0814 -0.0739  0.0172  0.0446 -0.0808  0.0766 -0.0748  0.0714
 0.0750  0.0322 -0.0912 -0.0731  0.0385  0.0187 -0.0359 -0.0661 -0.0900 -0.0796
-0.0727 -0.0812  0.0697 -0.0013  0.0695 -0.0917  0.0308 -0.0531  0.0499  0.0377
-0.0258 -0.0238 -0.0257  0.0131  0.0794  0.0159 -0.0122  0.0576  0.0846  0.0205
 0.0633 -0.0578  0.0453  0.0241 -0.0063 -0.0597 -0.0477  0.0807  0.0418  0.0266
 0.0054 -0.0166 -0.0635 -0.0537  0.0821  0.1011 -0.0770  0.0339  0.0956 -0.0701

Columns 11 to 20
-0.0914 -0.0502 -0.0691  0.0425 -0.0895 -0.0578  0.0118 -0.0372  0.0583  0.0190
-0.0772  0.0521  0.0170  0.0908  0.0387 -0.0273  0.0331 -0.0865 -0.0022  0.0819
 0.0882 -0.0113  0.0334 -0.0716  0.0880  0.0580  0.0255 -0.0841  0.0989 -0.0619
-0.0950 -0.0680  0.0510  0.0004  0.0724  0.0053 -0.0770  0.0352  0.0096  0.0952
-0.0561  0.0327  0.0934 -0.0451 -0.0829 -0.0175 -0.0112  0.0572  0.0858 -0.0224
-0.0058  0.0965  0.1006 -0.0698 -0.0227  0.0225 -0.0248 -0.0134  0.0422  0.0114

Columns 21 to 30
-0.0489  0.0223  0.0449  0.0613 -0.0560  0.0507  0.0903 -0.0812  0.0616  0.0624
-0.0542 -0.0807 -0.0814  0.0058  0.0033 -0.0012  0.0045 -0.0140 -0.0925  0.0785
 0.0022 -0.0302 -0.0955 -0.0496 -0.0059  0.0955 -0.0592 -0.0957  0.0658 -0.0755
 0.0881  0.0332  0.0217 -0.0280 -0.0823  0.0324 -0.0677  0.0584  0.0147 -0.0397
 0.0580  0.0048 -0.0105  0.0347  0.0796 -0.0093 -0.0854 -0.0454 -0.0378  0.0253
 0.0664  0.0226 -0.0959 -0.0750 -0.0165 -0.0189 -0.0392 -0.0701  0.0457 -0.0665

Columns 31 to 40
 0.0118 -0.0046 -0.0678 -0.0172  0.0990 -0.0982  0.0831 -0.0368  0.0177  0.0243
 0.0236 -0.0626  0.0161 -0.0876  0.0630  0.0060  0.0192  0.0056  0.0484  0.0142
 0.0103 -0.0579  0.0208  0.0879 -0.0276 -0.0739 -0.0222  0.0355  0.0534  0.0571
-0.1018 -0.0209  0.0890 -0.0031 -0.0225  0.0640  0.0846 -0.0635  0.0308 -0.0440
-0.0372  0.0577  0.0251 -0.0939  0.0309  0.0991 -0.0160  0.0172  0.0563  0.0097
-0.0035  0.0025 -0.0100 -0.0866  0.0736 -0.0697  0.0370  0.0018  0.0235  0.0418

Columns 41 to 50
-0.0525  0.0050 -0.0874 -0.0512  0.0065 -0.0947  0.0612 -0.0597  0.0634  0.0953
 0.0847 -0.0001  0.0248  0.0196  0.0137  0.0355 -0.0680  0.0659 -0.0939 -0.0394
 0.0742  0.0571  0.0909  0.0885  0.0536  0.0816 -0.0643  0.0888 -0.0715 -0.0058
-0.0897  0.0468  0.0172 -0.0012 -0.0228  0.0636  0.0322 -0.0356 -0.0372 -0.0172
-0.0400 -0.0357  0.0242  0.0540 -0.0047  0.0581 -0.0822  0.0217 -0.0062 -0.1007
-0.0637  0.0762 -0.0601  0.0093 -0.0064 -0.0560 -0.0981  0.0508  0.0790  0.0083

Columns 51 to 60
-0.0974  0.0972  0.0797  0.0150  0.0663 -0.0873 -0.0053  0.0542 -0.0256  0.0728
 0.0308  0.0444 -0.0108  0.0760  0.0002 -0.0131 -0.0189  0.0971  0.0654 -0.0073
-0.0732 -0.0687  0.0698  0.0059  0.0474  0.0323 -0.0735  0.0786 -0.0209  0.0554
-0.0073 -0.0338  0.0337 -0.0656 -0.0958 -0.0921 -0.0356  0.0600  0.0440  0.0475
 0.0194 -0.0170  0.0321  0.0803 -0.0108  0.0170  0.0526  0.0586  0.0408  0.0628
 0.0234 -0.0733 -0.0463 -0.0162 -0.0546  0.0940 -0.0278  0.0634 -0.0414  0.0314

Columns 61 to 70
-0.0415  0.0499 -0.0382 -0.0935 -0.0604  0.0737 -0.0125  0.0553  0.0815 -0.0673
-0.0751  0.0073  0.0730  0.0541  0.0322 -0.0961 -0.0926  0.0370  0.0088  0.0414
-0.0963  0.0856 -0.0460  0.0903 -0.0378 -0.0216 -0.0959 -0.0509  0.0556 -0.0539
 0.0526 -0.0524  0.0000  0.0970 -0.0565 -0.0365 -0.0801  0.0633 -0.0933 -0.0337
 0.0861 -0.0138 -0.0677 -0.0437  0.0869  0.0720 -0.0273 -0.0435 -0.0324 -0.0962
-0.0121  0.0180 -0.0824  0.0008  0.0483  0.0867  0.0940  0.0148 -0.0182 -0.0262

Columns 71 to 80
-0.0359 -0.0326 -0.0724 -0.0351  0.0989 -0.0206  0.0474 -0.0495 -0.0205  0.0794
-0.0165 -0.0358 -0.0116 -0.0946 -0.0707  0.0991 -0.0549  0.0477  0.0473  0.0423
-0.0257  0.0288  0.0642  0.0072  0.0250 -0.0692 -0.0144  0.0208 -0.0719 -0.0931
 0.0520 -0.0170 -0.0388 -0.0728 -0.0053  0.0954 -0.0306 -0.0950  0.0981 -0.0054
-0.0255  0.0370 -0.0717 -0.0308  0.0489  0.0098  0.0503  0.0406  0.0780  0.0296
-0.0817 -0.0318  0.0184  0.0924  0.0206 -0.0863 -0.0746 -0.0644 -0.0787  0.0717

Columns 81 to 90
-0.0198 -0.0990  0.0808  0.0121  0.0576 -0.0944  0.0353 -0.0129 -0.0785  0.0083
-0.0904  0.0799 -0.0980  0.0644  0.0961 -0.0435 -0.0063  0.0508 -0.0439 -0.0144
-0.0101 -0.0817 -0.0272  0.0626  0.0205  0.0709  0.0725  0.0489  0.0626  0.0224
-0.0520  0.0732 -0.0560  0.0554 -0.0224 -0.0175  0.0083 -0.0560 -0.0885  0.0314
-0.0149  0.0869 -0.0562  0.0883  0.0352  0.0960  0.0540  0.0166 -0.0875 -0.0695
 0.0838 -0.0937  0.0844  0.0143  0.0281  0.0584 -0.0465  0.0279 -0.0629 -0.0268

Columns 91 to 100
 0.0127  0.0993  0.0660  0.0253 -0.0268 -0.0215  0.0555 -0.0035  0.0148  0.0478
-0.0760  0.0579 -0.0250 -0.0965 -0.0499  0.0421 -0.0698  0.0817  0.0293 -0.0681
-0.0492 -0.0510  0.0343 -0.0609 -0.0537 -0.0473 -0.0133 -0.0727  0.0195 -0.0417
 0.0881  0.0542 -0.0618  0.0646 -0.0286 -0.0801 -0.0508  0.0561  0.0855  0.0317
-0.0230  0.0270 -0.0942  0.0358  0.0833 -0.0546  0.0229 -0.0123  0.0017 -0.0726
-0.0395  0.0337 -0.0035  0.0856  0.0887 -0.0642  0.0670  0.0396  0.0174  0.0212
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:10[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:10[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:10[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 40 [batchSize = 1]
 
2016-10-13 16:47:10[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.40398979187ms 
2016-10-13 16:47:10[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> saving network model to /home/robotec/catkin_ws/src/superchicko/farnn/src/network/glassfurnace_lstm-net.t7 
2016-10-13 16:47:10[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:10[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0546  0.0293  0.0814 -0.0739  0.0172  0.0446 -0.0808  0.0766 -0.0748  0.0714
 0.0750  0.0322 -0.0912 -0.0731  0.0386  0.0188 -0.0359 -0.0661 -0.0901 -0.0795
-0.0726 -0.0812  0.0697 -0.0013  0.0695 -0.0917  0.0308 -0.0530  0.0498  0.0377
-0.0258 -0.0239 -0.0257  0.0132  0.0794  0.0160 -0.0122  0.0576  0.0845  0.0205
 0.0634 -0.0578  0.0453  0.0241 -0.0063 -0.0597 -0.0477  0.0807  0.0417  0.0266
 0.0054 -0.0166 -0.0635 -0.0537  0.0821  0.1012 -0.0770  0.0339  0.0955 -0.0701

Columns 11 to 20
-0.0914 -0.0502 -0.0691  0.0425 -0.0895 -0.0578  0.0118 -0.0372  0.0583  0.0190
-0.0772  0.0521  0.0171  0.0908  0.0387 -0.0273  0.0331 -0.0866 -0.0022  0.0818
 0.0882 -0.0113  0.0334 -0.0716  0.0880  0.0580  0.0255 -0.0841  0.0989 -0.0619
-0.0950 -0.0680  0.0511  0.0004  0.0724  0.0054 -0.0770  0.0352  0.0096  0.0952
-0.0561  0.0327  0.0934 -0.0451 -0.0829 -0.0175 -0.0112  0.0572  0.0858 -0.0224
-0.0058  0.0965  0.1006 -0.0698 -0.0227  0.0225 -0.0248 -0.0134  0.0422  0.0114

Columns 21 to 30
-0.0490  0.0223  0.0449  0.0613 -0.0560  0.0507  0.0903 -0.0812  0.0616  0.0624
-0.0542 -0.0807 -0.0814  0.0058  0.0033 -0.0012  0.0045 -0.0139 -0.0925  0.0784
 0.0022 -0.0302 -0.0955 -0.0496 -0.0059  0.0955 -0.0592 -0.0957  0.0658 -0.0755
 0.0881  0.0332  0.0217 -0.0280 -0.0823  0.0324 -0.0677  0.0584  0.0147 -0.0398
 0.0580  0.0048 -0.0105  0.0347  0.0796 -0.0093 -0.0854 -0.0454 -0.0378  0.0253
 0.0664  0.0226 -0.0959 -0.0749 -0.0165 -0.0189 -0.0392 -0.0701  0.0457 -0.0665

Columns 31 to 40
 0.0118 -0.0045 -0.0678 -0.0172  0.0990 -0.0982  0.0831 -0.0368  0.0177  0.0243
 0.0236 -0.0626  0.0161 -0.0876  0.0630  0.0060  0.0192  0.0056  0.0484  0.0142
 0.0103 -0.0579  0.0208  0.0879 -0.0276 -0.0739 -0.0222  0.0355  0.0534  0.0571
-0.1019 -0.0208  0.0890 -0.0032 -0.0226  0.0640  0.0845 -0.0634  0.0308 -0.0439
-0.0372  0.0577  0.0251 -0.0939  0.0309  0.0991 -0.0160  0.0172  0.0563  0.0097
-0.0035  0.0025 -0.0100 -0.0866  0.0736 -0.0697  0.0369  0.0018  0.0235  0.0418

Columns 41 to 50
-0.0525  0.0050 -0.0874 -0.0512  0.0065 -0.0947  0.0612 -0.0597  0.0634  0.0953
 0.0847 -0.0001  0.0248  0.0195  0.0137  0.0356 -0.0680  0.0659 -0.0939 -0.0395
 0.0742  0.0571  0.0909  0.0885  0.0536  0.0816 -0.0643  0.0888 -0.0716 -0.0059
-0.0896  0.0468  0.0173 -0.0012 -0.0228  0.0636  0.0322 -0.0356 -0.0372 -0.0173
-0.0400 -0.0357  0.0242  0.0540 -0.0047  0.0581 -0.0823  0.0217 -0.0062 -0.1008
-0.0637  0.0762 -0.0601  0.0093 -0.0064 -0.0560 -0.0982  0.0508  0.0790  0.0082

Columns 51 to 60
-0.0974  0.0972  0.0797  0.0150  0.0663 -0.0873 -0.0053  0.0542 -0.0256  0.0727
 0.0308  0.0445 -0.0108  0.0760  0.0002 -0.0131 -0.0189  0.0971  0.0655 -0.0073
-0.0732 -0.0687  0.0699  0.0059  0.0474  0.0323 -0.0736  0.0786 -0.0209  0.0554
-0.0073 -0.0338  0.0337 -0.0655 -0.0958 -0.0922 -0.0357  0.0601  0.0440  0.0475
 0.0194 -0.0170  0.0321  0.0803 -0.0108  0.0169  0.0526  0.0586  0.0408  0.0628
 0.0234 -0.0733 -0.0463 -0.0161 -0.0545  0.0939 -0.0278  0.0634 -0.0414  0.0314

Columns 61 to 70
-0.0415  0.0500 -0.0382 -0.0935 -0.0604  0.0737 -0.0125  0.0553  0.0816 -0.0674
-0.0751  0.0073  0.0730  0.0541  0.0322 -0.0960 -0.0926  0.0370  0.0088  0.0413
-0.0963  0.0856 -0.0460  0.0903 -0.0378 -0.0216 -0.0959 -0.0508  0.0556 -0.0539
 0.0526 -0.0523  0.0000  0.0969 -0.0565 -0.0364 -0.0801  0.0633 -0.0932 -0.0338
 0.0862 -0.0138 -0.0677 -0.0437  0.0869  0.0720 -0.0273 -0.0435 -0.0324 -0.0962
-0.0120  0.0180 -0.0824  0.0008  0.0483  0.0867  0.0940  0.0148 -0.0182 -0.0263

Columns 71 to 80
-0.0359 -0.0325 -0.0724 -0.0351  0.0989 -0.0206  0.0474 -0.0495 -0.0205  0.0794
-0.0165 -0.0358 -0.0116 -0.0946 -0.0707  0.0992 -0.0549  0.0477  0.0473  0.0423
-0.0257  0.0288  0.0643  0.0072  0.0250 -0.0692 -0.0145  0.0208 -0.0719 -0.0932
 0.0520 -0.0170 -0.0388 -0.0728 -0.0054  0.0956 -0.0306 -0.0950  0.0982 -0.0054
-0.0255  0.0370 -0.0717 -0.0308  0.0489  0.0098  0.0503  0.0406  0.0780  0.0296
-0.0818 -0.0318  0.0184  0.0924  0.0206 -0.0863 -0.0747 -0.0644 -0.0787  0.0717

Columns 81 to 90
-0.0198 -0.0990  0.0808  0.0120  0.0576 -0.0944  0.0353 -0.0129 -0.0785  0.0083
-0.0904  0.0799 -0.0979  0.0644  0.0961 -0.0435 -0.0063  0.0508 -0.0439 -0.0144
-0.0101 -0.0817 -0.0272  0.0626  0.0205  0.0709  0.0725  0.0489  0.0626  0.0224
-0.0520  0.0732 -0.0559  0.0554 -0.0224 -0.0175  0.0083 -0.0560 -0.0885  0.0314
-0.0149  0.0869 -0.0562  0.0884  0.0352  0.0960  0.0540  0.0166 -0.0875 -0.0695
 0.0838 -0.0937  0.0844  0.0143  0.0281  0.0584 -0.0465  0.0279 -0.0629 -0.0268

Columns 91 to 100
 0.0127  0.0993  0.0660  0.0253 -0.0268 -0.0215  0.0555 -0.0035  0.0148  0.0478
-0.0760  0.0580 -0.0250 -0.0966 -0.0499  0.0421 -0.0698  0.0817  0.0293 -0.0681
-0.0492 -0.0510  0.0342 -0.0610 -0.0537 -0.0473 -0.0133 -0.0727  0.0195 -0.0417
 0.0881  0.0543 -0.0618  0.0645 -0.0285 -0.0801 -0.0508  0.0560  0.0854  0.0318
-0.0230  0.0271 -0.0942  0.0358  0.0834 -0.0546  0.0229 -0.0123  0.0017 -0.0726
-0.0395  0.0338 -0.0035  0.0856  0.0887 -0.0642  0.0670  0.0396  0.0174  0.0212
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:10[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:10[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:10[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 41 [batchSize = 1]
 
2016-10-13 16:47:10[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.451991399129ms 
2016-10-13 16:47:10[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:10[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0546  0.0293  0.0814 -0.0739  0.0173  0.0446 -0.0808  0.0766 -0.0748  0.0714
 0.0750  0.0322 -0.0912 -0.0731  0.0386  0.0188 -0.0359 -0.0661 -0.0901 -0.0795
-0.0726 -0.0812  0.0697 -0.0013  0.0695 -0.0916  0.0308 -0.0530  0.0498  0.0377
-0.0257 -0.0239 -0.0257  0.0132  0.0794  0.0160 -0.0122  0.0576  0.0845  0.0205
 0.0634 -0.0578  0.0453  0.0241 -0.0063 -0.0596 -0.0477  0.0808  0.0417  0.0266
 0.0054 -0.0166 -0.0635 -0.0536  0.0821  0.1012 -0.0770  0.0339  0.0955 -0.0701

Columns 11 to 20
-0.0914 -0.0502 -0.0691  0.0425 -0.0895 -0.0578  0.0118 -0.0372  0.0583  0.0190
-0.0772  0.0522  0.0171  0.0908  0.0387 -0.0273  0.0330 -0.0866 -0.0022  0.0818
 0.0882 -0.0113  0.0335 -0.0717  0.0880  0.0581  0.0255 -0.0841  0.0989 -0.0619
-0.0950 -0.0680  0.0512  0.0004  0.0724  0.0054 -0.0770  0.0351  0.0096  0.0951
-0.0561  0.0328  0.0934 -0.0451 -0.0829 -0.0175 -0.0112  0.0572  0.0858 -0.0224
-0.0058  0.0965  0.1006 -0.0698 -0.0227  0.0225 -0.0248 -0.0135  0.0422  0.0114

Columns 21 to 30
-0.0490  0.0223  0.0449  0.0613 -0.0560  0.0506  0.0903 -0.0812  0.0616  0.0623
-0.0542 -0.0807 -0.0814  0.0059  0.0033 -0.0012  0.0045 -0.0139 -0.0925  0.0784
 0.0022 -0.0302 -0.0955 -0.0496 -0.0059  0.0955 -0.0593 -0.0957  0.0658 -0.0755
 0.0881  0.0332  0.0217 -0.0280 -0.0823  0.0323 -0.0678  0.0585  0.0147 -0.0398
 0.0580  0.0048 -0.0105  0.0347  0.0796 -0.0093 -0.0854 -0.0454 -0.0378  0.0252
 0.0664  0.0226 -0.0958 -0.0749 -0.0165 -0.0189 -0.0393 -0.0701  0.0457 -0.0665

Columns 31 to 40
 0.0118 -0.0045 -0.0678 -0.0172  0.0990 -0.0982  0.0831 -0.0367  0.0177  0.0243
 0.0236 -0.0626  0.0160 -0.0876  0.0630  0.0060  0.0192  0.0057  0.0484  0.0142
 0.0103 -0.0578  0.0207  0.0879 -0.0276 -0.0739 -0.0223  0.0355  0.0534  0.0571
-0.1019 -0.0208  0.0890 -0.0032 -0.0226  0.0640  0.0844 -0.0634  0.0308 -0.0439
-0.0372  0.0577  0.0251 -0.0939  0.0309  0.0991 -0.0160  0.0172  0.0563  0.0097
-0.0035  0.0026 -0.0100 -0.0866  0.0736 -0.0697  0.0369  0.0019  0.0234  0.0418

Columns 41 to 50
-0.0524  0.0050 -0.0874 -0.0512  0.0065 -0.0947  0.0612 -0.0597  0.0634  0.0953
 0.0847 -0.0001  0.0248  0.0195  0.0137  0.0356 -0.0680  0.0660 -0.0939 -0.0395
 0.0742  0.0571  0.0909  0.0885  0.0536  0.0816 -0.0643  0.0888 -0.0716 -0.0059
-0.0896  0.0468  0.0173 -0.0013 -0.0228  0.0636  0.0322 -0.0356 -0.0373 -0.0174
-0.0400 -0.0357  0.0242  0.0539 -0.0047  0.0581 -0.0823  0.0218 -0.0062 -0.1008
-0.0637  0.0762 -0.0601  0.0093 -0.0064 -0.0560 -0.0982  0.0508  0.0790  0.0082

Columns 51 to 60
-0.0974  0.0972  0.0798  0.0151  0.0663 -0.0873 -0.0053  0.0543 -0.0256  0.0727
 0.0308  0.0445 -0.0107  0.0761  0.0002 -0.0131 -0.0189  0.0971  0.0655 -0.0073
-0.0732 -0.0687  0.0699  0.0059  0.0474  0.0323 -0.0736  0.0786 -0.0208  0.0554
-0.0073 -0.0338  0.0338 -0.0654 -0.0958 -0.0922 -0.0357  0.0601  0.0441  0.0475
 0.0194 -0.0170  0.0322  0.0803 -0.0108  0.0169  0.0526  0.0587  0.0409  0.0628
 0.0234 -0.0733 -0.0463 -0.0161 -0.0545  0.0939 -0.0278  0.0634 -0.0414  0.0313

Columns 61 to 70
-0.0415  0.0500 -0.0383 -0.0935 -0.0604  0.0737 -0.0125  0.0553  0.0816 -0.0674
-0.0751  0.0073  0.0729  0.0541  0.0322 -0.0960 -0.0926  0.0371  0.0089  0.0413
-0.0962  0.0857 -0.0460  0.0903 -0.0378 -0.0216 -0.0959 -0.0508  0.0556 -0.0540
 0.0527 -0.0523 -0.0000  0.0969 -0.0565 -0.0364 -0.0801  0.0633 -0.0932 -0.0339
 0.0862 -0.0137 -0.0678 -0.0438  0.0869  0.0720 -0.0273 -0.0435 -0.0324 -0.0963
-0.0120  0.0180 -0.0824  0.0008  0.0483  0.0867  0.0940  0.0148 -0.0181 -0.0263

Columns 71 to 80
-0.0359 -0.0325 -0.0724 -0.0351  0.0989 -0.0205  0.0474 -0.0495 -0.0205  0.0794
-0.0165 -0.0358 -0.0115 -0.0946 -0.0707  0.0992 -0.0549  0.0477  0.0474  0.0423
-0.0257  0.0288  0.0643  0.0072  0.0250 -0.0692 -0.0145  0.0208 -0.0719 -0.0932
 0.0520 -0.0170 -0.0388 -0.0728 -0.0054  0.0957 -0.0307 -0.0950  0.0982 -0.0054
-0.0255  0.0370 -0.0717 -0.0308  0.0488  0.0099  0.0503  0.0406  0.0780  0.0295
-0.0818 -0.0318  0.0184  0.0924  0.0205 -0.0863 -0.0747 -0.0644 -0.0787  0.0717

Columns 81 to 90
-0.0198 -0.0990  0.0809  0.0121  0.0576 -0.0944  0.0353 -0.0129 -0.0785  0.0083
-0.0905  0.0798 -0.0979  0.0645  0.0961 -0.0435 -0.0063  0.0508 -0.0439 -0.0144
-0.0101 -0.0817 -0.0272  0.0626  0.0205  0.0709  0.0725  0.0489  0.0626  0.0224
-0.0520  0.0732 -0.0559  0.0555 -0.0224 -0.0175  0.0083 -0.0560 -0.0885  0.0314
-0.0150  0.0869 -0.0562  0.0884  0.0352  0.0960  0.0540  0.0166 -0.0875 -0.0695
 0.0837 -0.0937  0.0844  0.0143  0.0281  0.0584 -0.0465  0.0279 -0.0629 -0.0268

Columns 91 to 100
 0.0127  0.0993  0.0660  0.0253 -0.0268 -0.0215  0.0555 -0.0035  0.0148  0.0478
-0.0760  0.0580 -0.0250 -0.0966 -0.0499  0.0421 -0.0698  0.0816  0.0292 -0.0680
-0.0492 -0.0510  0.0342 -0.0610 -0.0537 -0.0473 -0.0132 -0.0727  0.0195 -0.0416
 0.0881  0.0543 -0.0618  0.0645 -0.0285 -0.0801 -0.0508  0.0560  0.0854  0.0318
-0.0230  0.0271 -0.0942  0.0357  0.0834 -0.0546  0.0229 -0.0124  0.0016 -0.0725
-0.0395  0.0338 -0.0035  0.0856  0.0887 -0.0642  0.0670  0.0396  0.0174  0.0212
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:10[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:10[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:10[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 42 [batchSize = 1]
 
2016-10-13 16:47:11[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.360359191895ms 
2016-10-13 16:47:11[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:11[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0546  0.0293  0.0814 -0.0738  0.0172  0.0446 -0.0808  0.0766 -0.0748  0.0714
 0.0750  0.0321 -0.0912 -0.0730  0.0386  0.0188 -0.0359 -0.0660 -0.0901 -0.0795
-0.0726 -0.0813  0.0697 -0.0012  0.0695 -0.0916  0.0308 -0.0530  0.0498  0.0378
-0.0257 -0.0239 -0.0257  0.0132  0.0794  0.0160 -0.0122  0.0576  0.0844  0.0205
 0.0634 -0.0578  0.0453  0.0242 -0.0063 -0.0596 -0.0477  0.0808  0.0417  0.0267
 0.0054 -0.0166 -0.0635 -0.0536  0.0821  0.1012 -0.0770  0.0340  0.0955 -0.0701

Columns 11 to 20
-0.0914 -0.0502 -0.0691  0.0425 -0.0895 -0.0578  0.0118 -0.0372  0.0583  0.0190
-0.0772  0.0522  0.0171  0.0908  0.0387 -0.0273  0.0330 -0.0866 -0.0022  0.0818
 0.0882 -0.0113  0.0335 -0.0717  0.0880  0.0581  0.0255 -0.0842  0.0989 -0.0619
-0.0949 -0.0680  0.0512  0.0004  0.0724  0.0054 -0.0770  0.0351  0.0096  0.0951
-0.0561  0.0328  0.0935 -0.0451 -0.0829 -0.0174 -0.0112  0.0572  0.0858 -0.0224
-0.0057  0.0965  0.1007 -0.0699 -0.0227  0.0225 -0.0248 -0.0135  0.0422  0.0113

Columns 21 to 30
-0.0490  0.0223  0.0449  0.0613 -0.0560  0.0506  0.0903 -0.0811  0.0616  0.0623
-0.0542 -0.0807 -0.0814  0.0059  0.0033 -0.0012  0.0044 -0.0139 -0.0925  0.0784
 0.0022 -0.0302 -0.0955 -0.0496 -0.0059  0.0955 -0.0593 -0.0957  0.0658 -0.0755
 0.0880  0.0332  0.0217 -0.0279 -0.0823  0.0323 -0.0678  0.0585  0.0147 -0.0398
 0.0580  0.0048 -0.0105  0.0347  0.0796 -0.0093 -0.0855 -0.0454 -0.0378  0.0252
 0.0664  0.0226 -0.0958 -0.0749 -0.0165 -0.0189 -0.0393 -0.0701  0.0457 -0.0665

Columns 31 to 40
 0.0118 -0.0045 -0.0678 -0.0172  0.0990 -0.0982  0.0830 -0.0367  0.0177  0.0243
 0.0236 -0.0625  0.0160 -0.0876  0.0629  0.0060  0.0191  0.0057  0.0484  0.0142
 0.0103 -0.0578  0.0207  0.0879 -0.0276 -0.0739 -0.0223  0.0356  0.0534  0.0571
-0.1020 -0.0207  0.0890 -0.0032 -0.0226  0.0641  0.0844 -0.0633  0.0307 -0.0439
-0.0372  0.0577  0.0250 -0.0940  0.0309  0.0991 -0.0161  0.0173  0.0562  0.0097
-0.0035  0.0026 -0.0100 -0.0866  0.0736 -0.0697  0.0369  0.0019  0.0234  0.0418

Columns 41 to 50
-0.0524  0.0050 -0.0874 -0.0512  0.0065 -0.0947  0.0611 -0.0596  0.0634  0.0953
 0.0847 -0.0001  0.0248  0.0195  0.0137  0.0356 -0.0680  0.0660 -0.0939 -0.0396
 0.0743  0.0571  0.0909  0.0884  0.0536  0.0816 -0.0643  0.0888 -0.0716 -0.0059
-0.0895  0.0469  0.0173 -0.0013 -0.0228  0.0636  0.0322 -0.0355 -0.0373 -0.0174
-0.0399 -0.0357  0.0242  0.0539 -0.0047  0.0581 -0.0823  0.0218 -0.0063 -0.1008
-0.0636  0.0763 -0.0601  0.0093 -0.0064 -0.0560 -0.0982  0.0509  0.0790  0.0082

Columns 51 to 60
-0.0974  0.0972  0.0798  0.0151  0.0663 -0.0873 -0.0053  0.0543 -0.0256  0.0727
 0.0309  0.0445 -0.0107  0.0761  0.0002 -0.0131 -0.0189  0.0972  0.0655 -0.0073
-0.0732 -0.0687  0.0699  0.0060  0.0474  0.0323 -0.0736  0.0787 -0.0208  0.0554
-0.0073 -0.0338  0.0339 -0.0654 -0.0958 -0.0922 -0.0357  0.0601  0.0441  0.0475
 0.0194 -0.0170  0.0322  0.0804 -0.0107  0.0169  0.0526  0.0587  0.0409  0.0628
 0.0234 -0.0733 -0.0462 -0.0161 -0.0545  0.0939 -0.0278  0.0634 -0.0414  0.0313

Columns 61 to 70
-0.0415  0.0500 -0.0383 -0.0935 -0.0604  0.0737 -0.0125  0.0553  0.0816 -0.0675
-0.0750  0.0074  0.0729  0.0541  0.0322 -0.0960 -0.0926  0.0371  0.0089  0.0412
-0.0962  0.0857 -0.0460  0.0903 -0.0378 -0.0216 -0.0959 -0.0508  0.0557 -0.0540
 0.0527 -0.0522 -0.0000  0.0968 -0.0565 -0.0364 -0.0801  0.0633 -0.0932 -0.0339
 0.0862 -0.0137 -0.0678 -0.0438  0.0869  0.0720 -0.0273 -0.0435 -0.0323 -0.0963
-0.0120  0.0180 -0.0824  0.0008  0.0483  0.0867  0.0940  0.0148 -0.0181 -0.0263

Columns 71 to 80
-0.0359 -0.0325 -0.0724 -0.0351  0.0989 -0.0205  0.0473 -0.0495 -0.0205  0.0794
-0.0166 -0.0358 -0.0115 -0.0946 -0.0707  0.0993 -0.0550  0.0477  0.0474  0.0423
-0.0257  0.0288  0.0643  0.0072  0.0250 -0.0691 -0.0145  0.0208 -0.0719 -0.0932
 0.0519 -0.0170 -0.0387 -0.0728 -0.0054  0.0957 -0.0307 -0.0950  0.0982 -0.0054
-0.0255  0.0370 -0.0716 -0.0308  0.0488  0.0099  0.0503  0.0406  0.0780  0.0295
-0.0818 -0.0318  0.0184  0.0924  0.0205 -0.0862 -0.0747 -0.0644 -0.0787  0.0717

Columns 81 to 90
-0.0198 -0.0990  0.0809  0.0121  0.0576 -0.0944  0.0353 -0.0129 -0.0785  0.0083
-0.0905  0.0798 -0.0979  0.0645  0.0960 -0.0435 -0.0063  0.0508 -0.0439 -0.0144
-0.0101 -0.0817 -0.0272  0.0626  0.0205  0.0709  0.0725  0.0489  0.0626  0.0224
-0.0521  0.0732 -0.0559  0.0555 -0.0225 -0.0175  0.0083 -0.0560 -0.0885  0.0315
-0.0150  0.0869 -0.0561  0.0884  0.0352  0.0960  0.0540  0.0166 -0.0875 -0.0695
 0.0837 -0.0937  0.0844  0.0144  0.0281  0.0584 -0.0465  0.0279 -0.0629 -0.0268

Columns 91 to 100
 0.0127  0.0993  0.0660  0.0253 -0.0268 -0.0215  0.0555 -0.0035  0.0148  0.0478
-0.0760  0.0580 -0.0250 -0.0966 -0.0499  0.0421 -0.0698  0.0816  0.0292 -0.0680
-0.0493 -0.0510  0.0342 -0.0610 -0.0536 -0.0473 -0.0132 -0.0727  0.0194 -0.0416
 0.0881  0.0543 -0.0618  0.0645 -0.0285 -0.0801 -0.0508  0.0560  0.0854  0.0319
-0.0230  0.0271 -0.0942  0.0357  0.0834 -0.0546  0.0229 -0.0124  0.0016 -0.0725
-0.0395  0.0338 -0.0035  0.0856  0.0887 -0.0642  0.0670  0.0395  0.0173  0.0213
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:11[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:11[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:11[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 43 [batchSize = 1]
 
2016-10-13 16:47:11[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.308701833089ms 
2016-10-13 16:47:11[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:11[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0546  0.0293  0.0814 -0.0739  0.0172  0.0446 -0.0808  0.0766 -0.0749  0.0714
 0.0750  0.0321 -0.0912 -0.0730  0.0386  0.0188 -0.0359 -0.0660 -0.0901 -0.0795
-0.0726 -0.0813  0.0697 -0.0012  0.0695 -0.0916  0.0308 -0.0530  0.0497  0.0378
-0.0257 -0.0239 -0.0257  0.0133  0.0794  0.0161 -0.0122  0.0577  0.0843  0.0205
 0.0634 -0.0579  0.0453  0.0242 -0.0063 -0.0596 -0.0477  0.0808  0.0416  0.0267
 0.0054 -0.0166 -0.0635 -0.0536  0.0821  0.1012 -0.0770  0.0340  0.0955 -0.0701

Columns 11 to 20
-0.0914 -0.0502 -0.0691  0.0425 -0.0895 -0.0578  0.0118 -0.0372  0.0583  0.0189
-0.0772  0.0522  0.0172  0.0908  0.0387 -0.0273  0.0330 -0.0866 -0.0022  0.0818
 0.0882 -0.0113  0.0335 -0.0717  0.0880  0.0581  0.0255 -0.0842  0.0989 -0.0620
-0.0949 -0.0679  0.0513  0.0003  0.0724  0.0054 -0.0770  0.0351  0.0096  0.0950
-0.0561  0.0328  0.0935 -0.0451 -0.0829 -0.0174 -0.0112  0.0572  0.0858 -0.0225
-0.0057  0.0965  0.1007 -0.0699 -0.0227  0.0225 -0.0248 -0.0135  0.0422  0.0113

Columns 21 to 30
-0.0490  0.0223  0.0450  0.0613 -0.0560  0.0506  0.0903 -0.0811  0.0616  0.0623
-0.0543 -0.0807 -0.0814  0.0059  0.0033 -0.0013  0.0044 -0.0139 -0.0925  0.0784
 0.0021 -0.0302 -0.0955 -0.0496 -0.0059  0.0955 -0.0593 -0.0957  0.0658 -0.0755
 0.0880  0.0332  0.0217 -0.0279 -0.0823  0.0323 -0.0679  0.0585  0.0146 -0.0399
 0.0580  0.0048 -0.0105  0.0347  0.0796 -0.0093 -0.0855 -0.0453 -0.0378  0.0252
 0.0664  0.0226 -0.0958 -0.0749 -0.0165 -0.0189 -0.0393 -0.0701  0.0457 -0.0665

Columns 31 to 40
 0.0118 -0.0045 -0.0678 -0.0172  0.0990 -0.0982  0.0830 -0.0367  0.0177  0.0243
 0.0236 -0.0625  0.0160 -0.0877  0.0629  0.0061  0.0191  0.0057  0.0484  0.0142
 0.0102 -0.0578  0.0207  0.0879 -0.0277 -0.0739 -0.0223  0.0356  0.0534  0.0571
-0.1020 -0.0207  0.0889 -0.0033 -0.0227  0.0641  0.0843 -0.0633  0.0307 -0.0439
-0.0373  0.0577  0.0250 -0.0940  0.0308  0.0991 -0.0161  0.0173  0.0562  0.0097
-0.0035  0.0026 -0.0100 -0.0866  0.0736 -0.0697  0.0369  0.0019  0.0234  0.0418

Columns 41 to 50
-0.0524  0.0050 -0.0874 -0.0512  0.0065 -0.0947  0.0612 -0.0596  0.0633  0.0953
 0.0848 -0.0001  0.0248  0.0195  0.0137  0.0356 -0.0680  0.0660 -0.0940 -0.0396
 0.0743  0.0571  0.0910  0.0884  0.0536  0.0816 -0.0643  0.0888 -0.0716 -0.0060
-0.0895  0.0469  0.0173 -0.0014 -0.0228  0.0636  0.0322 -0.0355 -0.0373 -0.0174
-0.0399 -0.0357  0.0242  0.0539 -0.0047  0.0582 -0.0823  0.0218 -0.0063 -0.1008
-0.0636  0.0762 -0.0601  0.0093 -0.0064 -0.0560 -0.0982  0.0509  0.0790  0.0082

Columns 51 to 60
-0.0974  0.0972  0.0798  0.0151  0.0663 -0.0873 -0.0053  0.0543 -0.0256  0.0727
 0.0309  0.0445 -0.0107  0.0761  0.0002 -0.0131 -0.0189  0.0972  0.0655 -0.0073
-0.0732 -0.0687  0.0700  0.0060  0.0474  0.0322 -0.0736  0.0787 -0.0208  0.0554
-0.0073 -0.0338  0.0340 -0.0654 -0.0957 -0.0922 -0.0358  0.0602  0.0441  0.0475
 0.0194 -0.0170  0.0322  0.0804 -0.0107  0.0169  0.0526  0.0587  0.0409  0.0628
 0.0234 -0.0733 -0.0462 -0.0161 -0.0545  0.0939 -0.0278  0.0634 -0.0414  0.0313

Columns 61 to 70
-0.0415  0.0500 -0.0383 -0.0935 -0.0604  0.0737 -0.0125  0.0553  0.0816 -0.0674
-0.0750  0.0074  0.0729  0.0540  0.0322 -0.0960 -0.0926  0.0371  0.0089  0.0412
-0.0962  0.0857 -0.0460  0.0902 -0.0378 -0.0216 -0.0959 -0.0508  0.0557 -0.0540
 0.0527 -0.0522 -0.0001  0.0968 -0.0565 -0.0364 -0.0801  0.0634 -0.0931 -0.0340
 0.0862 -0.0137 -0.0678 -0.0438  0.0869  0.0720 -0.0273 -0.0435 -0.0323 -0.0963
-0.0120  0.0180 -0.0824  0.0008  0.0483  0.0867  0.0940  0.0148 -0.0181 -0.0263

Columns 71 to 80
-0.0359 -0.0325 -0.0724 -0.0351  0.0989 -0.0205  0.0474 -0.0495 -0.0205  0.0794
-0.0166 -0.0358 -0.0115 -0.0945 -0.0707  0.0993 -0.0550  0.0477  0.0474  0.0423
-0.0257  0.0288  0.0643  0.0072  0.0250 -0.0691 -0.0145  0.0208 -0.0719 -0.0932
 0.0519 -0.0170 -0.0387 -0.0727 -0.0054  0.0958 -0.0308 -0.0950  0.0982 -0.0054
-0.0255  0.0370 -0.0716 -0.0308  0.0488  0.0099  0.0502  0.0406  0.0780  0.0295
-0.0818 -0.0318  0.0184  0.0924  0.0205 -0.0862 -0.0747 -0.0644 -0.0787  0.0717

Columns 81 to 90
-0.0198 -0.0990  0.0808  0.0121  0.0576 -0.0944  0.0353 -0.0129 -0.0785  0.0083
-0.0905  0.0798 -0.0979  0.0645  0.0960 -0.0435 -0.0063  0.0508 -0.0440 -0.0144
-0.0102 -0.0817 -0.0272  0.0627  0.0205  0.0709  0.0725  0.0489  0.0626  0.0224
-0.0521  0.0732 -0.0559  0.0556 -0.0225 -0.0175  0.0083 -0.0560 -0.0885  0.0315
-0.0150  0.0869 -0.0561  0.0884  0.0352  0.0960  0.0540  0.0166 -0.0875 -0.0695
 0.0837 -0.0937  0.0844  0.0144  0.0281  0.0585 -0.0465  0.0279 -0.0629 -0.0268

Columns 91 to 100
 0.0127  0.0993  0.0660  0.0253 -0.0268 -0.0215  0.0555 -0.0035  0.0148  0.0478
-0.0761  0.0580 -0.0250 -0.0966 -0.0499  0.0421 -0.0698  0.0816  0.0292 -0.0680
-0.0493 -0.0510  0.0342 -0.0610 -0.0536 -0.0473 -0.0132 -0.0727  0.0194 -0.0416
 0.0881  0.0543 -0.0618  0.0645 -0.0284 -0.0801 -0.0508  0.0559  0.0853  0.0320
-0.0230  0.0271 -0.0942  0.0357  0.0834 -0.0546  0.0230 -0.0124  0.0016 -0.0725
-0.0395  0.0338 -0.0035  0.0856  0.0887 -0.0642  0.0670  0.0395  0.0173  0.0213
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:11[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:11[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:11[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 44 [batchSize = 1]
 
2016-10-13 16:47:11[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.48362159729ms 
2016-10-13 16:47:11[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:11[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0546  0.0293  0.0814 -0.0738  0.0172  0.0447 -0.0808  0.0767 -0.0749  0.0714
 0.0750  0.0321 -0.0912 -0.0730  0.0386  0.0189 -0.0360 -0.0660 -0.0902 -0.0795
-0.0726 -0.0813  0.0697 -0.0012  0.0695 -0.0915  0.0308 -0.0529  0.0497  0.0378
-0.0257 -0.0241 -0.0257  0.0134  0.0794  0.0162 -0.0122  0.0577  0.0842  0.0206
 0.0634 -0.0579  0.0453  0.0242 -0.0063 -0.0595 -0.0477  0.0808  0.0416  0.0267
 0.0054 -0.0167 -0.0635 -0.0536  0.0821  0.1012 -0.0770  0.0340  0.0955 -0.0700

Columns 11 to 20
-0.0913 -0.0502 -0.0690  0.0425 -0.0895 -0.0578  0.0118 -0.0372  0.0583  0.0189
-0.0772  0.0522  0.0172  0.0907  0.0387 -0.0273  0.0330 -0.0866 -0.0022  0.0817
 0.0882 -0.0113  0.0335 -0.0717  0.0880  0.0581  0.0254 -0.0842  0.0989 -0.0620
-0.0949 -0.0679  0.0514  0.0003  0.0723  0.0054 -0.0771  0.0350  0.0096  0.0949
-0.0561  0.0328  0.0935 -0.0451 -0.0829 -0.0174 -0.0112  0.0571  0.0858 -0.0225
-0.0057  0.0965  0.1007 -0.0699 -0.0227  0.0225 -0.0248 -0.0135  0.0422  0.0113

Columns 21 to 30
-0.0490  0.0223  0.0450  0.0613 -0.0560  0.0506  0.0902 -0.0811  0.0617  0.0623
-0.0543 -0.0807 -0.0814  0.0059  0.0033 -0.0013  0.0044 -0.0139 -0.0925  0.0784
 0.0021 -0.0302 -0.0955 -0.0496 -0.0059  0.0955 -0.0594 -0.0956  0.0658 -0.0756
 0.0880  0.0332  0.0217 -0.0279 -0.0824  0.0323 -0.0680  0.0586  0.0147 -0.0399
 0.0580  0.0048 -0.0105  0.0347  0.0796 -0.0093 -0.0855 -0.0453 -0.0378  0.0252
 0.0664  0.0226 -0.0958 -0.0749 -0.0165 -0.0189 -0.0393 -0.0701  0.0457 -0.0665

Columns 31 to 40
 0.0117 -0.0044 -0.0678 -0.0172  0.0989 -0.0982  0.0830 -0.0367  0.0177  0.0243
 0.0235 -0.0625  0.0160 -0.0877  0.0629  0.0061  0.0191  0.0058  0.0484  0.0142
 0.0102 -0.0578  0.0207  0.0879 -0.0277 -0.0739 -0.0224  0.0356  0.0534  0.0571
-0.1021 -0.0206  0.0889 -0.0033 -0.0227  0.0641  0.0842 -0.0632  0.0307 -0.0439
-0.0373  0.0578  0.0250 -0.0940  0.0308  0.0991 -0.0161  0.0173  0.0562  0.0097
-0.0035  0.0026 -0.0100 -0.0866  0.0736 -0.0697  0.0369  0.0019  0.0234  0.0418

Columns 41 to 50
-0.0524  0.0050 -0.0874 -0.0512  0.0065 -0.0947  0.0611 -0.0596  0.0633  0.0953
 0.0848 -0.0001  0.0248  0.0195  0.0137  0.0356 -0.0680  0.0660 -0.0940 -0.0396
 0.0743  0.0571  0.0910  0.0884  0.0536  0.0816 -0.0643  0.0888 -0.0717 -0.0060
-0.0895  0.0469  0.0173 -0.0014 -0.0228  0.0637  0.0322 -0.0355 -0.0374 -0.0175
-0.0399 -0.0357  0.0242  0.0539 -0.0047  0.0582 -0.0823  0.0218 -0.0063 -0.1009
-0.0636  0.0763 -0.0601  0.0092 -0.0064 -0.0560 -0.0982  0.0509  0.0789  0.0082

Columns 51 to 60
-0.0974  0.0972  0.0798  0.0151  0.0663 -0.0873 -0.0053  0.0543 -0.0256  0.0727
 0.0309  0.0445 -0.0106  0.0761  0.0003 -0.0131 -0.0189  0.0972  0.0656 -0.0073
-0.0732 -0.0687  0.0700  0.0060  0.0474  0.0322 -0.0736  0.0787 -0.0208  0.0553
-0.0073 -0.0337  0.0340 -0.0653 -0.0957 -0.0922 -0.0358  0.0602  0.0442  0.0475
 0.0194 -0.0170  0.0322  0.0804 -0.0107  0.0169  0.0525  0.0587  0.0409  0.0628
 0.0234 -0.0733 -0.0462 -0.0161 -0.0545  0.0939 -0.0278  0.0634 -0.0413  0.0313

Columns 61 to 70
-0.0415  0.0501 -0.0383 -0.0935 -0.0604  0.0737 -0.0125  0.0553  0.0816 -0.0675
-0.0750  0.0074  0.0729  0.0540  0.0322 -0.0960 -0.0926  0.0371  0.0089  0.0412
-0.0962  0.0858 -0.0460  0.0902 -0.0378 -0.0216 -0.0959 -0.0508  0.0557 -0.0541
 0.0528 -0.0520 -0.0001  0.0967 -0.0565 -0.0363 -0.0801  0.0634 -0.0931 -0.0341
 0.0862 -0.0137 -0.0678 -0.0438  0.0869  0.0720 -0.0273 -0.0435 -0.0323 -0.0964
-0.0120  0.0181 -0.0825  0.0007  0.0483  0.0867  0.0940  0.0148 -0.0181 -0.0264

Columns 71 to 80
-0.0359 -0.0325 -0.0723 -0.0350  0.0989 -0.0205  0.0473 -0.0495 -0.0205  0.0794
-0.0166 -0.0358 -0.0115 -0.0945 -0.0708  0.0993 -0.0550  0.0477  0.0474  0.0423
-0.0257  0.0288  0.0643  0.0072  0.0250 -0.0691 -0.0145  0.0208 -0.0718 -0.0932
 0.0519 -0.0169 -0.0386 -0.0727 -0.0055  0.0959 -0.0308 -0.0950  0.0982 -0.0054
-0.0255  0.0371 -0.0716 -0.0308  0.0488  0.0099  0.0502  0.0406  0.0780  0.0295
-0.0818 -0.0318  0.0185  0.0924  0.0205 -0.0862 -0.0747 -0.0644 -0.0787  0.0717

Columns 81 to 90
-0.0198 -0.0990  0.0808  0.0121  0.0576 -0.0944  0.0353 -0.0129 -0.0785  0.0083
-0.0905  0.0798 -0.0979  0.0646  0.0960 -0.0435 -0.0063  0.0508 -0.0440 -0.0144
-0.0102 -0.0817 -0.0272  0.0627  0.0205  0.0709  0.0725  0.0489  0.0625  0.0224
-0.0521  0.0732 -0.0558  0.0557 -0.0225 -0.0175  0.0083 -0.0560 -0.0885  0.0315
-0.0150  0.0869 -0.0561  0.0885  0.0352  0.0960  0.0540  0.0166 -0.0875 -0.0694
 0.0837 -0.0937  0.0844  0.0144  0.0281  0.0585 -0.0465  0.0279 -0.0629 -0.0267

Columns 91 to 100
 0.0126  0.0993  0.0660  0.0253 -0.0267 -0.0215  0.0556 -0.0036  0.0148  0.0479
-0.0761  0.0580 -0.0250 -0.0966 -0.0498  0.0421 -0.0698  0.0816  0.0291 -0.0680
-0.0493 -0.0509  0.0342 -0.0610 -0.0536 -0.0473 -0.0132 -0.0728  0.0194 -0.0415
 0.0880  0.0544 -0.0618  0.0644 -0.0283 -0.0801 -0.0507  0.0559  0.0852  0.0321
-0.0230  0.0271 -0.0942  0.0357  0.0834 -0.0546  0.0230 -0.0124  0.0016 -0.0724
-0.0395  0.0338 -0.0035  0.0855  0.0887 -0.0642  0.0670  0.0395  0.0173  0.0213
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:11[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:11[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:11[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 45 [batchSize = 1]
 
2016-10-13 16:47:11[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.515649159749ms 
2016-10-13 16:47:11[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:11[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0546  0.0293  0.0814 -0.0738  0.0172  0.0446 -0.0808  0.0767 -0.0749  0.0714
 0.0751  0.0321 -0.0912 -0.0729  0.0386  0.0189 -0.0360 -0.0660 -0.0902 -0.0795
-0.0726 -0.0813  0.0697 -0.0012  0.0695 -0.0915  0.0308 -0.0529  0.0497  0.0378
-0.0256 -0.0241 -0.0257  0.0134  0.0794  0.0162 -0.0122  0.0577  0.0842  0.0206
 0.0634 -0.0579  0.0453  0.0242 -0.0063 -0.0595 -0.0477  0.0808  0.0416  0.0267
 0.0054 -0.0167 -0.0635 -0.0536  0.0821  0.1012 -0.0770  0.0340  0.0954 -0.0700

Columns 11 to 20
-0.0913 -0.0502 -0.0690  0.0425 -0.0895 -0.0578  0.0118 -0.0372  0.0583  0.0189
-0.0772  0.0522  0.0172  0.0907  0.0387 -0.0273  0.0330 -0.0866 -0.0022  0.0817
 0.0882 -0.0112  0.0335 -0.0717  0.0880  0.0581  0.0254 -0.0842  0.0989 -0.0620
-0.0949 -0.0679  0.0514  0.0003  0.0723  0.0054 -0.0771  0.0351  0.0096  0.0949
-0.0561  0.0328  0.0935 -0.0452 -0.0829 -0.0174 -0.0112  0.0571  0.0858 -0.0225
-0.0057  0.0965  0.1007 -0.0699 -0.0227  0.0226 -0.0248 -0.0135  0.0422  0.0113

Columns 21 to 30
-0.0490  0.0223  0.0450  0.0613 -0.0560  0.0506  0.0902 -0.0811  0.0617  0.0623
-0.0543 -0.0806 -0.0814  0.0059  0.0033 -0.0013  0.0043 -0.0138 -0.0925  0.0784
 0.0021 -0.0302 -0.0955 -0.0495 -0.0059  0.0955 -0.0594 -0.0956  0.0658 -0.0756
 0.0880  0.0332  0.0217 -0.0279 -0.0824  0.0323 -0.0681  0.0586  0.0147 -0.0399
 0.0580  0.0048 -0.0105  0.0347  0.0796 -0.0093 -0.0855 -0.0453 -0.0378  0.0252
 0.0664  0.0226 -0.0958 -0.0749 -0.0165 -0.0189 -0.0393 -0.0701  0.0457 -0.0665

Columns 31 to 40
 0.0117 -0.0045 -0.0678 -0.0172  0.0989 -0.0982  0.0830 -0.0367  0.0177  0.0243
 0.0235 -0.0625  0.0160 -0.0877  0.0629  0.0061  0.0191  0.0058  0.0484  0.0142
 0.0102 -0.0578  0.0207  0.0878 -0.0277 -0.0739 -0.0224  0.0356  0.0533  0.0571
-0.1021 -0.0206  0.0889 -0.0034 -0.0227  0.0641  0.0842 -0.0632  0.0307 -0.0438
-0.0373  0.0578  0.0250 -0.0940  0.0308  0.0992 -0.0161  0.0173  0.0562  0.0098
-0.0036  0.0026 -0.0100 -0.0866  0.0736 -0.0697  0.0368  0.0019  0.0234  0.0419

Columns 41 to 50
-0.0524  0.0050 -0.0874 -0.0512  0.0065 -0.0947  0.0611 -0.0596  0.0633  0.0953
 0.0848 -0.0001  0.0249  0.0194  0.0137  0.0356 -0.0680  0.0660 -0.0940 -0.0396
 0.0743  0.0571  0.0910  0.0884  0.0536  0.0816 -0.0644  0.0888 -0.0717 -0.0060
-0.0895  0.0469  0.0173 -0.0014 -0.0228  0.0637  0.0321 -0.0355 -0.0374 -0.0175
-0.0399 -0.0356  0.0243  0.0538 -0.0047  0.0582 -0.0823  0.0218 -0.0063 -0.1009
-0.0636  0.0763 -0.0601  0.0092 -0.0064 -0.0560 -0.0982  0.0509  0.0789  0.0081

Columns 51 to 60
-0.0974  0.0972  0.0798  0.0151  0.0663 -0.0873 -0.0053  0.0542 -0.0256  0.0727
 0.0309  0.0445 -0.0106  0.0762  0.0003 -0.0131 -0.0190  0.0972  0.0656 -0.0073
-0.0732 -0.0687  0.0700  0.0060  0.0475  0.0322 -0.0736  0.0787 -0.0207  0.0553
-0.0072 -0.0337  0.0340 -0.0652 -0.0957 -0.0922 -0.0358  0.0602  0.0442  0.0475
 0.0194 -0.0170  0.0323  0.0804 -0.0107  0.0169  0.0525  0.0587  0.0409  0.0628
 0.0234 -0.0733 -0.0462 -0.0161 -0.0545  0.0939 -0.0278  0.0634 -0.0413  0.0313

Columns 61 to 70
-0.0415  0.0500 -0.0383 -0.0935 -0.0604  0.0737 -0.0125  0.0553  0.0816 -0.0674
-0.0750  0.0075  0.0729  0.0540  0.0322 -0.0960 -0.0926  0.0371  0.0090  0.0411
-0.0962  0.0858 -0.0461  0.0902 -0.0378 -0.0216 -0.0959 -0.0508  0.0557 -0.0541
 0.0528 -0.0521 -0.0001  0.0967 -0.0565 -0.0363 -0.0801  0.0634 -0.0931 -0.0341
 0.0862 -0.0136 -0.0678 -0.0438  0.0869  0.0720 -0.0273 -0.0435 -0.0323 -0.0964
-0.0120  0.0181 -0.0825  0.0007  0.0483  0.0867  0.0940  0.0148 -0.0181 -0.0264

Columns 71 to 80
-0.0359 -0.0325 -0.0723 -0.0350  0.0989 -0.0205  0.0474 -0.0495 -0.0205  0.0794
-0.0166 -0.0357 -0.0115 -0.0945 -0.0708  0.0993 -0.0550  0.0477  0.0474  0.0423
-0.0257  0.0288  0.0643  0.0073  0.0250 -0.0690 -0.0146  0.0209 -0.0718 -0.0932
 0.0519 -0.0169 -0.0386 -0.0727 -0.0055  0.0958 -0.0308 -0.0950  0.0982 -0.0054
-0.0255  0.0371 -0.0716 -0.0307  0.0488  0.0100  0.0502  0.0406  0.0780  0.0295
-0.0818 -0.0318  0.0185  0.0924  0.0205 -0.0862 -0.0747 -0.0644 -0.0787  0.0717

Columns 81 to 90
-0.0198 -0.0990  0.0808  0.0121  0.0576 -0.0944  0.0353 -0.0129 -0.0785  0.0083
-0.0905  0.0798 -0.0979  0.0646  0.0960 -0.0435 -0.0063  0.0508 -0.0440 -0.0144
-0.0102 -0.0817 -0.0271  0.0627  0.0205  0.0709  0.0725  0.0489  0.0625  0.0224
-0.0522  0.0732 -0.0558  0.0557 -0.0225 -0.0175  0.0083 -0.0560 -0.0885  0.0315
-0.0150  0.0869 -0.0561  0.0885  0.0352  0.0960  0.0540  0.0166 -0.0875 -0.0694
 0.0837 -0.0937  0.0844  0.0144  0.0281  0.0585 -0.0465  0.0279 -0.0629 -0.0267

Columns 91 to 100
 0.0126  0.0993  0.0660  0.0253 -0.0267 -0.0215  0.0555 -0.0035  0.0148  0.0478
-0.0761  0.0580 -0.0250 -0.0967 -0.0498  0.0421 -0.0697  0.0816  0.0291 -0.0679
-0.0493 -0.0509  0.0342 -0.0610 -0.0536 -0.0473 -0.0132 -0.0728  0.0194 -0.0415
 0.0880  0.0544 -0.0618  0.0644 -0.0283 -0.0801 -0.0507  0.0559  0.0852  0.0321
-0.0230  0.0271 -0.0942  0.0357  0.0834 -0.0546  0.0230 -0.0124  0.0015 -0.0724
-0.0395  0.0338 -0.0035  0.0855  0.0887 -0.0642  0.0670  0.0395  0.0173  0.0213
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:11[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:11[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:11[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 46 [batchSize = 1]
 
2016-10-13 16:47:12[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.48370107015ms 
2016-10-13 16:47:12[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:12[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0546  0.0293  0.0814 -0.0738  0.0173  0.0446 -0.0809  0.0767 -0.0749  0.0714
 0.0751  0.0320 -0.0912 -0.0729  0.0386  0.0190 -0.0360 -0.0660 -0.0902 -0.0795
-0.0725 -0.0813  0.0697 -0.0011  0.0695 -0.0914  0.0308 -0.0529  0.0496  0.0378
-0.0256 -0.0241 -0.0257  0.0136  0.0794  0.0162 -0.0123  0.0578  0.0841  0.0206
 0.0634 -0.0579  0.0453  0.0243 -0.0063 -0.0595 -0.0477  0.0809  0.0416  0.0267
 0.0054 -0.0167 -0.0635 -0.0536  0.0821  0.1013 -0.0770  0.0340  0.0954 -0.0700

Columns 11 to 20
-0.0913 -0.0502 -0.0690  0.0425 -0.0895 -0.0578  0.0118 -0.0372  0.0583  0.0189
-0.0772  0.0522  0.0172  0.0907  0.0387 -0.0272  0.0330 -0.0867 -0.0022  0.0817
 0.0882 -0.0112  0.0335 -0.0717  0.0879  0.0581  0.0254 -0.0842  0.0989 -0.0620
-0.0949 -0.0678  0.0514  0.0003  0.0723  0.0055 -0.0771  0.0350  0.0096  0.0949
-0.0561  0.0328  0.0935 -0.0452 -0.0829 -0.0174 -0.0112  0.0571  0.0858 -0.0225
-0.0057  0.0965  0.1007 -0.0699 -0.0227  0.0226 -0.0248 -0.0135  0.0422  0.0113

Columns 21 to 30
-0.0490  0.0223  0.0450  0.0614 -0.0561  0.0506  0.0902 -0.0811  0.0617  0.0623
-0.0543 -0.0806 -0.0814  0.0059  0.0033 -0.0013  0.0043 -0.0138 -0.0925  0.0783
 0.0021 -0.0301 -0.0955 -0.0495 -0.0059  0.0955 -0.0594 -0.0956  0.0658 -0.0756
 0.0879  0.0332  0.0217 -0.0278 -0.0824  0.0323 -0.0682  0.0587  0.0146 -0.0400
 0.0579  0.0048 -0.0105  0.0347  0.0796 -0.0093 -0.0856 -0.0453 -0.0378  0.0252
 0.0664  0.0226 -0.0958 -0.0749 -0.0165 -0.0189 -0.0394 -0.0700  0.0457 -0.0666

Columns 31 to 40
 0.0117 -0.0044 -0.0678 -0.0172  0.0989 -0.0982  0.0830 -0.0367  0.0177  0.0243
 0.0235 -0.0624  0.0160 -0.0877  0.0629  0.0061  0.0190  0.0058  0.0484  0.0142
 0.0102 -0.0577  0.0207  0.0878 -0.0277 -0.0738 -0.0224  0.0356  0.0533  0.0572
-0.1021 -0.0205  0.0889 -0.0034 -0.0227  0.0641  0.0842 -0.0631  0.0307 -0.0438
-0.0373  0.0578  0.0250 -0.0940  0.0308  0.0992 -0.0162  0.0174  0.0562  0.0098
-0.0036  0.0026 -0.0100 -0.0866  0.0736 -0.0697  0.0368  0.0019  0.0234  0.0419

Columns 41 to 50
-0.0524  0.0050 -0.0874 -0.0512  0.0065 -0.0947  0.0611 -0.0596  0.0633  0.0952
 0.0848 -0.0001  0.0249  0.0194  0.0137  0.0356 -0.0680  0.0660 -0.0940 -0.0397
 0.0743  0.0571  0.0910  0.0883  0.0536  0.0816 -0.0644  0.0888 -0.0717 -0.0061
-0.0894  0.0469  0.0174 -0.0015 -0.0228  0.0637  0.0321 -0.0354 -0.0374 -0.0176
-0.0399 -0.0356  0.0243  0.0538 -0.0047  0.0582 -0.0823  0.0218 -0.0063 -0.1009
-0.0636  0.0763 -0.0600  0.0092 -0.0064 -0.0559 -0.0982  0.0509  0.0789  0.0081

Columns 51 to 60
-0.0974  0.0972  0.0798  0.0151  0.0663 -0.0873 -0.0053  0.0543 -0.0255  0.0727
 0.0309  0.0445 -0.0106  0.0762  0.0003 -0.0131 -0.0190  0.0972  0.0656 -0.0073
-0.0732 -0.0687  0.0700  0.0061  0.0475  0.0322 -0.0737  0.0787 -0.0207  0.0553
-0.0072 -0.0337  0.0341 -0.0652 -0.0956 -0.0922 -0.0359  0.0602  0.0443  0.0475
 0.0194 -0.0170  0.0323  0.0804 -0.0107  0.0169  0.0525  0.0587  0.0410  0.0628
 0.0234 -0.0733 -0.0462 -0.0161 -0.0545  0.0939 -0.0278  0.0634 -0.0413  0.0313

Columns 61 to 70
-0.0415  0.0501 -0.0383 -0.0935 -0.0604  0.0738 -0.0125  0.0553  0.0816 -0.0674
-0.0750  0.0075  0.0729  0.0540  0.0322 -0.0960 -0.0926  0.0371  0.0090  0.0411
-0.0961  0.0858 -0.0461  0.0902 -0.0378 -0.0216 -0.0959 -0.0508  0.0557 -0.0541
 0.0528 -0.0520 -0.0001  0.0967 -0.0565 -0.0363 -0.0801  0.0634 -0.0930 -0.0341
 0.0863 -0.0136 -0.0678 -0.0439  0.0869  0.0721 -0.0273 -0.0434 -0.0323 -0.0964
-0.0120  0.0181 -0.0825  0.0007  0.0483  0.0867  0.0940  0.0148 -0.0181 -0.0264

Columns 71 to 80
-0.0359 -0.0325 -0.0723 -0.0350  0.0989 -0.0205  0.0473 -0.0495 -0.0205  0.0794
-0.0166 -0.0357 -0.0115 -0.0945 -0.0708  0.0994 -0.0550  0.0477  0.0474  0.0423
-0.0258  0.0289  0.0644  0.0073  0.0249 -0.0690 -0.0146  0.0209 -0.0718 -0.0932
 0.0518 -0.0169 -0.0386 -0.0726 -0.0055  0.0959 -0.0308 -0.0950  0.0982 -0.0054
-0.0256  0.0371 -0.0716 -0.0307  0.0488  0.0100  0.0502  0.0406  0.0781  0.0295
-0.0818 -0.0317  0.0185  0.0924  0.0205 -0.0862 -0.0747 -0.0644 -0.0787  0.0717

Columns 81 to 90
-0.0198 -0.0990  0.0809  0.0121  0.0576 -0.0944  0.0353 -0.0129 -0.0785  0.0083
-0.0905  0.0798 -0.0978  0.0646  0.0960 -0.0435 -0.0064  0.0508 -0.0440 -0.0144
-0.0102 -0.0817 -0.0271  0.0628  0.0204  0.0709  0.0725  0.0489  0.0625  0.0224
-0.0522  0.0732 -0.0558  0.0558 -0.0225 -0.0175  0.0083 -0.0561 -0.0885  0.0315
-0.0150  0.0869 -0.0561  0.0885  0.0352  0.0960  0.0540  0.0166 -0.0875 -0.0694
 0.0837 -0.0937  0.0844  0.0144  0.0281  0.0585 -0.0465  0.0279 -0.0629 -0.0267

Columns 91 to 100
 0.0126  0.0994  0.0660  0.0253 -0.0267 -0.0215  0.0556 -0.0036  0.0148  0.0478
-0.0761  0.0581 -0.0250 -0.0967 -0.0498  0.0421 -0.0697  0.0815  0.0291 -0.0679
-0.0493 -0.0509  0.0342 -0.0611 -0.0536 -0.0473 -0.0132 -0.0728  0.0193 -0.0415
 0.0880  0.0544 -0.0618  0.0644 -0.0283 -0.0801 -0.0506  0.0559  0.0851  0.0321
-0.0230  0.0271 -0.0942  0.0357  0.0834 -0.0546  0.0230 -0.0124  0.0015 -0.0724
-0.0395  0.0338 -0.0035  0.0855  0.0888 -0.0642  0.0671  0.0395  0.0173  0.0213
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:12[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:12[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:12[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 47 [batchSize = 1]
 
2016-10-13 16:47:12[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.378320058187ms 
2016-10-13 16:47:12[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:12[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0546  0.0293  0.0814 -0.0738  0.0173  0.0447 -0.0809  0.0767 -0.0749  0.0714
 0.0751  0.0320 -0.0912 -0.0729  0.0386  0.0190 -0.0360 -0.0659 -0.0902 -0.0795
-0.0725 -0.0814  0.0697 -0.0011  0.0695 -0.0914  0.0308 -0.0529  0.0496  0.0378
-0.0255 -0.0241 -0.0257  0.0136  0.0794  0.0163 -0.0123  0.0579  0.0840  0.0206
 0.0635 -0.0579  0.0453  0.0243 -0.0063 -0.0594 -0.0477  0.0809  0.0415  0.0267
 0.0054 -0.0167 -0.0635 -0.0536  0.0821  0.1013 -0.0770  0.0340  0.0954 -0.0700

Columns 11 to 20
-0.0913 -0.0502 -0.0690  0.0425 -0.0895 -0.0578  0.0118 -0.0372  0.0583  0.0189
-0.0772  0.0522  0.0172  0.0907  0.0387 -0.0272  0.0330 -0.0867 -0.0022  0.0817
 0.0882 -0.0112  0.0336 -0.0717  0.0879  0.0581  0.0254 -0.0842  0.0989 -0.0621
-0.0949 -0.0678  0.0515  0.0002  0.0723  0.0055 -0.0771  0.0349  0.0096  0.0948
-0.0561  0.0328  0.0935 -0.0452 -0.0829 -0.0174 -0.0112  0.0571  0.0858 -0.0225
-0.0057  0.0965  0.1007 -0.0699 -0.0227  0.0226 -0.0248 -0.0135  0.0422  0.0113

Columns 21 to 30
-0.0490  0.0223  0.0450  0.0614 -0.0561  0.0506  0.0902 -0.0811  0.0617  0.0623
-0.0543 -0.0806 -0.0814  0.0059  0.0033 -0.0013  0.0043 -0.0138 -0.0925  0.0783
 0.0021 -0.0301 -0.0955 -0.0495 -0.0059  0.0955 -0.0594 -0.0956  0.0658 -0.0756
 0.0879  0.0332  0.0217 -0.0278 -0.0824  0.0322 -0.0682  0.0588  0.0146 -0.0400
 0.0579  0.0048 -0.0105  0.0347  0.0796 -0.0093 -0.0856 -0.0453 -0.0378  0.0252
 0.0664  0.0226 -0.0958 -0.0749 -0.0165 -0.0189 -0.0394 -0.0700  0.0457 -0.0666

Columns 31 to 40
 0.0117 -0.0044 -0.0678 -0.0172  0.0989 -0.0982  0.0830 -0.0367  0.0177  0.0243
 0.0235 -0.0624  0.0160 -0.0877  0.0629  0.0061  0.0190  0.0058  0.0483  0.0142
 0.0101 -0.0577  0.0207  0.0878 -0.0277 -0.0738 -0.0224  0.0357  0.0533  0.0572
-0.1022 -0.0204  0.0888 -0.0034 -0.0227  0.0641  0.0841 -0.0631  0.0306 -0.0438
-0.0373  0.0578  0.0250 -0.0940  0.0308  0.0992 -0.0162  0.0174  0.0562  0.0098
-0.0036  0.0026 -0.0100 -0.0866  0.0736 -0.0697  0.0368  0.0019  0.0234  0.0419

Columns 41 to 50
-0.0524  0.0050 -0.0873 -0.0512  0.0065 -0.0947  0.0611 -0.0596  0.0633  0.0952
 0.0848 -0.0001  0.0249  0.0194  0.0137  0.0356 -0.0680  0.0660 -0.0941 -0.0397
 0.0744  0.0571  0.0910  0.0883  0.0536  0.0816 -0.0644  0.0888 -0.0717 -0.0061
-0.0893  0.0469  0.0175 -0.0016 -0.0228  0.0637  0.0321 -0.0354 -0.0375 -0.0177
-0.0399 -0.0356  0.0243  0.0538 -0.0047  0.0582 -0.0823  0.0218 -0.0064 -0.1010
-0.0636  0.0763 -0.0600  0.0092 -0.0064 -0.0559 -0.0982  0.0509  0.0789  0.0081

Columns 51 to 60
-0.0974  0.0972  0.0798  0.0151  0.0663 -0.0873 -0.0053  0.0543 -0.0255  0.0727
 0.0309  0.0445 -0.0106  0.0762  0.0003 -0.0131 -0.0190  0.0972  0.0656 -0.0073
-0.0732 -0.0687  0.0701  0.0061  0.0475  0.0322 -0.0737  0.0787 -0.0207  0.0554
-0.0072 -0.0337  0.0342 -0.0651 -0.0956 -0.0922 -0.0359  0.0603  0.0443  0.0475
 0.0194 -0.0169  0.0323  0.0805 -0.0106  0.0169  0.0525  0.0587  0.0410  0.0628
 0.0234 -0.0733 -0.0462 -0.0160 -0.0545  0.0939 -0.0278  0.0634 -0.0413  0.0313

Columns 61 to 70
-0.0415  0.0501 -0.0383 -0.0935 -0.0604  0.0738 -0.0125  0.0553  0.0816 -0.0674
-0.0749  0.0075  0.0729  0.0540  0.0322 -0.0960 -0.0926  0.0371  0.0090  0.0411
-0.0961  0.0858 -0.0461  0.0901 -0.0378 -0.0216 -0.0959 -0.0508  0.0558 -0.0541
 0.0529 -0.0519 -0.0001  0.0966 -0.0566 -0.0363 -0.0801  0.0634 -0.0929 -0.0341
 0.0863 -0.0136 -0.0678 -0.0439  0.0869  0.0721 -0.0273 -0.0434 -0.0322 -0.0964
-0.0120  0.0181 -0.0825  0.0007  0.0483  0.0868  0.0940  0.0148 -0.0181 -0.0264

Columns 71 to 80
-0.0359 -0.0325 -0.0723 -0.0350  0.0988 -0.0205  0.0473 -0.0495 -0.0205  0.0794
-0.0166 -0.0357 -0.0115 -0.0945 -0.0708  0.0994 -0.0551  0.0477  0.0474  0.0423
-0.0258  0.0289  0.0644  0.0073  0.0249 -0.0690 -0.0146  0.0209 -0.0718 -0.0932
 0.0518 -0.0169 -0.0386 -0.0726 -0.0056  0.0960 -0.0309 -0.0950  0.0983 -0.0055
-0.0256  0.0371 -0.0716 -0.0307  0.0488  0.0100  0.0502  0.0406  0.0781  0.0295
-0.0818 -0.0317  0.0185  0.0924  0.0205 -0.0862 -0.0748 -0.0644 -0.0787  0.0717

Columns 81 to 90
-0.0198 -0.0990  0.0809  0.0121  0.0576 -0.0944  0.0353 -0.0129 -0.0785  0.0083
-0.0905  0.0798 -0.0978  0.0647  0.0960 -0.0434 -0.0064  0.0508 -0.0440 -0.0144
-0.0102 -0.0817 -0.0271  0.0628  0.0204  0.0709  0.0725  0.0489  0.0625  0.0224
-0.0522  0.0732 -0.0558  0.0558 -0.0226 -0.0175  0.0083 -0.0561 -0.0886  0.0315
-0.0150  0.0869 -0.0561  0.0886  0.0352  0.0960  0.0540  0.0166 -0.0875 -0.0694
 0.0837 -0.0937  0.0844  0.0144  0.0281  0.0585 -0.0465  0.0279 -0.0629 -0.0267

Columns 91 to 100
 0.0126  0.0994  0.0660  0.0252 -0.0267 -0.0215  0.0556 -0.0036  0.0148  0.0478
-0.0761  0.0581 -0.0250 -0.0967 -0.0498  0.0421 -0.0697  0.0815  0.0291 -0.0679
-0.0493 -0.0509  0.0342 -0.0611 -0.0536 -0.0473 -0.0131 -0.0728  0.0193 -0.0415
 0.0880  0.0545 -0.0619  0.0643 -0.0283 -0.0801 -0.0506  0.0558  0.0850  0.0321
-0.0230  0.0272 -0.0942  0.0356  0.0835 -0.0546  0.0230 -0.0124  0.0015 -0.0724
-0.0395  0.0338 -0.0035  0.0855  0.0888 -0.0642  0.0671  0.0395  0.0173  0.0213
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:12[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:12[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:12[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 48 [batchSize = 1]
 
2016-10-13 16:47:12[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 92.900991439819ms 
2016-10-13 16:47:12[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:12[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0546  0.0293  0.0814 -0.0737  0.0173  0.0447 -0.0809  0.0767 -0.0750  0.0714
 0.0751  0.0320 -0.0912 -0.0729  0.0386  0.0190 -0.0360 -0.0659 -0.0903 -0.0795
-0.0725 -0.0814  0.0697 -0.0011  0.0695 -0.0914  0.0308 -0.0528  0.0496  0.0378
-0.0254 -0.0242 -0.0257  0.0137  0.0794  0.0165 -0.0123  0.0580  0.0839  0.0206
 0.0635 -0.0580  0.0453  0.0243 -0.0063 -0.0594 -0.0477  0.0809  0.0415  0.0267
 0.0055 -0.0167 -0.0635 -0.0535  0.0821  0.1013 -0.0770  0.0340  0.0954 -0.0700

Columns 11 to 20
-0.0913 -0.0502 -0.0690  0.0425 -0.0895 -0.0578  0.0118 -0.0373  0.0583  0.0189
-0.0772  0.0522  0.0173  0.0907  0.0387 -0.0272  0.0330 -0.0867 -0.0022  0.0817
 0.0883 -0.0112  0.0336 -0.0717  0.0879  0.0581  0.0254 -0.0842  0.0989 -0.0621
-0.0948 -0.0678  0.0516  0.0002  0.0723  0.0055 -0.0772  0.0349  0.0096  0.0947
-0.0561  0.0328  0.0936 -0.0452 -0.0829 -0.0174 -0.0112  0.0571  0.0858 -0.0226
-0.0057  0.0965  0.1007 -0.0699 -0.0227  0.0226 -0.0248 -0.0135  0.0422  0.0113

Columns 21 to 30
-0.0490  0.0223  0.0450  0.0614 -0.0561  0.0506  0.0902 -0.0810  0.0616  0.0623
-0.0543 -0.0806 -0.0814  0.0059  0.0033 -0.0013  0.0043 -0.0138 -0.0925  0.0783
 0.0021 -0.0301 -0.0955 -0.0495 -0.0060  0.0954 -0.0595 -0.0955  0.0658 -0.0756
 0.0878  0.0332  0.0218 -0.0277 -0.0824  0.0322 -0.0683  0.0589  0.0146 -0.0402
 0.0579  0.0049 -0.0105  0.0347  0.0796 -0.0093 -0.0856 -0.0452 -0.0378  0.0251
 0.0664  0.0226 -0.0958 -0.0749 -0.0165 -0.0189 -0.0394 -0.0700  0.0457 -0.0666

Columns 31 to 40
 0.0117 -0.0044 -0.0678 -0.0173  0.0989 -0.0982  0.0829 -0.0367  0.0177  0.0243
 0.0234 -0.0624  0.0159 -0.0877  0.0629  0.0061  0.0190  0.0058  0.0483  0.0143
 0.0101 -0.0577  0.0206  0.0878 -0.0277 -0.0738 -0.0225  0.0357  0.0533  0.0572
-0.1023 -0.0203  0.0888 -0.0035 -0.0228  0.0642  0.0840 -0.0630  0.0306 -0.0438
-0.0374  0.0578  0.0250 -0.0940  0.0308  0.0992 -0.0162  0.0174  0.0562  0.0098
-0.0036  0.0027 -0.0100 -0.0866  0.0735 -0.0697  0.0368  0.0020  0.0234  0.0419

Columns 41 to 50
-0.0524  0.0050 -0.0873 -0.0512  0.0065 -0.0947  0.0611 -0.0596  0.0633  0.0952
 0.0849 -0.0001  0.0249  0.0194  0.0137  0.0356 -0.0680  0.0660 -0.0941 -0.0398
 0.0744  0.0571  0.0910  0.0883  0.0536  0.0816 -0.0644  0.0889 -0.0717 -0.0062
-0.0893  0.0469  0.0175 -0.0015 -0.0228  0.0637  0.0320 -0.0354 -0.0376 -0.0179
-0.0398 -0.0356  0.0243  0.0538 -0.0047  0.0582 -0.0823  0.0218 -0.0064 -0.1010
-0.0636  0.0763 -0.0600  0.0092 -0.0064 -0.0559 -0.0982  0.0509  0.0789  0.0081

Columns 51 to 60
-0.0974  0.0972  0.0799  0.0151  0.0664 -0.0873 -0.0054  0.0543 -0.0255  0.0727
 0.0309  0.0445 -0.0105  0.0762  0.0004 -0.0131 -0.0190  0.0973  0.0656 -0.0073
-0.0732 -0.0687  0.0701  0.0061  0.0475  0.0322 -0.0737  0.0788 -0.0207  0.0554
-0.0072 -0.0337  0.0344 -0.0651 -0.0954 -0.0922 -0.0359  0.0604  0.0443  0.0475
 0.0194 -0.0169  0.0323  0.0805 -0.0106  0.0169  0.0525  0.0588  0.0410  0.0628
 0.0234 -0.0733 -0.0461 -0.0160 -0.0544  0.0939 -0.0278  0.0634 -0.0413  0.0313

Columns 61 to 70
-0.0415  0.0501 -0.0383 -0.0936 -0.0604  0.0738 -0.0125  0.0553  0.0816 -0.0675
-0.0749  0.0076  0.0729  0.0539  0.0322 -0.0960 -0.0926  0.0371  0.0090  0.0411
-0.0961  0.0859 -0.0461  0.0901 -0.0378 -0.0215 -0.0959 -0.0508  0.0558 -0.0541
 0.0530 -0.0519 -0.0002  0.0965 -0.0566 -0.0363 -0.0801  0.0634 -0.0928 -0.0342
 0.0863 -0.0136 -0.0678 -0.0439  0.0869  0.0721 -0.0273 -0.0434 -0.0322 -0.0964
-0.0119  0.0181 -0.0825  0.0007  0.0483  0.0868  0.0940  0.0148 -0.0180 -0.0264

Columns 71 to 80
-0.0359 -0.0325 -0.0723 -0.0350  0.0988 -0.0204  0.0473 -0.0495 -0.0205  0.0794
-0.0166 -0.0357 -0.0114 -0.0945 -0.0708  0.0994 -0.0551  0.0477  0.0474  0.0423
-0.0258  0.0289  0.0644  0.0073  0.0249 -0.0689 -0.0146  0.0209 -0.0718 -0.0932
 0.0517 -0.0168 -0.0386 -0.0725 -0.0056  0.0961 -0.0310 -0.0950  0.0983 -0.0055
-0.0256  0.0371 -0.0716 -0.0307  0.0488  0.0100  0.0501  0.0406  0.0781  0.0295
-0.0818 -0.0317  0.0185  0.0925  0.0205 -0.0861 -0.0748 -0.0643 -0.0786  0.0717

Columns 81 to 90
-0.0199 -0.0990  0.0809  0.0122  0.0576 -0.0944  0.0353 -0.0129 -0.0785  0.0083
-0.0905  0.0798 -0.0978  0.0647  0.0960 -0.0434 -0.0064  0.0508 -0.0440 -0.0144
-0.0102 -0.0817 -0.0271  0.0628  0.0204  0.0709  0.0725  0.0489  0.0625  0.0224
-0.0523  0.0732 -0.0557  0.0559 -0.0226 -0.0175  0.0083 -0.0561 -0.0886  0.0315
-0.0150  0.0869 -0.0561  0.0886  0.0351  0.0960  0.0540  0.0166 -0.0875 -0.0694
 0.0837 -0.0937  0.0844  0.0145  0.0281  0.0585 -0.0465  0.0279 -0.0629 -0.0267

Columns 91 to 100
 0.0126  0.0994  0.0660  0.0252 -0.0267 -0.0215  0.0556 -0.0036  0.0147  0.0479
-0.0761  0.0581 -0.0250 -0.0967 -0.0498  0.0421 -0.0697  0.0815  0.0290 -0.0679
-0.0493 -0.0509  0.0342 -0.0611 -0.0536 -0.0473 -0.0131 -0.0728  0.0193 -0.0415
 0.0879  0.0545 -0.0619  0.0642 -0.0282 -0.0801 -0.0505  0.0558  0.0849  0.0322
-0.0230  0.0272 -0.0942  0.0356  0.0835 -0.0546  0.0230 -0.0125  0.0015 -0.0724
-0.0395  0.0338 -0.0035  0.0855  0.0888 -0.0642  0.0671  0.0395  0.0173  0.0213
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:12[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:12[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:12[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 49 [batchSize = 1]
 
2016-10-13 16:47:12[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 62.328974405924ms 
2016-10-13 16:47:12[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:12[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0546  0.0293  0.0814 -0.0737  0.0173  0.0447 -0.0809  0.0767 -0.0750  0.0714
 0.0751  0.0320 -0.0912 -0.0729  0.0386  0.0191 -0.0360 -0.0659 -0.0903 -0.0795
-0.0725 -0.0814  0.0697 -0.0011  0.0695 -0.0913  0.0307 -0.0528  0.0496  0.0378
-0.0254 -0.0242 -0.0257  0.0137  0.0795  0.0165 -0.0123  0.0579  0.0839  0.0206
 0.0635 -0.0580  0.0453  0.0243 -0.0063 -0.0593 -0.0477  0.0809  0.0415  0.0267
 0.0055 -0.0167 -0.0635 -0.0535  0.0821  0.1013 -0.0770  0.0341  0.0954 -0.0700

Columns 11 to 20
-0.0913 -0.0502 -0.0690  0.0425 -0.0895 -0.0578  0.0118 -0.0372  0.0583  0.0189
-0.0772  0.0523  0.0173  0.0907  0.0387 -0.0272  0.0330 -0.0867 -0.0022  0.0816
 0.0883 -0.0112  0.0336 -0.0718  0.0879  0.0581  0.0254 -0.0843  0.0989 -0.0621
-0.0948 -0.0677  0.0516  0.0002  0.0723  0.0055 -0.0772  0.0349  0.0097  0.0948
-0.0560  0.0328  0.0936 -0.0452 -0.0829 -0.0174 -0.0112  0.0571  0.0858 -0.0226
-0.0057  0.0965  0.1007 -0.0699 -0.0227  0.0226 -0.0248 -0.0135  0.0422  0.0112

Columns 21 to 30
-0.0490  0.0223  0.0450  0.0614 -0.0561  0.0506  0.0902 -0.0810  0.0616  0.0623
-0.0543 -0.0806 -0.0814  0.0060  0.0033 -0.0013  0.0042 -0.0137 -0.0925  0.0783
 0.0021 -0.0301 -0.0955 -0.0495 -0.0060  0.0954 -0.0595 -0.0955  0.0658 -0.0757
 0.0878  0.0332  0.0218 -0.0277 -0.0824  0.0322 -0.0683  0.0588  0.0146 -0.0402
 0.0579  0.0049 -0.0105  0.0348  0.0796 -0.0094 -0.0856 -0.0452 -0.0378  0.0251
 0.0663  0.0226 -0.0958 -0.0749 -0.0165 -0.0189 -0.0394 -0.0700  0.0457 -0.0666

Columns 31 to 40
 0.0117 -0.0044 -0.0678 -0.0173  0.0989 -0.0982  0.0829 -0.0367  0.0177  0.0243
 0.0234 -0.0624  0.0159 -0.0877  0.0629  0.0061  0.0189  0.0059  0.0483  0.0143
 0.0101 -0.0577  0.0206  0.0878 -0.0277 -0.0738 -0.0225  0.0357  0.0533  0.0572
-0.1023 -0.0203  0.0887 -0.0035 -0.0228  0.0642  0.0840 -0.0630  0.0306 -0.0438
-0.0374  0.0579  0.0249 -0.0940  0.0308  0.0992 -0.0163  0.0174  0.0562  0.0098
-0.0036  0.0027 -0.0101 -0.0866  0.0735 -0.0697  0.0367  0.0020  0.0234  0.0419

Columns 41 to 50
-0.0524  0.0050 -0.0873 -0.0512  0.0065 -0.0947  0.0611 -0.0596  0.0633  0.0952
 0.0849 -0.0001  0.0249  0.0193  0.0137  0.0356 -0.0681  0.0660 -0.0941 -0.0398
 0.0744  0.0571  0.0911  0.0883  0.0536  0.0816 -0.0644  0.0889 -0.0718 -0.0062
-0.0893  0.0469  0.0176 -0.0015 -0.0228  0.0637  0.0320 -0.0354 -0.0375 -0.0179
-0.0398 -0.0356  0.0243  0.0538 -0.0047  0.0582 -0.0823  0.0218 -0.0064 -0.1010
-0.0636  0.0763 -0.0600  0.0092 -0.0064 -0.0559 -0.0982  0.0509  0.0789  0.0080

Columns 51 to 60
-0.0974  0.0972  0.0799  0.0151  0.0664 -0.0873 -0.0054  0.0543 -0.0255  0.0727
 0.0309  0.0445 -0.0105  0.0762  0.0004 -0.0131 -0.0190  0.0973  0.0657 -0.0073
-0.0732 -0.0687  0.0702  0.0061  0.0475  0.0322 -0.0737  0.0788 -0.0207  0.0554
-0.0072 -0.0337  0.0344 -0.0650 -0.0954 -0.0923 -0.0360  0.0604  0.0443  0.0475
 0.0194 -0.0169  0.0324  0.0805 -0.0106  0.0169  0.0525  0.0588  0.0410  0.0628
 0.0234 -0.0733 -0.0461 -0.0160 -0.0544  0.0939 -0.0279  0.0635 -0.0413  0.0313

Columns 61 to 70
-0.0415  0.0501 -0.0383 -0.0936 -0.0604  0.0738 -0.0125  0.0553  0.0816 -0.0675
-0.0749  0.0076  0.0729  0.0539  0.0322 -0.0959 -0.0926  0.0371  0.0091  0.0411
-0.0961  0.0859 -0.0461  0.0901 -0.0378 -0.0215 -0.0959 -0.0508  0.0558 -0.0542
 0.0529 -0.0518 -0.0002  0.0965 -0.0566 -0.0363 -0.0801  0.0634 -0.0928 -0.0343
 0.0863 -0.0135 -0.0678 -0.0439  0.0869  0.0721 -0.0273 -0.0434 -0.0322 -0.0965
-0.0119  0.0181 -0.0825  0.0007  0.0483  0.0868  0.0940  0.0149 -0.0180 -0.0264

Columns 71 to 80
-0.0360 -0.0325 -0.0723 -0.0350  0.0988 -0.0204  0.0473 -0.0495 -0.0205  0.0794
-0.0166 -0.0357 -0.0114 -0.0944 -0.0708  0.0995 -0.0551  0.0477  0.0474  0.0422
-0.0258  0.0289  0.0644  0.0073  0.0249 -0.0689 -0.0147  0.0209 -0.0718 -0.0932
 0.0517 -0.0168 -0.0386 -0.0725 -0.0056  0.0961 -0.0311 -0.0950  0.0983 -0.0055
-0.0256  0.0371 -0.0716 -0.0307  0.0488  0.0101  0.0501  0.0406  0.0781  0.0295
-0.0818 -0.0317  0.0185  0.0925  0.0205 -0.0861 -0.0748 -0.0643 -0.0786  0.0717

Columns 81 to 90
-0.0199 -0.0990  0.0809  0.0121  0.0576 -0.0944  0.0353 -0.0129 -0.0785  0.0083
-0.0905  0.0798 -0.0978  0.0647  0.0960 -0.0434 -0.0064  0.0508 -0.0440 -0.0144
-0.0102 -0.0817 -0.0271  0.0629  0.0204  0.0709  0.0725  0.0489  0.0625  0.0224
-0.0522  0.0732 -0.0557  0.0559 -0.0226 -0.0175  0.0083 -0.0561 -0.0886  0.0315
-0.0150  0.0869 -0.0561  0.0886  0.0351  0.0960  0.0540  0.0166 -0.0876 -0.0694
 0.0837 -0.0937  0.0844  0.0145  0.0280  0.0585 -0.0465  0.0279 -0.0629 -0.0267

Columns 91 to 100
 0.0126  0.0994  0.0660  0.0252 -0.0267 -0.0215  0.0556 -0.0036  0.0147  0.0479
-0.0761  0.0581 -0.0250 -0.0968 -0.0498  0.0421 -0.0697  0.0815  0.0290 -0.0678
-0.0493 -0.0509  0.0342 -0.0612 -0.0535 -0.0474 -0.0131 -0.0728  0.0193 -0.0414
 0.0879  0.0545 -0.0619  0.0642 -0.0282 -0.0801 -0.0505  0.0558  0.0849  0.0322
-0.0230  0.0272 -0.0942  0.0356  0.0835 -0.0546  0.0231 -0.0125  0.0014 -0.0723
-0.0395  0.0338 -0.0036  0.0854  0.0888 -0.0642  0.0671  0.0395  0.0172  0.0213
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:12[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:12[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on training set:  
2016-10-13 16:47:12[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> online epoch # 50 [batchSize = 1]
 
2016-10-13 16:47:13[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> time to learn 1 sample = 61.417023340861ms 
2016-10-13 16:47:13[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> saving network model to /home/robotec/catkin_ws/src/superchicko/farnn/src/network/glassfurnace_lstm-net.t7 
2016-10-13 16:47:13[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  publishing neunet weights:  nn.Sequencer @ nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> output]
  (1): nn.LSTM(3 -> 3)
  (2): nn.Dropout(0.3, busy)
  (3): nn.LSTM(3 -> 10)
  (4): nn.Dropout(0.3, busy)
  (5): nn.LSTM(10 -> 100)
  (6): nn.Dropout(0.3, busy)
  (7): nn.Linear(100 -> 6)
} 
2016-10-13 16:47:13[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Columns 1 to 10
-0.0546  0.0293  0.0814 -0.0738  0.0173  0.0447 -0.0808  0.0767 -0.0750  0.0714
 0.0751  0.0319 -0.0912 -0.0728  0.0386  0.0191 -0.0360 -0.0659 -0.0903 -0.0795
-0.0725 -0.0814  0.0697 -0.0010  0.0695 -0.0913  0.0307 -0.0528  0.0495  0.0378
-0.0254 -0.0242 -0.0257  0.0137  0.0795  0.0165 -0.0123  0.0580  0.0838  0.0206
 0.0635 -0.0580  0.0453  0.0243 -0.0063 -0.0593 -0.0477  0.0809  0.0415  0.0267
 0.0055 -0.0168 -0.0635 -0.0535  0.0821  0.1014 -0.0770  0.0341  0.0954 -0.0700

Columns 11 to 20
-0.0914 -0.0502 -0.0690  0.0425 -0.0895 -0.0578  0.0118 -0.0372  0.0583  0.0189
-0.0771  0.0523  0.0173  0.0907  0.0386 -0.0272  0.0330 -0.0867 -0.0023  0.0816
 0.0883 -0.0112  0.0336 -0.0718  0.0879  0.0581  0.0254 -0.0843  0.0989 -0.0621
-0.0948 -0.0677  0.0516  0.0002  0.0723  0.0055 -0.0772  0.0348  0.0096  0.0948
-0.0560  0.0329  0.0936 -0.0452 -0.0829 -0.0174 -0.0112  0.0570  0.0858 -0.0226
-0.0057  0.0966  0.1007 -0.0699 -0.0227  0.0226 -0.0249 -0.0136  0.0422  0.0112

Columns 21 to 30
-0.0490  0.0223  0.0450  0.0614 -0.0561  0.0506  0.0902 -0.0810  0.0616  0.0623
-0.0543 -0.0806 -0.0814  0.0060  0.0033 -0.0013  0.0042 -0.0137 -0.0925  0.0783
 0.0021 -0.0301 -0.0955 -0.0495 -0.0060  0.0954 -0.0595 -0.0955  0.0658 -0.0757
 0.0878  0.0333  0.0218 -0.0277 -0.0824  0.0322 -0.0682  0.0588  0.0146 -0.0402
 0.0579  0.0049 -0.0105  0.0348  0.0796 -0.0094 -0.0857 -0.0452 -0.0378  0.0251
 0.0663  0.0226 -0.0958 -0.0749 -0.0165 -0.0189 -0.0394 -0.0700  0.0457 -0.0666

Columns 31 to 40
 0.0117 -0.0044 -0.0678 -0.0173  0.0989 -0.0982  0.0829 -0.0367  0.0177  0.0243
 0.0234 -0.0623  0.0159 -0.0878  0.0629  0.0061  0.0189  0.0059  0.0483  0.0143
 0.0100 -0.0576  0.0206  0.0878 -0.0277 -0.0738 -0.0225  0.0357  0.0533  0.0572
-0.1024 -0.0203  0.0887 -0.0035 -0.0228  0.0642  0.0839 -0.0630  0.0306 -0.0438
-0.0374  0.0579  0.0249 -0.0941  0.0308  0.0992 -0.0163  0.0174  0.0562  0.0098
-0.0036  0.0027 -0.0101 -0.0867  0.0735 -0.0697  0.0367  0.0020  0.0234  0.0419

Columns 41 to 50
-0.0524  0.0050 -0.0873 -0.0512  0.0065 -0.0947  0.0611 -0.0596  0.0633  0.0952
 0.0849 -0.0001  0.0249  0.0193  0.0137  0.0356 -0.0681  0.0660 -0.0941 -0.0398
 0.0744  0.0571  0.0911  0.0882  0.0536  0.0817 -0.0644  0.0889 -0.0718 -0.0062
-0.0893  0.0469  0.0176 -0.0015 -0.0228  0.0637  0.0320 -0.0354 -0.0375 -0.0179
-0.0398 -0.0356  0.0243  0.0537 -0.0047  0.0582 -0.0823  0.0218 -0.0064 -0.1011
-0.0636  0.0763 -0.0600  0.0091 -0.0064 -0.0559 -0.0982  0.0509  0.0789  0.0080

Columns 51 to 60
-0.0974  0.0972  0.0799  0.0151  0.0664 -0.0873 -0.0054  0.0543 -0.0256  0.0727
 0.0309  0.0445 -0.0104  0.0763  0.0004 -0.0131 -0.0190  0.0973  0.0657 -0.0073
-0.0731 -0.0686  0.0702  0.0062  0.0476  0.0322 -0.0737  0.0788 -0.0206  0.0554
-0.0072 -0.0337  0.0344 -0.0650 -0.0954 -0.0923 -0.0360  0.0604  0.0443  0.0475
 0.0194 -0.0169  0.0324  0.0805 -0.0106  0.0169  0.0524  0.0588  0.0410  0.0628
 0.0234 -0.0732 -0.0461 -0.0160 -0.0544  0.0939 -0.0279  0.0635 -0.0413  0.0313

Columns 61 to 70
-0.0415  0.0501 -0.0383 -0.0936 -0.0604  0.0738 -0.0125  0.0553  0.0816 -0.0675
-0.0749  0.0076  0.0728  0.0539  0.0321 -0.0959 -0.0926  0.0371  0.0091  0.0410
-0.0960  0.0859 -0.0461  0.0901 -0.0379 -0.0215 -0.0959 -0.0507  0.0559 -0.0542
 0.0529 -0.0518 -0.0002  0.0964 -0.0566 -0.0363 -0.0801  0.0634 -0.0928 -0.0342
 0.0864 -0.0135 -0.0679 -0.0439  0.0868  0.0721 -0.0273 -0.0434 -0.0321 -0.0965
-0.0119  0.0182 -0.0825  0.0007  0.0483  0.0868  0.0940  0.0149 -0.0180 -0.0264

Columns 71 to 80
-0.0360 -0.0325 -0.0723 -0.0350  0.0988 -0.0205  0.0473 -0.0495 -0.0205  0.0794
-0.0167 -0.0357 -0.0114 -0.0944 -0.0708  0.0995 -0.0551  0.0477  0.0474  0.0422
-0.0258  0.0289  0.0644  0.0074  0.0249 -0.0689 -0.0147  0.0209 -0.0718 -0.0932
 0.0517 -0.0168 -0.0386 -0.0725 -0.0056  0.0961 -0.0311 -0.0950  0.0983 -0.0055
-0.0256  0.0371 -0.0715 -0.0307  0.0488  0.0101  0.0501  0.0406  0.0781  0.0295
-0.0818 -0.0317  0.0185  0.0925  0.0205 -0.0861 -0.0748 -0.0643 -0.0786  0.0717

Columns 81 to 90
-0.0199 -0.0990  0.0809  0.0121  0.0576 -0.0944  0.0353 -0.0129 -0.0785  0.0083
-0.0906  0.0798 -0.0978  0.0648  0.0960 -0.0434 -0.0064  0.0508 -0.0440 -0.0144
-0.0102 -0.0817 -0.0271  0.0629  0.0204  0.0709  0.0724  0.0489  0.0625  0.0225
-0.0523  0.0732 -0.0557  0.0559 -0.0226 -0.0175  0.0083 -0.0561 -0.0886  0.0315
-0.0150  0.0869 -0.0560  0.0886  0.0351  0.0960  0.0540  0.0166 -0.0876 -0.0694
 0.0837 -0.0937  0.0845  0.0145  0.0280  0.0585 -0.0465  0.0279 -0.0629 -0.0267

Columns 91 to 100
 0.0126  0.0994  0.0660  0.0252 -0.0267 -0.0215  0.0556 -0.0035  0.0147  0.0478
-0.0761  0.0581 -0.0250 -0.0968 -0.0497  0.0421 -0.0696  0.0815  0.0290 -0.0678
-0.0493 -0.0509  0.0342 -0.0612 -0.0535 -0.0474 -0.0131 -0.0729  0.0192 -0.0414
 0.0879  0.0545 -0.0619  0.0642 -0.0282 -0.0801 -0.0505  0.0558  0.0849  0.0322
-0.0231  0.0272 -0.0942  0.0356  0.0835 -0.0546  0.0231 -0.0125  0.0014 -0.0723
-0.0395  0.0339 -0.0036  0.0854  0.0888 -0.0642  0.0671  0.0395  0.0172  0.0214
[torch.CudaTensor of size 6x100]
 weights 
2016-10-13 16:47:13[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  <trainer> on testing Set: 
2016-10-13 16:47:13[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Experiment started at:  1476395221.4045 
2016-10-13 16:47:13[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Experiment Ended at:  1476395233.1911 
2016-10-13 16:47:13[outputs/lstm,batchSize=1,maxIter=10 Deep Head Motion Control]:  Total Time Taken =  11.786524772644 secs 
